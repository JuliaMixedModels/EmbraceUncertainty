<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Phillip Alday" />
  <meta name="author" content="Dave Kleinschmidt" />
  <meta name="author" content="Reinhold Kliegl" />
  <meta name="author" content="Douglas Bates" />
  <title>The linear mixed-effects probability model - Embrace Uncertainty</title>
  <link rel="shortcut icon" type="image/png" href="/favicon.png"/>
  <link rel="stylesheet" href="/style.css"/>
    <script src="/mousetrap.min.js"></script>
    <!-- TODO: Add url_prefix. -->
  <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("/JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="/github.min.css">
<script src="/highlight.min.js"></script>
<script src="/julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        hljs.highlightElement(el);
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="/">Embrace Uncertainty</a>
</div><br />
<span class="books-subtitle">
Fitting Mixed-Effects Models with Julia
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="/ExamLMM"><b>1</b> A Simple, Linear, Mixed-..</a></li>
<li><a class="menu-level-2" href="/memod"><b>1.1</b> Mixed-effects models</a></li>
<li><a class="menu-level-2" href="/DyestuffData"><b>1.2</b> The dyestuff and dyest..</a></li>
<li><a class="menu-level-2" href="/FittingLMMs"><b>1.3</b> Fitting linear mixed m..</a></li>
<li><a class="menu-level-2" href="/Probability"><b>1.4</b> The linear mixed-effec..</a></li>
<li><a class="menu-level-2" href="/variability"><b>1.5</b> Assessing the variabil..</a></li>
<li><a class="menu-level-2" href="/assessRE"><b>1.6</b> Assessing the random e..</a></li>
<li><a class="menu-level-2" href="/ChIntroSummary"><b>1.7</b> Chapter summary</a></li>
<li><a class="menu-level-1" href="/Multiple"><b>2</b> Models With Multiple Ran..</a></li>
<li><a class="menu-level-2" href="/crossedRE"><b>2.1</b> A Model With Crossed R..</a></li>
<li><a class="menu-level-2" href="/NestedRE"><b>2.2</b> A Model With Nested Ra..</a></li>
<li><a class="menu-level-2" href="/partially"><b>2.3</b> A Model With Partially..</a></li>
<li><a class="menu-level-2" href="/MultSummary"><b>2.4</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/longitudinal"><b>3</b> # Models for Longitudina..</a></li>
<li><a class="menu-level-2" href="/sleep"><b>3.1</b> Data</a></li>
<li><a class="menu-level-2" href="/SleepMixed"><b>3.2</b> Data</a></li>
<li><a class="menu-level-2" href="/assess-prec-param"><b>3.3</b> Assessing the Precisio..</a></li>
<li><a class="menu-level-2" href="/fm07re"><b>3.4</b> Examining the Random E..</a></li>
<li><a class="menu-level-2" href="/chapter-summary"><b>3.5</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/computational"><b>4</b> Computational Methods fo..</a></li>
<li><a class="menu-level-2" href="/defnLMM"><b>4.1</b> Definitions and Basic ..</a></li>
<li><a class="menu-level-2" href="/conddistUgivenY"><b>4.2</b> </a></li>
<li><a class="menu-level-2" href="/IntegratingH"><b>4.3</b> in the Linear Mixed Mo..</a></li>
<li><a class="menu-level-2" href="/PLSsol"><b>4.4</b> </a></li>
<li><a class="menu-level-2" href="/REML"><b>4.5</b> The REML Criterion</a></li>
<li><a class="menu-level-2" href="/stepByStep"><b>4.6</b> Step-by-step Evaluatio..</a></li>
<li><a class="menu-level-2" href="/general"><b>4.7</b> Generalizing to Other ..</a></li>
<li><a class="menu-level-2" href="/lmmsummary"><b>4.8</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/GLMMbinomial"><b>5</b> Generalized Linear Mixed..</a></li>
<li><a class="menu-level-2" href="/contraception"><b>5.1</b> Artificial contracepti..</a></li>
<li><a class="menu-level-2" href="/GLMMlink"><b>5.2</b> Link functions and int..</a></li>
<li><a class="menu-level-1" href="/references"><b></b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="1.4" id="sec:Probability"><span class="header-section-number">1.4</span> The linear mixed-effects probability model</h2>
<p>In explaining some of parameter estimates related to the random effects we have used terms such as “unconditional distribution” from the theory of probability. Before proceeding further we clarify the linear mixed-effects probability model and define several terms and concepts that will be used throughout the book. Readers who are more interested in practical results than in the statistical theory should feel free to skip this section.</p>
<h3 data-number="1.4.1" id="sec:definitions"><span class="header-section-number">1.4.1</span> Definitions and results</h3>
<p>In this section we provide some definitions and formulas without derivation and with minimal explanation, so that we can use these terms in what follows. In Section <a href="/computational#sec:computational">4</a> we revisit these definitions providing derivations and more explanation.</p>
<p>As mentioned in Section <a href="/memod#sec:memod">1.1</a>, a mixed model incorporates two random variables: <span class="math inline">\(\mathcal{B}\)</span>, the <span class="math inline">\(q\)</span>-dimensional vector of random effects, and <span class="math inline">\(\mathcal{Y}\)</span>, the <span class="math inline">\(n\)</span>-dimensional response vector. In a linear mixed model the unconditional distribution of <span class="math inline">\(\mathcal{B}\)</span> and the conditional distribution, <span class="math inline">\((\mathcal{Y}|\mathcal{B}=\mathbf{b})\)</span>, are both multivariate Gaussian (or “normal”) distributions <span id="eq:LMMdist"><span class="math display">\[
  \begin{aligned}
    (\mathcal{Y}|\mathcal{B}=\mathbf{b})&amp;\sim\mathcal{N}(\mathbf{X}\mathbf{\beta}+\mathbf{Z}\mathbf{b},\sigma^2\mathbf{I})\\
    \mathcal{B}&amp;\sim\mathcal{N}(\mathbf{0},\Sigma_\theta) .
  \end{aligned}
\qquad(1)\]</span></span> The <em>conditional mean</em> of <span class="math inline">\(\mathcal{Y}\)</span>, given <span class="math inline">\(\mathcal{B}=\mathbf{b}\)</span>, is the <em>linear predictor</em>, <span class="math inline">\(\mathbf{X}\beta+\mathbf{Z}\mathbf{b}\)</span>, which depends on the <span class="math inline">\(p\)</span>-dimensional <em>fixed-effects parameter</em>, <span class="math inline">\(\mathbf{\beta}\)</span>, and on <span class="math inline">\(\mathbf{b}\)</span>. The <em>model matrices</em>, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span>, of dimension <span class="math inline">\(n\times p\)</span> and <span class="math inline">\(n\times q\)</span>, respectively, are determined from the formula for the model and the values of covariates. Although the matrix <span class="math inline">\(\mathbf{Z}\)</span> can be large (i.e. both <span class="math inline">\(n\)</span> and <span class="math inline">\(q\)</span> can be large), it is sparse (i.e. most of the elements in the matrix are zero).</p>
<p>The <em>relative covariance factor</em>, <span class="math inline">\(\Lambda_\theta\)</span>, is a <span class="math inline">\(q\times q\)</span> matrix, depending on the <em>variance-component parameter</em>, <span class="math inline">\(\mathbf{\theta}\)</span>, and generating the symmetric <span class="math inline">\(q\times q\)</span> variance-covariance matrix, <span class="math inline">\(\Sigma_\theta\)</span>, according to <span id="eq:relcovfac"><span class="math display">\[
\Sigma_\theta=\sigma^2\Lambda_\theta\Lambda_\theta&#39; .
\qquad(2)\]</span></span> The <em>spherical random effects</em>, <span class="math inline">\(\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I}_q)\)</span>, determine <span class="math inline">\(\mathcal{B}\)</span> according to <span class="math display">\[
\mathcal{B}=\Lambda_\theta\mathcal{U} .
\]</span></p>
<p>The <em>penalized residual sum of squares</em> (PRSS), <span class="math display">\[
r^2(\mathbf{\theta},\mathbf{\beta},\mathbf{u})=
\|\mathbf{y} -\mathbf{X}\mathbf{\beta} -\mathbf{Z}\Lambda_\theta\mathbf{u}\|^2  + \|\mathbf{u}\|^2,
\]</span> is the sum of squared residuals, measuring fidelity of the model to the data, and a penalty on the size of <span class="math inline">\(\mathbf{u}\)</span>, measuring the complexity of the model. Minimizing <span class="math inline">\(r^2\)</span> with respect to <span class="math inline">\(\mathbf{u}\)</span>, <span class="math display">\[
r^2_{\beta,\theta}=\min_{\mathbf{u}}\left\{\|\mathbf{y} -\mathbf{X}\mathbf{\beta} -\mathbf{Z}\Lambda_\theta\mathbf{u}\|^2+\|\mathbf{u}\|^2\right\}
\]</span> is a direct (i.e. non-iterative) computation during which we calculate the <em>sparse Cholesky factor</em>, <span class="math inline">\(\mathbf{L}_\theta\)</span>, which is a lower triangular <span class="math inline">\(q\times q\)</span> matrix satisfying <span id="eq:sparseCholesky1"><span class="math display">\[
  \mathbf{L}_\theta\mathbf{L}_\theta&#39;=
  \Lambda_\theta&#39;\mathbf{Z}&#39;\mathbf{Z}\Lambda_\theta+\mathbf{I}_q .
\qquad(3)\]</span></span> where <span class="math inline">\(\mathbf{I}_q\)</span> is the <span class="math inline">\(q\times q\)</span> <em>identity matrix</em>.</p>
<p>The objective (negative twice the log-likelihood) for the parameters, given the data, <span class="math inline">\(\mathbf{y}\)</span>, is <span id="eq:LMMdeviance"><span class="math display">\[
  d(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y})
  =n\log(2\pi\sigma^2)+\log(|\mathbf{L}_\theta|^2)+\frac{r^2_{\beta,\theta}}{\sigma^2}.
\qquad(4)\]</span></span> where <span class="math inline">\(|\mathbf{L}_\theta|\)</span> denotes the <em>determinant</em> of <span class="math inline">\(\mathbf{L}_\theta\)</span>. Because <span class="math inline">\(\mathbf{L}_\theta\)</span> is triangular, its determinant is the product of its diagonal elements.</p>
<p>Because the conditional mean, <span class="math inline">\(\mathbf{\mu}_{\mathcal{Y}|\mathcal{B}=\mathbf{b}}=\mathbf{X}\mathbf{\beta}+\mathbf{Z}\Lambda_\theta\mathbf{u}\)</span>, is a linear function of both <span class="math inline">\(\mathbf{\beta}\)</span> and <span class="math inline">\(\mathbf{u}\)</span>, minimization of the PRSS with respect to both <span class="math inline">\(\mathbf{\beta}\)</span> and <span class="math inline">\(\mathbf{u}\)</span> to produce <span class="math display">\[
r^2_\theta =\min_{\mathbf{\beta},\mathbf{u}}\left\{\|\mathbf{y} -\mathbf{X}\mathbf{\beta} -\mathbf{Z}\Lambda_\theta\mathbf{u}\|^2+\|\mathbf{u}\|^2\right\}
\]</span> is also a direct calculation. The values of <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{\beta}\)</span> that provide this minimum are called, respectively, the <em>conditional mode</em>, <span class="math inline">\(\tilde{\mathbf{u}}_\theta\)</span>, of the spherical random effects and the conditional estimate, <span class="math inline">\(\widehat{\mathbf{\beta}}_\theta\)</span>, of the fixed effects. At the conditional estimate of the fixed effects the objective, negative twice the log-likelihood, is <span id="eq:LMMprdev"><span class="math display">\[
  d(\mathbf{\theta},\widehat{\beta}_\theta,\sigma|\mathbf{y})
  =n\log(2\pi\sigma^2)+\log(|\mathbf{L}_\theta|^2)+\frac{r^2_\theta}{\sigma^2}
\qquad(5)\]</span></span></p>
<p>Minimizing this expression with respect to <span class="math inline">\(\sigma^2\)</span> produces the conditional estimate <span class="math display">\[
\widehat{\sigma^2}_\theta=\frac{r^2_\theta}{n}
\]</span> which provides the <em>profiled objective</em>, <span id="eq:LMMprofdev"><span class="math display">\[
  \tilde{d}(\mathbf{\theta}|\mathbf{y})=d(\mathbf{\theta},\widehat{\beta}_\theta,\widehat{\sigma}_\theta|\mathbf{y})
  =\log(|\mathbf{L}_\theta|^2)+n\left[1 +
    \log\left(\frac{2\pi r^2_\theta}{n}\right)\right],
\qquad(6)\]</span></span> a function of <span class="math inline">\(\mathbf{\theta}\)</span> alone.</p>
<p>The MLE of <span class="math inline">\(\mathbf{\theta}\)</span>, written <span class="math inline">\(\widehat{\mathbf{\theta}}\)</span>, is the value that minimizes the profiled objective (Equation <a href="#eq:LMMprofdev">6</a>). We determine this value by numerical optimization. In the process of evaluating <span class="math inline">\(\tilde{d}(\widehat{\mathbf{\theta}}|\mathbf{y})\)</span> we determine <span class="math inline">\(\widehat{\mathbf{\beta}}=\widehat{\mathbf{\beta}}_{\widehat\theta}\)</span>, <span class="math inline">\(\tilde{\mathbf{u}}_{\widehat{\theta}}\)</span> and <span class="math inline">\(r^2_{\widehat{\theta}}\)</span>, from which we can evaluate <span class="math inline">\(\widehat{\sigma}=\sqrt{r^2_{\widehat{\theta}}/n}\)</span>.</p>
<p>The elements of the conditional mode of <span class="math inline">\(\mathcal{B}\)</span>, evaluated at the parameter estimates, <span class="math display">\[
  \tilde{b}_{\widehat{\theta}}=\Lambda_{\widehat{\theta}}\tilde{u}_{\widehat{\theta}}
\]</span> are sometimes called the <em>best linear unbiased predictors</em> or BLUPs of the random effects. Although it has an appealing acronym, we don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best?”) and prefer the term “conditional mode,” which is explained in .</p>
<h3 data-number="1.4.2" id="sec:FittedModel"><span class="header-section-number">1.4.2</span> Matrices and vectors in the fitted model object</h3>
<p>The optional argument, <code>thin=1</code>, in a call to <code>fit</code> causes all the values of <span class="math inline">\(\mathbf{\theta}\)</span> and the objective during the progress of the iterative optimization of <span class="math inline">\(\tilde{d}(\mathbf{\theta}|\mathbf{y})\)</span> to be stored in the <code>optsum</code> member of the fit.</p>
<pre class="language-julia"><code>m1trace = fit(MixedModel, @formula(yield ~ 1 + (1|batch)), dyestuff; thin=1)
DataFrame(m1trace.optsum.fitlog)</code></pre>
<table>
<thead>
<tr class="header">
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">[1.0]</td>
<td style="text-align: right;">327.76702162461663</td>
</tr>
<tr class="even">
<td style="text-align: right;">[1.0]</td>
<td style="text-align: right;">327.76702162461663</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[1.75]</td>
<td style="text-align: right;">331.03619322245146</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.25]</td>
<td style="text-align: right;">330.64583141448713</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.97618963546647]</td>
<td style="text-align: right;">327.69511270610235</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.9285689063994098]</td>
<td style="text-align: right;">327.5663091453212</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.8333274482652895]</td>
<td style="text-align: right;">327.3825965130741</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.8071883308455869]</td>
<td style="text-align: right;">327.353154540847</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7996883308455869]</td>
<td style="text-align: right;">327.34662982410373</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.7921883308455868]</td>
<td style="text-align: right;">327.3410019200219</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7771883308455869]</td>
<td style="text-align: right;">327.3325253537053</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.7471883308455869]</td>
<td style="text-align: right;">327.32733056111874</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7396883308455869]</td>
<td style="text-align: right;">327.328619097772</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.7527765100666763]</td>
<td style="text-align: right;">327.32706023604106</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7535265100666763]</td>
<td style="text-align: right;">327.32706815454657</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.7525837537781024]</td>
<td style="text-align: right;">327.327059881212</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7525087537781023]</td>
<td style="text-align: right;">327.32705992904994</td>
</tr>
<tr class="even">
<td style="text-align: right;">[0.7525912537781024]</td>
<td style="text-align: right;">327.32705988216696</td>
</tr>
<tr class="odd">
<td style="text-align: right;">[0.7525806394967323]</td>
<td style="text-align: right;">327.327059881143</td>
</tr>
</tbody>
</table>
<p>Here the algorithm converges after 17 function evaluations to a profiled deviance of 327.327059881143 at <span class="math inline">\(\theta=0.0.7525806394967323\)</span>. In this model the scalar parameter <span class="math inline">\(\theta\)</span> is the ratio <span class="math inline">\(\sigma_1/\sigma\)</span>. (Different versions of packages or of Julia or different choices of libraries for the basic linear algebra subroutines, or BLAS, may result in slightly different values.)</p>
<p>The actual values of many of the matrices and vectors defined above are available as properties of the fitted model object.</p>
<p>In this case the <span class="math inline">\(\Lambda_\theta\)</span> matrix will be a <span class="math inline">\(6\times 6\)</span> diagonal matrix with the diagonal elements all equal to <span class="math inline">\(\theta=0.7525807289241839\)</span>.</p>
<p>The Cholesky factor, <span class="math inline">\(\mathbf{L}\)</span>, is</p>
<pre class="language-julia"><code>sparseL(m1; full=true)</code></pre>
<pre class="output"><code>8×8 SparseArrays.SparseMatrixCSC{Float64, Int32} with 21 stored entries:
    1.95752      ⋅           ⋅           ⋅           ⋅           ⋅           ⋅          ⋅ 
     ⋅          1.95752      ⋅           ⋅           ⋅           ⋅           ⋅          ⋅ 
     ⋅           ⋅          1.95752      ⋅           ⋅           ⋅           ⋅          ⋅ 
     ⋅           ⋅           ⋅          1.95752      ⋅           ⋅           ⋅          ⋅ 
     ⋅           ⋅           ⋅           ⋅          1.95752      ⋅           ⋅          ⋅ 
     ⋅           ⋅           ⋅           ⋅           ⋅          1.95752      ⋅          ⋅ 
    1.92228     1.92228     1.92228     1.92228     1.92228     1.92228     2.79804     ⋅ 
 2893.03     2937.24     3006.45     2879.58     3075.65     2825.75     4274.01     271.178</code></pre>
<p>which consists of three blocks</p>
<pre class="language-julia"><code>BlockDescription(m1)</code></pre>
<pre class="output"><code>rows:    batch         fixed     
   6:   Diagonal   
   2:    Dense         Dense     
</code></pre>
<p>As we get to larger models we will see that large sparse matrices are displayed as patterns rather than as numerical values.</p>
<p>In this simple model <span class="math inline">\(\Lambda=\theta\mathbf{I}_6\)</span> is a multiple of the identity matrix and the <span class="math inline">\(30\times 6\)</span> model matrix <span class="math inline">\(\mathbf{Z}\)</span>, whose transpose is</p>
<pre class="language-julia"><code>Int.(Array(first(m1.reterms)))&#39;</code></pre>
<pre class="output"><code>6×30 adjoint(::Matrix{Int64}) with eltype Int64:
 1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1</code></pre>
<p>(The conversion to integer, or <code>Int</code>, values is to save space when printing.)</p>
<p>Thus <span class="math inline">\(\mathbf{Z}\)</span> is the indicator columns for <code>batch</code>. Because the data are balanced with respect to <code>batch</code>, the upper-left block of the Cholesky factor, <span class="math inline">\(\mathbf{L}\)</span>, is also a multiple of the identity.</p>
<p>The vector <span class="math inline">\(\mathbf{u}\)</span> is available as a row vector</p>
<pre class="language-julia"><code>first(m1.u)</code></pre>
<pre class="output"><code>1×6 Matrix{Float64}:
 -22.0949  0.490999  35.8429  -28.9689  71.1948  -56.4648</code></pre>
<p>The vector <span class="math inline">\(\mathbf{\beta}\)</span> and the model matrix <span class="math inline">\(\mathbf{X}\)</span> are available as</p>
<pre class="language-julia"><code>Int.(m1.X&#39;)</code></pre>
<pre class="output"><code>1×30 Matrix{Int64}:
 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1</code></pre>
<p>and</p>
<pre class="language-julia"><code>m1.β</code></pre>
<pre class="output"><code>[1527.4999999999989]</code></pre>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="/FittingLMMs"><b>1.3</b> Fitting linear mixed m..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="/variability"><b>1.5</b> Assessing the variabil..</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Phillip Alday, Dave Kleinschmidt, Reinhold Kliegl, Douglas Bates
</div>
</div>
</div>
</body>
</html>