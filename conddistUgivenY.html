<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Phillip Alday" />
  <meta name="author" content="Dave Kleinschmidt" />
  <meta name="author" content="Reinhold Kliegl" />
  <meta name="author" content="Douglas Bates" />
  <title> - Embrace Uncertainty</title>
  <link rel="shortcut icon" type="image/png" href="/favicon.png"/>
  <link rel="stylesheet" href="/style.css"/>
    <script src="/mousetrap.min.js"></script>
    <!-- TODO: Add url_prefix. -->
  <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("/JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="/github.min.css">
<script src="/highlight.min.js"></script>
<script src="/julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        hljs.highlightElement(el);
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="/">Embrace Uncertainty</a>
</div><br />
<span class="books-subtitle">
Fitting Mixed-Effects Models with Julia
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="/ExamLMM"><b>1</b> A Simple, Linear, Mixed-..</a></li>
<li><a class="menu-level-2" href="/memod"><b>1.1</b> Mixed-effects models</a></li>
<li><a class="menu-level-2" href="/DyestuffData"><b>1.2</b> The dyestuff and dyest..</a></li>
<li><a class="menu-level-2" href="/FittingLMMs"><b>1.3</b> Fitting linear mixed m..</a></li>
<li><a class="menu-level-2" href="/Probability"><b>1.4</b> The linear mixed-effec..</a></li>
<li><a class="menu-level-2" href="/variability"><b>1.5</b> Assessing the variabil..</a></li>
<li><a class="menu-level-2" href="/assessRE"><b>1.6</b> Assessing the random e..</a></li>
<li><a class="menu-level-2" href="/ChIntroSummary"><b>1.7</b> Chapter summary</a></li>
<li><a class="menu-level-1" href="/Multiple"><b>2</b> Models With Multiple Ran..</a></li>
<li><a class="menu-level-2" href="/crossedRE"><b>2.1</b> A Model With Crossed R..</a></li>
<li><a class="menu-level-2" href="/NestedRE"><b>2.2</b> A Model With Nested Ra..</a></li>
<li><a class="menu-level-2" href="/partially"><b>2.3</b> A Model With Partially..</a></li>
<li><a class="menu-level-2" href="/MultSummary"><b>2.4</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/longitudinal"><b>3</b> # Models for Longitudina..</a></li>
<li><a class="menu-level-2" href="/sleep"><b>3.1</b> Data</a></li>
<li><a class="menu-level-2" href="/SleepMixed"><b>3.2</b> Data</a></li>
<li><a class="menu-level-2" href="/assess-prec-param"><b>3.3</b> Assessing the Precisio..</a></li>
<li><a class="menu-level-2" href="/fm07re"><b>3.4</b> Examining the Random E..</a></li>
<li><a class="menu-level-2" href="/chapter-summary"><b>3.5</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/computational"><b>4</b> Computational Methods fo..</a></li>
<li><a class="menu-level-2" href="/defnLMM"><b>4.1</b> Definitions and Basic ..</a></li>
<li><a class="menu-level-2" href="/conddistUgivenY"><b>4.2</b> </a></li>
<li><a class="menu-level-2" href="/IntegratingH"><b>4.3</b> in the Linear Mixed Mo..</a></li>
<li><a class="menu-level-2" href="/PLSsol"><b>4.4</b> </a></li>
<li><a class="menu-level-2" href="/REML"><b>4.5</b> The REML Criterion</a></li>
<li><a class="menu-level-2" href="/stepByStep"><b>4.6</b> Step-by-step Evaluatio..</a></li>
<li><a class="menu-level-2" href="/general"><b>4.7</b> Generalizing to Other ..</a></li>
<li><a class="menu-level-2" href="/lmmsummary"><b>4.8</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/GLMMbinomial"><b>5</b> Generalized Linear Mixed..</a></li>
<li><a class="menu-level-2" href="/contraception"><b>5.1</b> Artificial contracepti..</a></li>
<li><a class="menu-level-2" href="/GLMMlink"><b>5.2</b> Link functions and int..</a></li>
<li><a class="menu-level-1" href="/references"><b></b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="4.2" id="sec:conddistUgivenY"><span class="header-section-number">4.2</span> The Conditional Distribution <span class="math inline">\((\mathcal{U}|\mathcal{Y}=\mathbf{y})\)</span></h2>
<p>In this chapter it will help to be able to distinguish between the observed response vector and an arbitrary value of <span class="math inline">\(\mathcal{Y}\)</span>. For this chapter only we will write the observed data vector as <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>, with the understanding that <span class="math inline">\(\mathbf{y}\)</span> without the subscript will refer to an arbitrary value of the random variable <span class="math inline">\(\mathcal{Y}\)</span>.</p>
<p>The likelihood of the parameters, <span class="math inline">\(\mathbf{\theta}\)</span>, <span class="math inline">\(\mathbf{\beta}\)</span>, and <span class="math inline">\(\sigma\)</span>, given the observed data, <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>, is the probability density of <span class="math inline">\(\mathcal{Y}\)</span>, evaluated at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>. Although the numerical values of the probability density and the likelihood are identical, the interpretations of these functions are different. In the density we consider the parameters to be fixed and the value of <span class="math inline">\(\mathbf{y}\)</span> as varying. In the likelihood we consider <span class="math inline">\(\mathbf{y}\)</span> to be fixed at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span> and the parameters, <span class="math inline">\(\mathbf{\theta}\)</span>, <span class="math inline">\(\mathbf{\beta}\)</span> and <span class="math inline">\(\sigma\)</span>, as varying.</p>
<p>The natural approach for evaluating the likelihood is to determine the marginal distribution of <span class="math inline">\(\mathcal{Y}\)</span>, which in this case amounts to determining the marginal density of <span class="math inline">\(\mathcal{Y}\)</span>, and evaluate that density at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>. To follow this course we would first determine the joint density of <span class="math inline">\(\mathcal{U}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span>, written <span class="math inline">\(f_{\mathcal{U},\mathcal{Y}}(\mathbf{u},\mathbf{y})\)</span>, then integrate this density with respect to <span class="math inline">\(\mathbf{u}\)</span> to create the marginal density, <span class="math inline">\(f_{\mathcal{Y}}(\mathbf{y})\)</span>, and finally evaluate this marginal density at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>.</p>
<p>To allow for later generalizations we will change the order of these steps slightly. We evaluate the joint density function, <span class="math inline">\(f_{\mathcal{U},\mathcal{Y}}(\mathbf{u},\mathbf{y})\)</span>, at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>, producing the <em>unnormalized conditional density</em>, <span class="math inline">\(h(\mathbf{u})\)</span>. We say that <span class="math inline">\(h\)</span> is “unnormalized” because the conditional density is a multiple of <span class="math inline">\(h\)</span> <span class="math display">\[\label{eq:conddenUgivenY}
  f_{\mathcal{U}|\mathcal{Y}}(\mathbf{u}|\mathbf{y}_{\text{obs}})=\frac
  {h(\mathbf{u})}{\int_{\mathbb{R}^q}h(\mathbf{u})\,d\mathbf{u}}  .
\]</span> In some theoretical developments the normalizing constant, which is the integral in the denominator of an expression like (<a href="#eq:conddenUgivenY" data-reference-type="ref" data-reference="eq:conddenUgivenY">[eq:conddenUgivenY]</a>), is not of interest. Here it is of interest because the normalizing constant is exactly the likelihood that we wish to evaluate, <span class="math display">\[\label{eq:LMMlikelihood}
  L(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y}_{\text{obs}}) =
  \int_{\mathbb{R}^q}h(\mathbf{u})\,d\mathbf{u} .
\]</span></p>
<p>For a linear mixed model, where all the distributions of interest are multivariate Gaussian and the conditional mean, <span class="math inline">\(\mathbf{\mu}\)</span>, is a linear function of both <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{\beta}\)</span>, the distinction between evaluating the joint density at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span> to produce <span class="math inline">\(h(\mathbf{u})\)</span> then integrating with respect to <span class="math inline">\(\mathbf{u}\)</span>, as opposed to first integrating the joint density then evaluating at <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>, is not terribly important. For other mixed models this distinction can be important. In particular, generalized linear mixed models, described in Chap. <a href="#chap:GLMMbinomial" data-reference-type="ref" data-reference="chap:GLMMbinomial">[chap:GLMMbinomial]</a>, are often used to model a discrete response, such as a binary response or a count, leading to a joint distribution for <span class="math inline">\(\mathcal{Y}\)</span> and <span class="math inline">\(\mathcal{U}\)</span> that is discrete with respect to one variable, <span class="math inline">\(\mathbf{y}\)</span>, and continuous with respect to the other, <span class="math inline">\(\mathbf{u}\)</span>. In such cases there isn’t a joint density for <span class="math inline">\(\mathcal{Y}\)</span> and <span class="math inline">\(\mathcal{U}\)</span>. The necessary distribution theory for general <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{u}\)</span> is well-defined but somewhat awkward to describe. It is much easier to realize that we are only interested in the observed response vector, <span class="math inline">\(\mathbf{y}_{\text{obs}}\)</span>, not some arbitrary value of <span class="math inline">\(\mathbf{y}\)</span>, so we can concentrate on the conditional distribution of <span class="math inline">\(\mathcal{U}\)</span> given <span class="math inline">\(\mathcal{Y}=\mathbf{y}_{\text{obs}}\)</span>. For all the mixed models we will consider, the conditional distribution, <span class="math inline">\((\mathcal{U}|\mathcal{Y}=\mathbf{y}_{\text{obs}})\)</span>, is continuous and both the conditional density, <span class="math inline">\(f_{\mathcal{U}|\mathcal{Y}}(\mathbf{u}|\mathbf{y}_{\text{obs}})\)</span>, and its unnormalized form, <span class="math inline">\(h(\mathbf{u})\)</span>, are well-defined.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="/defnLMM"><b>4.1</b> Definitions and Basic ..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="/IntegratingH"><b>4.3</b> in the Linear Mixed Mo..</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Phillip Alday, Dave Kleinschmidt, Reinhold Kliegl, Douglas Bates
</div>
</div>
</div>
</body>
</html>