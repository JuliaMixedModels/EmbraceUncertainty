[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Embrace Uncertainty",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html#sec-memod",
    "href": "intro.html#sec-memod",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.1 Mixed-effects models",
    "text": "1.1 Mixed-effects models\nMixed-effects models, like many other types of statistical models, describe a relationship between a response variable and some of the covariates that have been measured or observed along with the response. In mixed-effects models at least one of the covariates is a categorical covariate representing experimental or observational “units” in the data set. In the example from the chemical industry that is given in this chapter, the observational unit is the batch of an intermediate product used in production of a dye. In medical and social sciences the observational units are often the human or animal subjects in the study. In agriculture the experimental units may be the plots of land or the specific plants being studied.\nIn all of these cases the categorical covariate or covariates are observed at a set of discrete levels. We may use numbers, such as subject identifiers, to designate the particular levels that we observed but these numbers are simply labels. The important characteristic of a categorical covariate is that, at each observed value of the response, the covariate takes on the value of one of a set of distinct levels.\nParameters associated with the particular levels of a covariate are sometimes called the “effects” of the levels. If the set of possible levels of the covariate is fixed and reproducible we model the covariate using fixed-effects parameters. If the levels that we observed represent a random sample from the set of all possible levels we incorporate random effects in the model.\nThere are two things to notice about this distinction between fixed-effects parameters and random effects. First, the names are misleading because the distinction between fixed and random is more a property of the levels of the categorical covariate than a property of the effects associated with them. Second, we distinguish between “fixed-effects parameters”, which are indeed parameters in the statistical model, and “random effects”, which, strictly speaking, are not parameters. As we will see shortly, random effects are unobserved random variables.\nTo make the distinction more concrete, suppose that we wish to model the annual reading test scores for students in a school district and that the covariates recorded with the score include a student identifier and the student’s gender. Both of these are categorical covariates. The levels of the gender covariate, male and female, are fixed. If we consider data from another school district or we incorporate scores from earlier tests, we will not change those levels. On the other hand, the students whose scores we observed would generally be regarded as a sample from the set of all possible students whom we could have observed. Adding more data, either from more school districts or from results on previous or subsequent tests, will increase the number of distinct levels of the student identifier.\nMixed-effects models or, more simply, mixed models are statistical models that incorporate both fixed-effects parameters and random effects. Because of the way that we will define random effects, a model with random effects always includes at least one fixed-effects parameter. Thus, any model with random effects is a mixed model.\nWe characterize the statistical model in terms of two random variables: a \\(q\\)-dimensional vector of random effects represented by the random variable \\({\\mathcal{B}}\\) and an \\(n\\)-dimensional response vector represented by the random variable \\({\\mathcal{Y}}\\). (We use upper-case “script” characters to denote random variables. The corresponding lower-case upright letter denotes a particular value of the random variable, with vector-valued random variable values in boldface.) We observe the value, \\({\\mathbf{y}}\\), of \\({\\mathcal{Y}}\\). We do not observe the value, \\({\\mathbf{b}}\\), of \\({\\mathcal{B}}\\).\nWhen formulating the model we describe the unconditional distribution of \\({\\mathcal{B}}\\) and the conditional distribution, \\(({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\). The descriptions of the distributions involve the form of the distribution and the values of certain parameters. We use the observed values of the response and the covariates to estimate these parameters and to make inferences about them.\nThat’s the big picture. Now let’s make this more concrete by describing a particular, versatile class of mixed models called linear mixed models and by studying a simple example of such a model. First we describe the data in the example."
  },
  {
    "objectID": "intro.html#sec-DyestuffData",
    "href": "intro.html#sec-DyestuffData",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.2 The dyestuff and dyestuff2 data",
    "text": "1.2 The dyestuff and dyestuff2 data\nModels with random effects have been in use for a long time. The first edition of the classic book, Statistical Methods in Research and Production, edited by O.L. Davies, was published in 1947 and contained examples of the use of random effects to characterize batch-to-batch variability in chemical processes. The data from one of these examples are available as the dyestuff data in the MixedModels package. In this section we describe and plot these data and introduce a second example, the dyestuff2 data, described in Box & Tiao (1973).\n\n1.2.1 The dyestuff data\nThe data are described in Davies & Goldsmith (1972, Table 6.3, p. 131), the fourth edition of the book mentioned above, as coming from\n\nan investigation to find out how much the variation from batch to batch in the quality of an intermediate product (H-acid) contributes to the variation in the yield of the dyestuff (Naphthalene Black 12B) made from it. In the experiment six samples of the intermediate, representing different batches of works manufacture, were obtained, and five preparations of the dyestuff were made in the laboratory from each sample. The equivalent yield of each preparation as grams of standard colour was determined by dye-trial.\n\nTo access these data within Julia we must first attach this package, and others that we will use, to our session.\n\n\nCode\nusing AlgebraOfGraphics # high-level graphics\nusing CairoMakie        # graphics back-end\nusing DataFrameMacros   # elegant DataFrame manipulation\nusing DataFrames        # DataFrame implementation\nusing Markdown          # utilities for generating markdown text\nusing MixedModels       # fit and examine mixed-effects models\nusing MixedModelsMakie  # graphics for mixed-effects models\nusing Printf            # formatted printing\nusing ProgressMeter     # progress of optimizer iterations\nusing Random            # random number generation\nusing StatsBase         # basic statistical summaries\n\nusing EmbraceUncertainty: dataset # `dataset` means this one\n\nCairoMakie.activate!(; type=\"svg\")    # Scalable Vector Graphics\nProgressMeter.ijulia_behavior(:clear) # suppress progress output\n\n\nA package must be attached before any of the data sets or functions in the package can be used. If entering this line results in an error report stating that there is no package by one of these names then you must first install the package(s). If you are using Julia version 1.8.0 or later the error report will include instructions on how to do this.\nIn what follows, we will assume that these packages have been installed and attached to the session before any of the code shown has been run.\nThe EmbraceUncertainty package provides access to the datasets used in this book. The directive\nusing EmbraceUncertainty: dataset\nprovides access to the dataset function in that package without having to “qualify” the name by writing EmbraceUncertainty.dataset. The definition of this particular dataset function also provides access to the datasets used in examples and in tests of the MixedModels package.\n\n\n\n\n\n\nAdd metadata to Arrow files for datasets\n\n\n\n\n\n\n\n\n\n\ndyestuff = dataset(:dyestuff)\n\nArrow.Table with 30 rows, 2 columns, and schema:\n :batch  String\n :yield  Int16\n\n\nThe output indicates that this dataset is a Table read from a file in the Arrow data format.\nA Table is a general, but “bare bones”, tabular form defined in the Tables package. This particular table, read from an Arrow file, will be read-only. Often it is convenient to convert the read-only table form to a DataFrame to be able to use the full power of the DataFrames package.\n\ndyestuff = DataFrame(dyestuff)\n\n30×2 DataFrame5 rows omitted\n\n\n\nRow\nbatch\nyield\n\n\n\nString\nInt16\n\n\n\n\n1\nA\n1545\n\n\n2\nA\n1440\n\n\n3\nA\n1440\n\n\n4\nA\n1520\n\n\n5\nA\n1580\n\n\n6\nB\n1540\n\n\n7\nB\n1555\n\n\n8\nB\n1490\n\n\n9\nB\n1560\n\n\n10\nB\n1495\n\n\n11\nC\n1595\n\n\n12\nC\n1550\n\n\n13\nC\n1605\n\n\n⋮\n⋮\n⋮\n\n\n19\nD\n1465\n\n\n20\nD\n1545\n\n\n21\nE\n1595\n\n\n22\nE\n1630\n\n\n23\nE\n1515\n\n\n24\nE\n1635\n\n\n25\nE\n1625\n\n\n26\nF\n1520\n\n\n27\nF\n1455\n\n\n28\nF\n1450\n\n\n29\nF\n1480\n\n\n30\nF\n1445\n\n\n\n\n\n\nThe describe method for a DataFrame provides a concise description of the structure of the data,\n\ndescribe(dyestuff)\n\n2×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nbatch\n\nA\n\nF\n0\nString\n\n\n2\nyield\n1527.5\n1440\n1530.0\n1635\n0\nInt16\n\n\n\n\n\n\nCombining this information with the initial description of the Table we see that the data frame consists of 30 observations of the yield, the response variable, and of the covariate, batch, which is a categorical variable whose levels are character strings.\n\ntypeof(dyestuff.batch)\n\nArrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}\n\n\n(DictEncoded is the Arrow term for a categorical structure where the levels form a dictionary or lookup table and the values are stored as indices into this lookup table.)\n\nshow(levels(dyestuff.batch))\n\n[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n\n\nIf the labels for the factor levels are arbitrary, as they are here, we will use letters instead of numbers for the labels. That is, we label the batches as \"A\" through \"F\" rather than 1 through 6. When the labels are letters it is clear that the variable is categorical. When the labels are numbers a categorical covariate can be mistaken for a numeric covariate, with unintended consequences.\nIt is a good practice to apply describe to any data frame the first time you work with it and to check carefully that any categorical variables are indeed represented as factors.\nThe data in a data frame are viewed as a table with columns corresponding to variables and rows to observations. The functions first and last select the first or last few rows of the table.\n\nfirst(dyestuff, 7)\n\n7×2 DataFrame\n\n\n\nRow\nbatch\nyield\n\n\n\nString\nInt16\n\n\n\n\n1\nA\n1545\n\n\n2\nA\n1440\n\n\n3\nA\n1440\n\n\n4\nA\n1520\n\n\n5\nA\n1580\n\n\n6\nB\n1540\n\n\n7\nB\n1555\n\n\n\n\n\n\n\nlast(dyestuff, 7)\n\n7×2 DataFrame\n\n\n\nRow\nbatch\nyield\n\n\n\nString\nInt16\n\n\n\n\n1\nE\n1635\n\n\n2\nE\n1625\n\n\n3\nF\n1520\n\n\n4\nF\n1455\n\n\n5\nF\n1450\n\n\n6\nF\n1480\n\n\n7\nF\n1445\n\n\n\n\n\n\nor we could tabulate the data using groupby and combine from DataFrames.\n\ncombine(groupby(dyestuff, :batch), :yield =&gt; mean, nrow =&gt; :n)\n\n6×3 DataFrame\n\n\n\nRow\nbatch\nyield_mean\nn\n\n\n\nString\nFloat64\nInt64\n\n\n\n\n1\nA\n1505.0\n5\n\n\n2\nB\n1528.0\n5\n\n\n3\nC\n1564.0\n5\n\n\n4\nD\n1498.0\n5\n\n\n5\nE\n1600.0\n5\n\n\n6\nF\n1470.0\n5\n\n\n\n\n\n\nAlthough this table does show us an important property of the data, namely that there are exactly 5 observations on each batch — a property that we will describe by saying that the data are balanced with respect to batch — we usually learn much more about the structure of such data from plots like Figure 1.1\n\n\nCode\nlet\n  sumry = sort!(combine(groupby(dyestuff, :batch), :yield =&gt; mean =&gt; :yield), :yield)\n  mp = mapping(\n    :yield =&gt; \"Yield of dyestuff [g]\",\n    :batch =&gt; sorter(sumry.batch) =&gt; \"Batch of intermediate product\",\n  )\n  draw(\n    (data(dyestuff) * mp * visual(Scatter; marker='○', markersize=12)) +\n    (data(sumry) * mp * visual(Lines));\n    figure=(; resolution=(800, 400)),\n  )\nend\n\n\n\n\n\nFigure 1.1: Yield of dyestuff by batch. The line joins the mean yields.\n\n\n\n\nthan we do from numerical summaries.\nIn Figure 1.1 we can see that there is considerable variability in yield, even for preparations from the same batch, but there is also noticeable batch-to-batch variability. For example, four of the five preparations from batch F produced lower yields than did any of the preparations from batches B, C, and E.\nThis plot, and essentially all the other plots in this book, were created using the Makie package (Danisch & Krumbiegel, 2021).\nIn yet-to-be-written-appendix we review some of the principles of data graphics, such as reordering the levels of the factor by increasing mean response, that enhance the informativeness of the plot. For example, in this plot the levels of batch are sorted by increasing mean yield, to make visual comparisons between batches easier, and the vertical positions are jittered to avoid overplotting of points. (Note that the two lowest yields of samples from batch A are identical.)\n\n\n\n\n\n\nJittering has not yet been added.\n\n\n\n\n\n\nAt this point we will concentrate on the information conveyed by the plot and not on how the plot is created.\nIn Section 1.3 we will use mixed models to quantify the variability in yield between batches. For the time being let us just note that the particular batches used in this experiment are a selection or sample from the set of all batches that we wish to consider. Furthermore, the extent to which one particular batch tends to increase or decrease the mean yield of the process — in other words, the “effect” of that particular batch on the yield — is not as interesting to us as is the extent of the variability between batches. For the purposes of designing, monitoring and controlling a process we want to predict the yield from future batches, taking into account the batch-to-batch variability and the within-batch variability. Being able to estimate the extent to which a particular batch in the past increased or decreased the yield is not usually an important goal for us. We will model the effects of the batches as random effects rather than as fixed-effects parameters.\n\n\n1.2.2 The dyestuff2 data\nThe data are simulated data presented in Box & Tiao (1973, Table 5.1.4, p. 247) where the authors state\n\nThese data had to be constructed for although examples of this sort undoubtedly occur in practice they seem to be rarely published.\n\nThe structure and summary\n\ndyestuff2 = dataset(:dyestuff2)\n\nArrow.Table with 30 rows, 2 columns, and schema:\n :batch  String\n :yield  Float64\n\n\n\ndyestuff2 = DataFrame(dyestuff2)\ndescribe(dyestuff2)\n\n2×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nbatch\n\nA\n\nF\n0\nString\n\n\n2\nyield\n5.6656\n-0.892\n5.365\n13.434\n0\nFloat64\n\n\n\n\n\n\nare intentionally similar to those of the dyestuff data.\nA data plot (Figure 1.2)\n\n\nCode\nlet\n  sumry = sort!(combine(groupby(dyestuff2, :batch), :yield =&gt; mean =&gt; :yield), :yield)\n  mp = mapping(\n    :yield =&gt; \"Simulated yield\",\n    :batch =&gt; sorter(sumry.batch) =&gt; \"Batch\",\n  )\n  draw(\n    (data(dyestuff2) * mp * visual(Scatter; marker='○', markersize=12)) +\n    (data(sumry) * mp * visual(Lines));\n    figure=(; resolution=(800, 400)),\n  )\nend\n\n\n\n\n\nFigure 1.2: Artificial data of yield by batch. The line joins the mean yields.\n\n\n\n\nshows that the batch-to-batch variability in these data is small compared to the within-batch variability.\nIn some approaches to mixed models it can be difficult to fit models to such data. Paradoxically, small “variance components” can be more difficult to estimate than large variance components.\nThe methods we will present are not compromised when estimating small variance components."
  },
  {
    "objectID": "intro.html#sec-fittinglmms",
    "href": "intro.html#sec-fittinglmms",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.3 Fitting linear mixed models",
    "text": "1.3 Fitting linear mixed models\nBefore we formally define a linear mixed model, let’s go ahead and fit models to these data sets using MixedModels. The simplest way to do this is to use the generic fit function with arguments describing the type of model to be fit (i.e. MixedModel), a formula specifying the model and the data on which to evaluate the formula.\nWe will explain the structure of the formula after we have considered an example.\n\n1.3.1 A model for the dyestuff data\nWe fit a model to the data allowing for an overall level of the yield and for an additive random effect for each level of batch.\n\ndsm01 = let\n  form = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, form, dyestuff)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_batch\n\n\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n&lt;1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\nBy default, this compact display of mixed-effects model fits is used in Jupyter notebooks, which are the evaluation engine for quarto books with Julia code chunks. To obtain more information we can print the model, as in\n\nprintln(dsm01)\n\nLinear mixed model fit by maximum likelihood\n yield ~ 1 + (1 | batch)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -163.6635   327.3271   333.3271   334.2501   337.5307\n\nVariance components:\n            Column    Variance Std.Dev.\nbatch    (Intercept)  1388.3332 37.2603\nResidual              2451.2501 49.5101\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────\n              Coef.  Std. Error      z  Pr(&gt;|z|)\n\n\n\n────────────────────────────────────────────────\n(Intercept)  1527.5     17.6946  86.33    &lt;1e-99\n────────────────────────────────────────────────\n\n\nThe call to fit constructs a LinearMixedModel object, evaluates the maximum likelihood parameter estimates, assigns the results to the name dsm01, and displays a summary of the fitted model.\n\n\n1.3.2 Details of the printed display\nThe display of the fitted model has four major sections:\n\na description of the model that was fit\nsome statistics characterizing the model fit\na summary of properties of the random effects and\na summary of the fixed-effects parameter estimates.\n\nWe consider each of these sections in turn.\nThe description section states that this is a linear mixed model in which the parameters have been estimate by maximum likelihood (ML). The model formula is displayed for later reference.\nThe display of a model fit by maximum likelihood provides several model-fit statistics such as Akaike’s Information Criterion (Sakamoto et al., 1986), Schwarz’s Bayesian Information Criterion (Schwarz, 1978), the log-likelihood at the parameter estimates, and negative twice the log-likelihood, which is the estimation criterion transformed to the scale of the deviance. For linear mixed models we refer to -2 loglik as the value of the objective because this is the value that is minimized during the optimization phase of fitting the model. To evaluate the deviance we should subtract the value of this criterion at a saturated or baseline model but it is not clear how to define such a baseline model in these cases.\nHowever, it is still possible to perform likelihood ratio tests of different models fit to the same data using the difference in the minimized objectives, because this difference is the same as the difference in the deviances. (Recall that a ratio of likelihoods corresponds to a difference in log-likelihoods.)\nThe third section is the table of estimates of parameters associated with the random effects. There are two sources of variability in the model we have fit, a batch-to-batch variability in the level of the response and the residual or per-observation variability — also called the within-batch variability. The name “residual” is used in statistical modeling to denote the part of the variability that cannot be explained or modeled with the other terms. It is the variation in the observed data that is “left over” after we have determined the estimates of the parameters in the other parts of the model.\nSome of the variability in the response is associated with the fixed-effects terms. In this model there is only one such term, labeled as the (Intercept). The name “intercept”, which is better suited to models based on straight lines written in a slope/intercept form, should be understood to represent an overall “typical” or mean level of the response in this case. (In case you are wondering about the parentheses around the name (Intercept), they are included so that you can’t accidentally create a variable with a name that conflicts with this name.) The line labeled batch in the random effects table shows that the random effects added to the term, one for each level of the factor batch, are modeled as random variables whose unconditional variance is estimated as 1388.33 g\\(^2\\) in the ML fit. The corresponding standard deviation is 37.26 g.\nNote that the last column in the random effects summary table is the estimate of the variability expressed as a standard deviation rather than as a variance. These values are provided because usually it is easier to visualize standard deviations, which are on the scale of the response, than it is to visualize the magnitude of a variance. The values in this column are a simple re-expression (the square root) of the estimated variances. Do not confuse them with standard errors of the variance estimators, which are not given here. In add-section-reference-here we explain why we do not provide standard errors of variance estimates.\nThe line labeled Residual in this table gives the estimate of the variance of the residuals (also in g\\(^2\\)) and its corresponding standard deviation. The estimated standard deviation of the residuals is 49.5 g.\nThe last line in the random effects table states the number of observations to which the model was fit and the number of levels of any “grouping factors” for the random effects. In this case we have a single random effects term, (1|batch), in the model formula and the grouping factor for that term is batch. There will be a total of six random effects, one for each level of batch.\nThe final part of the printed display gives the estimates and standard errors of any fixed-effects parameters in the model. The only fixed-effects term in the model formula is the 1, denoting a constant which, as explained above, is labeled as (Intercept). The estimate of this parameter is 1527.5 g, which happens to be the mean yield across all the data - a consequence of the experiment being balanced, in the sense that there was the same number of observations for each batch.\nThis coefficient table also includes the standard error of the estimate, which is the estimated standard deviation of the estimator, a z ratio, which is the ratio of the estimate to its standard error, and the probability the absolute value of a standard normal distribution exceeding the absolute value of the z ratio.\nIf the hypothesis that the coefficient is zero is of interest, which is not the case here, then the value in the fourth column is an approximate p-value for the hypothesis test. An alternative measure of precision of the estimate - a coverage interval based on a parametric bootstrap - is presented in Section 1.5\n\n\n1.3.3 A model for the dyestuff2 data\nFitting a similar model to the dyestuff2 data produces an estimate \\(\\widehat{\\sigma}_1=0\\).\n\ndsm02 = let\n  form = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, form, dyestuff2)\nend\nprintln(dsm02)\n\nLinear mixed model fit by maximum likelihood\n yield ~ 1 + (1 | batch)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   -81.4365   162.8730   168.8730   169.7961   173.0766\n\nVariance components:\n            Column   Variance Std.Dev.\nbatch    (Intercept)   0.00000 0.00000\nResidual              13.34610 3.65323\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────\n              Coef.  Std. Error     z  Pr(&gt;|z|)\n───────────────────────────────────────────────\n(Intercept)  5.6656    0.666986  8.49    &lt;1e-16\n───────────────────────────────────────────────\n\n\nAn estimate of \\(0\\) for \\(\\sigma_1\\) does not mean that there is no variation between the groups. Indeed Figure 1.2 shows that there is some small amount of variability between the groups. The estimate, \\(\\widehat{\\sigma}_1=0\\), simply indicates that the level of “between-group” variability is not sufficient to warrant incorporating random effects in the model.\nThis point is worth reiterating. An estimate of zero for a variance component does not imply that there is no variability between the groups. There will always be variability between groups that is induced by the per-observation variability. If we arbitrarily divided 30 observations from a homogeneous distribution into six groups of five observations, which is likely the way that this sample was simulated, the sample averages would inherit a level of variability from the original sample. What is being estimated in the variance component for batch is the excess variability between groups beyond that induced by the residual variability.\nIn other words, an estimate \\(\\widehat{\\sigma}_1=0\\) is not inconsistent with the model. The important point to take away from this example is that we must allow for the estimates of variance components to be zero.\nWe describe such a model as being degenerate, in the sense that the estimated distribution of the random effects is a degenerate distribution, representing a point mass at zero. It corresponds to a linear model in which we have removed the random effects associated with batch.\nDegenerate models can and do occur in practice. Even when the final fitted model is not degenerate, we must allow for such models when determining the parameter estimates through numerical optimization.\nTo reiterate, this model can be reduced to a linear model because the random effects are inert, in the sense that they have a variance of zero."
  },
  {
    "objectID": "intro.html#sec-modelformulation",
    "href": "intro.html#sec-modelformulation",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.4 Model formulation",
    "text": "1.4 Model formulation\nA common way of writing the statistical model being fit in dsm01 and dsm02 is as an expression for the i’th observation in the j’th batch \\[\ny_{i,j}=\\mu+b_j+\\epsilon_{i,j},\\quad i = 1,\\dots,5;\\;j=1,\\dots,6\n\\] with the further specification that the per-observation noise terms, \\(\\epsilon_{i,j}\\), are independently and identically distributed as \\({\\mathcal{N}}(0, \\sigma^2)\\) and the random effects, \\(b_j\\), are independently and identically distributed as \\({\\mathcal{N}}(0, \\sigma_1^2)\\). The three parameters in the model are the overall mean, \\(\\mu\\), the standard deviation of the random effects, \\(\\sigma_1\\), and the standard deviation of the per-observation noise, \\(\\sigma\\).\nGeneralizing this formulation to unbalanced data sets and more complex models can become unwieldy. For the range of models shown in this book it is more convenient to use a matrix-vector representation in which the random effects are described as a \\(q\\)-dimensional multivariate Gaussian random variable \\[\n{\\mathcal{B}}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma_1^2{\\mathbf{I}}) .\n\\tag{1.1}\\] The multivariate Gaussian distribution is described in more detail in Section B.2.3. At this point the important thing to know is that Equation 1.1 describes a vector of independent Gaussian random variables, each of which has mean \\(0\\) and variance \\(\\sigma_1^2\\).\nThe random effects are associated with the observations through a model matrix, \\({\\mathbf{Z}}\\). For a model with \\(n\\) observations and a total of \\(q\\) random effects, \\({\\mathbf{Z}}\\) will be of size \\(n\\times q\\). Furthermore, it is sparse - meaning that most of the elements of the matrix are zeros. In this case, \\({\\mathbf{Z}}\\) is the indicator matrix for the batch covariate.\n\nZdsm01 = Int.(Matrix(only(dsm01.reterms)))\n\n30×6 Matrix{Int64}:\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  1  0  0  0\n 0  0  1  0  0  0\n ⋮              ⋮\n 0  0  0  1  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n\n\nThe expression to produce that output first extracted the random-effects terms information from dsm01, extracted the term for the first grouping factor, checking that there is only one, converted it to a matrix and converted the matrix to integers to save on space when printing.\nIn cases like this we will often print out the transpose of the model matrix to save more space\n\nZdsm01'\n\n6×30 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  1  1  1  1  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  1  1  1  1  1  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  1  1  1     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  1  1  1  1  1  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  1  1  1  1  1\n\n\nor, even more compact, as a sparse matrix pattern\n\nsparse(Zdsm01')\n\n6×30 SparseArrays.SparseMatrixCSC{Int64, Int64} with 30 stored entries:\n⎡⠉⠉⠑⠒⠒⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⎤\n⎣⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠤⠤⢄⣀⣀⎦\n\n\nThe fixed-effects parameters, of which there is only one here, are gathered into a vector, \\({\\boldsymbol{\\beta}}\\), of length \\(p\\), and multiplied by another model matrix, \\({\\mathbf{X}}\\), of size \\(n\\times p\\).\n\ndsm01.β\n\n1-element Vector{Float64}:\n 1527.4999999999989\n\n\n\nInt.(dsm01.X')\n\n1×30 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n\n\nA linear model without random effects can be described as \\({\\mathcal{Y}}\\sim{\\mathcal{N}}({\\mathbf{X}}{\\boldsymbol{\\beta}}, \\sigma^2{\\mathbf{I}})\\). That is, the expected value of the response vector is the linear predictor, \\({\\mathbf{X}}{\\boldsymbol{\\beta}}\\), and the covariance matrix of the multivariate Gaussian distribution for the response is \\(\\sigma^2{\\mathbf{I}}\\), corresponding to independent, constant variance per-observation noise terms.\nFor the linear mixed model, we describe the conditional distribution of the response, given a particular value, \\({\\mathbf{b}}\\), of the random effects as multivariate Gaussian with mean determined by a linear predictor expression involving both the assumed random effects value, \\({\\mathbf{b}}\\), and the fixed-effects parameter vector, \\({\\boldsymbol{\\beta}}\\), \\[\n({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\sim{\\mathcal{N}}({\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\mathbf{b}},\\sigma^2{\\mathbf{I}}) .\n\\]\nThe complete model can be written as \\[\n\\begin{aligned}\n({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})&\\sim{\\mathcal{N}}({\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\mathbf{b}},\\sigma^2{\\mathbf{I}})\\\\\n{\\mathcal{B}}&\\sim{\\mathcal{N}}(\\mathbf{0},{\\boldsymbol{\\Sigma}}_\\theta) .\n\\end{aligned}\n\\tag{1.2}\\]\nFrom these two distributions, the joint distribution of \\({\\mathcal{Y}}\\) and \\({\\mathcal{B}}\\), which is also multivariate Gaussian, can be determined and from that the likelihood for the parameters, given the observed value \\({\\mathbf{y}}\\) of \\({\\mathcal{Y}}\\).\nDetails on the derivation and evaluation of the log-likelihood are given in Section B.7. At this point the important results are that the profiled log-likelihood for models dsm01 and dsm02 can be evaluated directly (i.e. without an iterative optimization) from a single parameter, \\(\\theta=\\sigma_1/\\sigma\\).\nFurthermore, this profiled log-likelihood can be evaluated from a matrix of \\({\\mathbf{X}}'{\\mathbf{X}}\\)-like products, \\[\n{\\mathbf{A}}=\n\\begin{bmatrix}\n{\\mathbf{Z}}'{\\mathbf{Z}}& {\\mathbf{Z}}'{\\mathbf{X}}& {\\mathbf{Z}}'{\\mathbf{y}}\\\\\n{\\mathbf{X}}'{\\mathbf{Z}}& {\\mathbf{X}}'{\\mathbf{X}}& {\\mathbf{X}}'{\\mathbf{y}}\\\\\n{\\mathbf{y}}'{\\mathbf{Z}}& {\\mathbf{y}}'{\\mathbf{X}}& {\\mathbf{y}}'{\\mathbf{y}}\n\\end{bmatrix}\n\\] which is stored in three pieces\n\nfirst(dsm01.A)\n\n6×6 LinearAlgebra.Diagonal{Float64, Vector{Float64}}:\n 5.0   ⋅    ⋅    ⋅    ⋅    ⋅ \n  ⋅   5.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅    ⋅   5.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅    ⋅   5.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅    ⋅   5.0\n\n\n\ndsm01.A[2]\n\n2×6 Matrix{Float64}:\n    5.0     5.0     5.0     5.0     5.0     5.0\n 7525.0  7640.0  7820.0  7490.0  8000.0  7350.0\n\n\n\nlast(dsm01.A)\n\n2×2 Matrix{Float64}:\n    30.0  45825.0\n 45825.0      7.01129e7\n\n\nThe first, diagonal matrix is \\({\\mathbf{Z}}'{\\mathbf{Z}}\\). A simple, scalar random effects term like (1|batch) in the formula for models dsm01 and dsm02 results in \\({\\mathbf{Z}}\\) being the indicator matrix for the levels of the grouping factor and, hence, in a diagonal \\({\\mathbf{Z}}'{\\mathbf{Z}}\\), whose diagonal elements are the frequencies of the levels. Thus this block shows that there are exactly 5 observations on each of the 6 batches.\nThe second, rectangular matrix is \\(\\begin{bmatrix} {\\mathbf{Z}}'{\\mathbf{X}}\\\\ {\\mathbf{Z}}'{\\mathbf{y}}\\end{bmatrix}\\). Again, the fact that \\({\\mathbf{Z}}\\) is the indicator matrix for the levels of the grouping factor and that \\({\\mathbf{X}}\\) is a single column of 1’s results in the frequencies of the levels of the factor in the first row of this block. The second row of this block consists of the sums of the yields for each batch.\nThe third, square symmetric matrix is \\[\n\\begin{bmatrix}\n{\\mathbf{X}}'{\\mathbf{X}}& {\\mathbf{X}}'{\\mathbf{y}}\\\\\n{\\mathbf{y}}'{\\mathbf{X}}& {\\mathbf{y}}'{\\mathbf{y}}\n\\end{bmatrix}\n\\]\nThe parameters enter into the computation via a relative covariance factor, \\({\\boldsymbol{\\Lambda}}_{\\boldsymbol\\theta}\\), which is the \\(6\\times 6\\) matrix, \\(\\theta\\,{\\mathbf{I}}_6\\), in this case.\nThe method hinges on being able to evaluate efficiently the Cholesky factor (see Section B.2.2 for details) of \\[\n\\begin{bmatrix}\n{\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'Z}{\\boldsymbol{\\Lambda}}_\\theta+{\\mathbf{I}}&\n{\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'X} & {\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'y} \\\\\n\\mathbf{X'Z}{\\boldsymbol{\\Lambda}}_\\theta & \\mathbf{X'X} & \\mathbf{X'y} \\\\\n\\mathbf{y'Z}{\\boldsymbol{\\Lambda}}_\\theta & \\mathbf{y'X} & \\mathbf{y'y}\n\\end{bmatrix}\n\\]\nThe optimal value of \\(\\theta\\) for model dsm01 is\n\nonly(dsm01.θ)\n\n0.7525806394967323\n\n\nproducing a lower Cholesky factor of\n\nsparseL(dsm01; full=true)\n\n8×8 SparseArrays.SparseMatrixCSC{Float64, Int32} with 21 stored entries:\n    1.95752      ⋅           ⋅       …      ⋅           ⋅          ⋅ \n     ⋅          1.95752      ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅          1.95752         ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅       …     1.95752      ⋅          ⋅ \n    1.92228     1.92228     1.92228        1.92228     2.79804     ⋅ \n 2893.03     2937.24     3006.45        2825.75     4274.01     271.178"
  },
  {
    "objectID": "intro.html#sec-furtherassess",
    "href": "intro.html#sec-furtherassess",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.5 Variability of parameter estimates",
    "text": "1.5 Variability of parameter estimates\nThe parameter estimates in a statistical model represent our “best guess” at the unknown values of the model parameters and, as such, are important results in statistical modeling. However, they are not the whole story. Statistical models characterize the variability in the data and we should assess the effect of this variability on the precision of the parameter estimates and of predictions made from the model.\nOne method of assessing the variability in the parameter estimates is through a parametric bootstrap, a process where a large number of data sets are simulated from the assumed model using the estimated parameter values as the “true” parameter values. The distribution of the parameter estimators is inferred from the distribution of the parameter estimates from these generated data sets.\nThis method is well-suited to Julia code because refitting an existing model to a simulated data set can be very fast.\nFor methods that involve simulation, it is best to initialize a random number generator to a known state so that the “random” sample can be reproduced if desired. Beginning with Julia v1.7.0 the default random number generator is the Xoshiro generator, which we initialize to an arbitrary, but reproducible, value.\nThe object returned by a call to parametricbootstrap has a somewhat complex internal structure to allow for the ability to simulate from complex models while still maintaining a comparatively small storage footprint. To examine the distribution of the parameter estimates we extract a table of all the estimated parameters and convert it to a DataFrame.\n\nconst hide_progress = true # hide the progress bar when sampling\nRandom.seed!(4321234)      # random number generator\ndsm01samp = parametricbootstrap(10_000, dsm01; hide_progress)\ndsm01pars = DataFrame(dsm01samp.allpars)\nfirst(dsm01pars, 7)\n\n7×5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n1515.34\n\n\n2\n1\nσ\nbatch\n(Intercept)\n10.1817\n\n\n3\n1\nσ\nresidual\nmissing\n54.2804\n\n\n4\n2\nβ\nmissing\n(Intercept)\n1502.81\n\n\n5\n2\nσ\nbatch\n(Intercept)\n35.3983\n\n\n6\n2\nσ\nresidual\nmissing\n48.3692\n\n\n7\n3\nβ\nmissing\n(Intercept)\n1529.74\n\n\n\n\n\n\n\n\n\n\n\n\nSwitch to Table(dsm01samp.tbl) formulation\n\n\n\n\n\n\n\nlast(dsm01pars, 7)\n\n7×5 DataFrame\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n9998\nσ\nresidual\nmissing\n42.8227\n\n\n2\n9999\nβ\nmissing\n(Intercept)\n1519.04\n\n\n3\n9999\nσ\nbatch\n(Intercept)\n11.0336\n\n\n4\n9999\nσ\nresidual\nmissing\n58.4295\n\n\n5\n10000\nβ\nmissing\n(Intercept)\n1474.47\n\n\n6\n10000\nσ\nbatch\n(Intercept)\n23.4016\n\n\n7\n10000\nσ\nresidual\nmissing\n37.5372\n\n\n\n\n\n\nPlots of the bootstrap estimates for individual parameters are obtained by extracting subsets of the rows of this dataframe using subset methods from the DataFrames package or the @subset macro from the DataFramesMacros package. For example,\n\nβdf = @subset(dsm01pars, :type == \"β\")\n\n10000×5 DataFrame9975 rows omitted\n\n\n\nRow\niter\ntype\ngroup\nnames\nvalue\n\n\n\nInt64\nString\nString?\nString?\nFloat64\n\n\n\n\n1\n1\nβ\nmissing\n(Intercept)\n1515.34\n\n\n2\n2\nβ\nmissing\n(Intercept)\n1502.81\n\n\n3\n3\nβ\nmissing\n(Intercept)\n1529.74\n\n\n4\n4\nβ\nmissing\n(Intercept)\n1537.34\n\n\n5\n5\nβ\nmissing\n(Intercept)\n1516.77\n\n\n6\n6\nβ\nmissing\n(Intercept)\n1522.16\n\n\n7\n7\nβ\nmissing\n(Intercept)\n1523.27\n\n\n8\n8\nβ\nmissing\n(Intercept)\n1527.52\n\n\n9\n9\nβ\nmissing\n(Intercept)\n1518.53\n\n\n10\n10\nβ\nmissing\n(Intercept)\n1538.92\n\n\n11\n11\nβ\nmissing\n(Intercept)\n1518.34\n\n\n12\n12\nβ\nmissing\n(Intercept)\n1537.79\n\n\n13\n13\nβ\nmissing\n(Intercept)\n1533.77\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9989\n9989\nβ\nmissing\n(Intercept)\n1542.72\n\n\n9990\n9990\nβ\nmissing\n(Intercept)\n1522.55\n\n\n9991\n9991\nβ\nmissing\n(Intercept)\n1517.65\n\n\n9992\n9992\nβ\nmissing\n(Intercept)\n1570.07\n\n\n9993\n9993\nβ\nmissing\n(Intercept)\n1531.39\n\n\n9994\n9994\nβ\nmissing\n(Intercept)\n1542.33\n\n\n9995\n9995\nβ\nmissing\n(Intercept)\n1513.89\n\n\n9996\n9996\nβ\nmissing\n(Intercept)\n1550.58\n\n\n9997\n9997\nβ\nmissing\n(Intercept)\n1517.84\n\n\n9998\n9998\nβ\nmissing\n(Intercept)\n1513.58\n\n\n9999\n9999\nβ\nmissing\n(Intercept)\n1519.04\n\n\n10000\n10000\nβ\nmissing\n(Intercept)\n1474.47\n\n\n\n\n\n\nWe begin by examining density plots constructed using the AlgebraOfGraphics package. (In general we “fold” the code used to generate plots as it tends to have considerable detail that may distract from the plot itself. You can check the details by clicking on the “Code” button in the HTML version of the plot or in the quarto files on the github repository for this book.)\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"β\")) *\n  mapping(\n    :value =&gt; \"Bootstrap samples of β\";\n    color=(:names =&gt; \"Names\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.3: Kernel density plot of bootstrap fixed-effects parameter estimates from dsm01\n\n\n\n\n\n\n\n\n\n\nUse :compact=true when interpolating numeric values\n\n\n\n\n\nFind a way to use :compact=true when interpolating numeric values into the text\n\n\n\n\n\nThe distribution of the estimates of β₁ is more-or-less a Gaussian (or “normal”) shape, with a mean value of 1528.021212 which is close to the estimated β₁ of 1527.500000.\nSimilarly the standard deviation of the simulated β values, 17.71143325897102 is close to the standard error of the parameter, 17.694552727788437.\n\n\nIn other words, the estimator of the fixed-effects parameter in this case behaves as we would expect. The estimates are approximately normally distributed centered about the “true” parameter value with a standard deviation given by the standard error of the parameter.\nThe situation is different for the estimates of the standard deviation parameters, \\(\\sigma\\) and \\(\\sigma_1\\), as shown in Figure 1.4.\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"σ\")) *\n  mapping(\n    :value =&gt; \"Bootstrap samples of σ\";\n    color=(:group =&gt; \"Group\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.4: Kernel density plot of bootstrap variance-component parameter estimates from model dsm01\n\n\n\n\nThe estimator for the residual standard deviation, \\(\\sigma\\), is approximately normally distributed but the estimator for \\(\\sigma_1\\), the standard deviation of the batch random effects is bimodal (i.e. has two “modes” or local maxima).\n\n\nThere is one peak around the “true” value for the simulation, 37.26034309083298, and another peak at zero.\n\n\nThe apparent distribution of the estimates of \\(\\sigma_1\\) in Figure 1.4 is being distorted by the method of approximating the density. A kernel density estimate approximates a probability density from a finite sample by blurring or smearing the positions of the sample values according to a kernel such as a narrow Gaussian distribution (see the linked article for details). In this case the distribution of the estimates is a combination of a continuous distribution and a spike or point mass at zero as shown in a histogram, Figure 1.5.\n\n\n\n\n\n\nAdjust the alpha in multiple histograms\n\n\n\n\n\nUse a lower alpha in the colors for multiple histograms so the bars behind another color are more visible\n\n\n\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"σ\")) *\n  mapping(\n    :value =&gt; \"Bootstrap parameter estimates of σ\";\n    color=(:group =&gt; \"Group\"),\n  ) *\n  AlgebraOfGraphics.histogram(; bins=80);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.5: Histogram of bootstrap variance-components as standard deviations from model dsm01\n\n\n\n\nNearly 1000 of the 10,000 bootstrap fits are “singular” in that one (or more) of the estimated unconditional distribution(s) of the random effects are degenerate. For this model the only way singularity can occur is for \\(\\sigma_1\\) to be zero.\n\ncount(issingular(dsm01samp))\n\n993\n\n\nThe distribution of the estimator of \\(\\sigma_1\\) with this point mass at zero is not at all like a Gaussian or normal distribution. This is why characterizing the uncertainty of these parameter estimates with a standard error is misleading. Quoting an estimate and a standard error for the estimate is only meaningful if these values adequately characterize the distribution of the values of the estimator.\nIn many cases standard errors are quoted for estimates of the variance components, \\(\\sigma_1^2\\) and \\(\\sigma^2\\), whose distribution (Figure 1.6) in the bootstrap sample is even less like a normal or Gaussian distribution than is the distribution of estimates of \\(\\sigma_1\\).\n\n\nCode\ndraw(\n  data(@transform(@subset(dsm01pars, :type == \"σ\"), abs2(:value))) *\n  mapping(\n    :value_abs2 =&gt; \"Bootstrap sample of estimates of σ²\",\n    color=:group,\n  ) *\n  AlgebraOfGraphics.histogram(; bins=200);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.6: Histogram of bootstrap variance-components from model dsm01\n\n\n\n\nThe approach of creating coverage intervals from a bootstrap sample of parameter estimates, described in the next section, does accomodate non-normal distributions.\n\n1.5.1 Confidence intervals on the parameters\nA bootstrap sample of parameter estimates also provides us with a method of creating bootstrap coverage intervals, which can be regarded as confidence intervals on the parameters.\nFor, say, a 95% coverage interval based on 10,000 parameter estimates we determine an interval that contains 95% (9,500, in this case) of the estimated parameter values.\nThere are many such intervals. For example, from the sorted parameter values we could return the interval from the smallest up to the 9,500th largest, or from the second smallest up to the 9,501 largest, and so on.\nThe shortestcovint method returns the shortest of all these potential intervals which will correspond to the interval with the highest empirical density.\n\nDataFrame(shortestcovint(dsm01samp))\n\n3×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n1493.65\n1563.5\n\n\n2\nσ\nbatch\n(Intercept)\n0.0\n54.3062\n\n\n3\nσ\nresidual\nmissing\n35.3499\n62.6078\n\n\n\n\n\n\nFor the (Intercept) fixed-effects parameter the interval is similar to an “asymptotic” or Wald interval constructed as the estimate plus or minus two standard errors.\n\nshow(only(dsm01.β) .+ [-2, 2] * only(dsm01.stderror))\n\n[1492.110894544422, 1562.8891054555756]\n\n\nThe interval on the residual standard deviation, \\(\\sigma\\), is reasonable, given that there are only 30 observations, but the interval of the standard deviation of the random effects, \\(\\sigma_1\\), extends all the way down to zero.\n\n\n1.5.2 Tracking the progress of the iterations\nThe optional argument, thin=1, in a call to fit causes all the values of \\(\\boldsymbol\\theta\\) and the corresponding value of objective from the iterative optimization to be stored in the optsum property.\n\ndsm01trace = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  thin=1,\n)\nDataFrame(dsm01trace.optsum.fitlog)\n\n18×2 DataFrame\n\n\n\nRow\n1\n2\n\n\n\nArray…\nFloat64\n\n\n\n\n1\n[1.0]\n327.767\n\n\n2\n[1.75]\n331.036\n\n\n3\n[0.25]\n330.646\n\n\n4\n[0.97619]\n327.695\n\n\n5\n[0.928569]\n327.566\n\n\n6\n[0.833327]\n327.383\n\n\n7\n[0.807188]\n327.353\n\n\n8\n[0.799688]\n327.347\n\n\n9\n[0.792188]\n327.341\n\n\n10\n[0.777188]\n327.333\n\n\n11\n[0.747188]\n327.327\n\n\n12\n[0.739688]\n327.329\n\n\n13\n[0.752777]\n327.327\n\n\n14\n[0.753527]\n327.327\n\n\n15\n[0.752584]\n327.327\n\n\n16\n[0.752509]\n327.327\n\n\n17\n[0.752591]\n327.327\n\n\n18\n[0.752581]\n327.327\n\n\n\n\n\n\n\n\nHere the algorithm converges after 18 function evaluations to a profiled deviance of 327.327059881143 at θ = 0.7525806394967323."
  },
  {
    "objectID": "intro.html#sec-assessRE",
    "href": "intro.html#sec-assessRE",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.6 Assessing the random effects",
    "text": "1.6 Assessing the random effects\nWhat are sometimes called the BLUPs (or best linear unbiased predictors) of the random effects, \\({\\mathcal{B}}\\), are the mode (location of the maximum probability density) of the conditional distribution, \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\), evaluated at the parameter estimates and the observed response vector, \\({\\mathbf{y}}\\). Although BLUP is an appealing acronym, we don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional mode” to describe this value.\nThese values are often considered as some sort of “estimates” of the random effects. It can be helpful to think of them this way but it can also be misleading. As we have stated, the random effects are not, strictly speaking, parameters — they are unobserved random variables. We don’t estimate the random effects in the same sense that we estimate parameters. Instead, we consider the conditional distribution of \\({\\mathcal{B}}\\) given the observed data, \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\).\nBecause the unconditional distribution, \\({\\mathcal{B}}\\sim{\\mathcal{N}}(\\mathbf{0},\\Sigma_\\theta)\\) is continuous, the conditional distribution, \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\) will also be continuous. In general, the mode of a probability density is the point of maximum density, so the phrase “conditional mode” refers to the point at which this conditional density is maximized. Because this definition relates to the probability model, the values of the parameters are assumed to be known. In practice, of course, we don’t know the values of the parameters (if we did there would be no purpose in forming the parameter estimates), so we use the estimated values of the parameters to evaluate the conditional modes.\nThose who are familiar with the multivariate Gaussian distribution may recognize that, because both \\({\\mathcal{B}}\\) and \\(({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\) are multivariate Gaussian, \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\) will also be multivariate Gaussian and the conditional mode will also be the conditional mean of \\({\\mathcal{B}}\\), given \\({\\mathcal{Y}}={\\mathbf{y}}\\). This is the case for a linear mixed model but it does not carry over to other forms of mixed models. In the general case all we can say about \\(\\tilde{{\\mathbf{b}}}\\) is that they maximize a conditional density, which is why we use the term “conditional mode” to describe these values. We will only use the term “conditional mean” and the symbol, \\({\\boldsymbol{\\mu}}\\), in reference to \\(\\mathrm{E}({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\), which is the conditional mean of \\({\\mathcal{Y}}\\) given \\({\\mathcal{B}}\\), and an important part of the formulation of all types of mixed-effects models.\nThe conditional modes are available as a vector of matrices\n\nonly(dsm01.b)\n\n1×6 Matrix{Float64}:\n -16.6282  0.369516  26.9747  -21.8014  53.5798  -42.4943\n\n\nIn this case the vector consists of a single matrix because there is only one random-effects term, (1|batch), in the model and, hence, only one grouping factor, batch, for the random effects. There is only one row in the matrix because the random-effects term, (1|batch), is a simple, scalar term.\nTo make this more explicit, random-effects terms in the model formula are those that contain the vertical bar (|) character. The variable or expression on the right of the | is the grouping factor for the random effects generated by this term. If the expression on the left of the vertical bar is 1, as it is here, we describe the term as a simple, scalar, random-effects term. The designation “scalar” means there will be exactly one random effect generated for each level of the grouping factor. A simple, scalar term generates a block of indicator columns — the indicators for the grouping factor — in \\({\\mathbf{Z}}\\). Because there is only one random-effects term in this model and because that term is a simple, scalar term, the model matrix \\({\\mathbf{Z}}\\) for this model is the indicator matrix for the levels of batch, as shown earlier in this section.\nIn the next chapter we fit models with multiple simple, scalar terms and, in subsequent chapters, we extend random-effects terms beyond simple, scalar terms. When we have only simple, scalar terms in the model, each term has a unique grouping factor and the elements of the list returned by can be considered as associated with terms or with grouping factors. In more complex models a particular grouping factor may occur in more than one term. In such cases the terms associated with the same grouping factor are internally amalgamated into a single term. Thus internally the random effects are associated with grouping factors, not the terms in the model formula.\nGiven the data, \\({\\mathbf{y}}\\), and the parameter estimates, we can evaluate a measure of the dispersion of \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\). In the case of a linear mixed model, this is the conditional standard deviation, from which we can obtain a prediction interval. A plot of these prediction intervals is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar when there are many levels of the grouping factor.\n\n\nCode\ncaterpillar!(\n  Figure(resolution=(800, 250)),\n  ranefinfo(dsm01, :batch),\n)\n\n\n\n\n\nFigure 1.7: Caterpillar plot of prediction intervals for dsm01 random effects\n\n\n\n\nThe caterpillar function returns a plot with linear spacing of the levels on the y axis. An alternative, the qqcaterpillar function\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 250)),\n  ranefinfo(dsm01, :batch),\n)\n\n\n\n\n\nFigure 1.8: Quantile caterpillar plot of prediction intervals for dsm01 random effects\n\n\n\n\nreturns a plot like Figure 1.8 where the intervals are plotted with vertical spacing corresponding to the quantiles of the standard normal distribution.\nThe caterpillar plot is preferred when there are only a few levels of the grouping factor, as in this case. When there are hundreds or thousands of random effects the qqcaterpillar form is preferred because it focuses attention on the “important few” at the extremes and de-emphasizes the “trivial many” that are close to zero."
  },
  {
    "objectID": "intro.html#sec-stylistic",
    "href": "intro.html#sec-stylistic",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.7 Some stylistic conventions",
    "text": "1.7 Some stylistic conventions\nTo make it easier to recognize the form of the many models that will be fit in this book, we will adopt certain conventions regarding the argument specifications. In particular we will establish the convention of specifying contrasts for any categorical covariates and the possibility of using certain transformations, such as centering and scaling, of numeric covariates.\nThe name contrasts is used in a general sense here to specify certain transformations that are to take place during the process of converting a formula and the structure, or schema, of the data into model matrices.\nThe StatsModels.jl package allows for contrasts to be specified as a key-value dictionary where the keys are symbols and the values are of a type that specializes StatsModels.AbstractContrasts. The MixedModels.jl package provides for a Grouping() contrast and the StandardizedPredictors.jl package allows transformations to be expressed as contrasts.\nFor models dsm01 and dsm02 the only covariate in the model formula is batch, which is a grouping factor for the random effects. Thus the desired contrasts specification is\n\ncontrasts = Dict(:batch =&gt; Grouping())\n\n(Symbols in Julia can be written as a colon followed by the name of the symbol. The colon creates an expression but, if the expression consists of a single name, then the expression is the symbol.)\nIt is best to get into the habit of specifying Grouping() contrasts for any grouping factors in the data. Doing so is not terribly important when the number of levels of the grouping factor is small, as in the examples in this chapter. However, when the number of levels is large, as for some of the models in later chapters, failure to specify Grouping() contrasts can cause memory faults when constructing the numeric represention of the model.\nThere is an advantage in assigning the contrasts dictionary to the name contrasts because a call to fit with the optional, named argument contrasts\n\ndsm01 = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  contrasts=contrasts,\n)\n\ncan be condensed to\n\ndsm01 = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  contrasts,\n)\n\nThat is, if the name of the object to be passed as a named argument is the same as the name of the argument, like contrasts=contrasts, then the name does not need to be repeated. Note that the comma after the positional arguments must be changed to a semicolon in this convention. This is necessary because it indicates that arguments following the semicolon are named arguments, not positional.\nAnother convention we will use is assigning the formula separately from the call to fit as part of a let block. Often the formula can become rather long, sometimes needing multiple lines in the call, and it becomes difficult to keep track of the other arguments. Assigning the formula separately helps in keeping track of the arguments to fit.\nA let block is a way of making temporary assignments that do not affect the global state. An assignment to a variable name inside a let block is local to the block.\nThus dsm01 can be assigned as\n\ndsm01 = let f = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, f, dyestuff; contrasts)\nend"
  },
  {
    "objectID": "intro.html#sec-ChIntroSummary",
    "href": "intro.html#sec-ChIntroSummary",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.8 Chapter summary",
    "text": "1.8 Chapter summary\nA considerable amount of material has been presented in this chapter, especially considering the word “simple” in its title (it’s the model that is simple, not the material). A summary may be in order.\nA mixed-effects model incorporates fixed-effects parameters and random effects, which are unobserved random variables, \\({\\mathcal{B}}\\). In a linear mixed model, both the unconditional distribution of \\({\\mathcal{B}}\\) and the conditional distribution, \\(({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\), are multivariate Gaussian distributions. Furthermore, this conditional distribution is a spherical Gaussian with mean, \\({\\boldsymbol{\\mu}}\\), determined by the linear predictor, \\({\\mathbf{Z}}{\\mathbf{b}}+{\\mathbf{X}}{\\boldsymbol{\\beta}}\\). That is, \\[\n({\\mathcal{Y}}|{\\mathcal{B}}={\\mathbf{b}})\\sim\n{\\mathcal{N}}({\\mathbf{Z}}{\\mathbf{b}}+{\\mathbf{X}}{\\boldsymbol{\\beta}}, \\sigma^2{\\mathbf{I}}_n) .\n\\] The unconditional distribution of \\({\\mathcal{B}}\\) has mean \\(\\mathbf{0}\\) and a parameterized \\(q\\times q\\) variance-covariance matrix, \\(\\Sigma_\\theta\\).\nIn the models we considered in this chapter, \\(\\Sigma_\\theta\\), is a scalar multiple of the identity matrix, \\({\\mathbf{I}}_6\\). This matrix is always a multiple of the identity in models with just one random-effects term that is a simple, scalar term. The reason for introducing all the machinery that we did is to allow for more general model specifications.\nThe maximum likelihood estimates of the parameters are obtained by minimizing the deviance. For linear mixed models we can minimize the profiled deviance, which is a function of \\({\\boldsymbol{\\theta}}\\) only, thereby considerably simplifying the optimization problem.\nTo assess the precision of the parameter estimates we use a parametric bootstrap sample, when feasible. Re-fitting a simple model, such as those shown in this chapter, to a randomly generated response vector is quite fast and it is reasonable to work with bootstrap samples of tens of thousands of replicates. With larger data sets and more complex models, large bootstrap samples of parameter estimates could take much longer.\nPrediction intervals from the conditional distribution of the random effects, given the observed data, allow us to assess the precision of the random effects.\n\n\n\n\nBox, G. E. P., & Tiao, G. C. (1973). Bayesian inference in statistical analysis. Addison-Wesley.\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible high-performance data visualization for Julia. Journal of Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical methods in research and production (4th ed.). Hafner.\n\n\nSakamoto, Y., Ishiguro, M., & Kitagawa, G. (1986). Akaike information criterion statistics (p. 290). Reidel.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6, 461–464."
  },
  {
    "objectID": "multiple.html#sec-crossedre",
    "href": "multiple.html#sec-crossedre",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.1 A model with crossed random effects",
    "text": "2.1 A model with crossed random effects\nOne of the areas in which the methods in the MixedModels.jl package are particularly effective is in fitting models to cross-classified data where several factors have random effects associated with them. For example, in many experiments in psychology a reaction time for each of a group of subjects exposed to some or all of a group of stimuli or items is measured. If the subjects are considered to be a sample from a population of subjects and the items are a sample from a population of items, then it would make sense to associate random effects with both these factors.\nIn the past it was difficult to fit mixed models with multiple, crossed grouping factors to large, possibly unbalanced, data sets. The methods in MixedModels.jl are able to do this. To introduce the methods let us first consider a small, balanced data set with crossed grouping factors.\n\n2.1.1 The penicillin data\nThe data are derived from Table 6.6, p. 144 of Davies & Goldsmith (1972) where they are described as coming from an investigation to\n\nassess the variability between samples of penicillin by the B. subtilis method. In this test method a bulk-innoculated nutrient agar medium is poured into a Petri dish of approximately 90 mm. diameter, known as a plate. When the medium has set, six small hollow cylinders or pots (about 4 mm. in diameter) are cemented onto the surface at equally spaced intervals. A few drops of the penicillin solutions to be compared are placed in the respective cylinders, and the whole plate is placed in an incubator for a given time. Penicillin diffuses from the pots into the agar, and this produces a clear circular zone of inhibition of growth of the organisms, which can be readily measured. The diameter of the zone is related in a known way to the concentration of penicillin in the solution.\n\nAs with the dyestuff data, we examine the structure\n\npenicillin = dataset(:penicillin)\n\nArrow.Table with 144 rows, 3 columns, and schema:\n :plate     String\n :sample    String\n :diameter  Int8\n\n\nand a summary\n\npenicillin = DataFrame(penicillin)\ndescribe(penicillin)\n\n3×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nplate\n\na\n\nx\n0\nString\n\n\n2\nsample\n\nA\n\nF\n0\nString\n\n\n3\ndiameter\n22.9722\n18\n23.0\n27\n0\nInt8\n\n\n\n\n\n\nof the data, then plot it\n\n\nCode\n\"\"\"\n    _meanrespfrm(df, :resp::Symbol, :grps::Symbol; sumryf::Function=mean)\n\nReturns a `DataFrame` created from df with the levels of `grps` reordered according to\n`combine(groupby(df, grps), resp =&gt; sumryf)` and this summary DataFrame, also with the\nlevels of `grps` reordered.\n\"\"\"\nfunction _meanrespfrm(\n  df,\n  resp::Symbol,\n  grps::Symbol;\n  sumryf::Function=mean,\n)\n  # ensure the relevant columns are types that Makie can deal with \n  df = transform(\n    df,\n    resp =&gt; Array,\n    grps =&gt; CategoricalArray;\n    renamecols=false,\n  )\n  # create a summary table by mean resp\n  sumry =\n    sort!(combine(groupby(df, grps), resp =&gt; sumryf =&gt; resp), resp)\n  glevs = string.(sumry[!, grps])   # group levels in ascending order of mean resp\n  levels!(df[!, grps], glevs)\n  levels!(sumry[!, grps], glevs)\n  return df, sumry\nend\n\nlet\n  df, _ = _meanrespfrm(penicillin, :diameter, :plate)\n  sort!(df, [:plate, :sample])\n\n  mp = mapping(\n    :diameter =&gt; \"Diameter of inhibition zone [mm]\",\n    :plate =&gt; \"Plate\";\n    color=:sample,\n  )\n  draw(\n    data(df) * mp * visual(ScatterLines; marker='○', markersize=12),\n  )\nend\n\n\n\n\n\nFigure 2.1: Diameter of inhibition zone by plate and sample. Plates are ordered by increasing mean response.\n\n\n\n\nThe variation in the diameter is associated with the plates and with the samples. Because each plate is used only for the six samples shown here we are not interested in the contributions of specific plates as much as we are interested in the variation due to plates, and in assessing the potency of the samples after accounting for this variation. Thus, we will use random effects for the plate factor. We will also use random effects for the sample factor because, as in the dyestuff example, we are more interested in the sample-to-sample variability in the penicillin samples than in the potency of a particular sample.\nIn this experiment each sample is used on each plate. We say that the plate and sample factors are crossed, as opposed to nested factors, which we will describe in the next section. By itself, the designation “crossed” just means that the factors are not nested. If we wish to be more specific, we could describe these factors as being completely crossed, which means that we have at least one observation for each combination of a level of plate and a level of sample. We can see this in Figure 2.1. Alternatively, because there are moderate numbers of levels in these factors, we could also check by a cross-tabulation of these factors.\nLike the dyestuff data, the factors in the penicillin data are balanced. That is, there are exactly the same number of observations on each plate and for each sample and, furthermore, there is the same number of observations on each combination of levels. In this case there is exactly one observation for each combination of sample and plate. We would describe the configuration of these two factors as an unreplicated, completely balanced, crossed design.\nIn general, balance is a desirable but precarious property of a data set. We may be able to impose balance in a designed experiment but we typically cannot expect that data from an observation study will be balanced. Also, as anyone who analyzes real data soon finds out, expecting that balance in the design of an experiment will produce a balanced data set is contrary to Murphy’s law. That’s why statisticians allow for missing data. Even when we apply each of the six samples to each of the 24 plates, something could go wrong for one of the samples on one of the plates, leaving us without a measurement for that combination of levels and thus an unbalanced data set.\n\n\n2.1.2 A model for the penicillin data\nA model incorporating random effects for both the plate and the sample is straightforward to specify — we include simple, scalar random effects terms for both these factors.\n\npnm01 = let f = @formula diameter ~ 1 + (1 | plate) + (1 | sample)\n  fit(MixedModel, f, penicillin; contrasts)\nend\n\nMinimizing 44    Time: 0:00:00 ( 9.21 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_plate\nσ_sample\n\n\n\n\n(Intercept)\n22.9722\n0.7446\n30.85\n&lt;1e-99\n0.8456\n1.7706\n\n\nResidual\n0.5499\n\n\n\n\n\n\n\n\n\n\nThis model display indicates that the sample-to-sample variability has the greatest contribution, then plate-to-plate variability and finally the “residual” variability that cannot be attributed to either the sample or the plate. These conclusions are consistent with what we see in the data plot (Figure 2.1).\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 480)),\n  ranefinfo(pnm01, :plate),\n)\n\n\n\n\n\nFigure 2.2: Conditional modes and 95% prediction intervals of random effects for plate in model pnm01\n\n\n\n\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 160)),\n  ranefinfo(pnm01, :sample),\n)\n\n\n\n\n\nFigure 2.3: Conditional modes and 95% prediction intervals of random effects for sample in model pnm01\n\n\n\n\nThe prediction intervals on the random effects (Figure 2.2 and Figure 2.3) confirm that the conditional distribution of the random effects for plate much less variability than does the conditional distribution of the random effects for sample. (Note that the horizontal scales on these two plots are different.) However, the conditional distribution of the random effect for a particular sample, say sample F, has less variability than the conditional distribution of the random effect for a particular plate, say plate m. That is, the lines in Figure 2.3 are wider than the lines in Figure 2.2, even after taking the different axis scales into account. This is because the conditional distribution of the random effect for a particular sample depends on 24 responses while the conditional distribution of the random effect for a particular plate depends on only 6 responses.\nIn Chapter 1 we saw that a model with a single, simple, scalar random-effects term generated a random-effects model matrix, \\({\\mathbf{Z}}\\), that is the matrix of indicators of the levels of the grouping factor. When we have multiple, simple, scalar random-effects terms, as in model pnm01, each term generates a matrix of indicator columns and these sets of indicators are concatenated to form the model matrix \\({\\mathbf{Z}}\\) whose transpose is\n\nvcat(transpose.(sparse.(pnm01.reterms))...)\n\n30×144 SparseArrays.SparseMatrixCSC{Float64, Int32} with 288 stored entries:\n⎡⠉⠙⠒⠒⠒⠤⠤⠤⢄⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠒⠒⠒⠦⠤⠤⢤⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠓⠒⠒⠢⠤⠤⠤⣀⣀⣀⡀⠀⎥\n⎣⠑⢌⠢⡜⢦⠓⢌⠢⡘⢦⠓⢌⠢⡘⢦⠳⢌⠢⡑⢦⠳⢌⠢⡑⢦⠳⡌⠢⡑⢤⠳⡌⠢⡑⢤⠳⡜⠢⡙⢍⎦\n\n\nThe relative covariance factor, \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\), for this model is block diagonal, with two blocks, one of size 24 and one of size 6, each of which is a multiple of the identity. The diagonal elements of the two blocks are \\(\\theta_1\\) and \\(\\theta_2\\), respectively. The numeric values of these parameters can be obtained as\n\npnm01.θ'\n\n1×2 adjoint(::Vector{Float64}) with eltype Float64:\n 1.53758  3.21975\n\n\nThe first parameter is the relative standard deviation of the random effects for plate, which has the value \\(0.845565/0.549933=1.53758\\) at convergence, and the second is the relative standard deviation of the sample random effects (\\(1.770648/0.549933=3.21975\\)).\nBecause \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\) is diagonal, the pattern of non-zeros in \\({\\boldsymbol{\\Lambda}}_{\\boldsymbol{\\theta}}'{\\mathbf{Z}}'{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{\\boldsymbol{\\theta}}+{\\mathbf{I}}\\) will be the same as that in \\({\\mathbf{Z}}'{\\mathbf{Z}}\\). The sparse Cholesky factor, \\({\\mathbf{L}}\\), is lower triangular and has non-zero elements in the lower right hand corner in positions where \\({\\mathbf{Z}}'{\\mathbf{Z}}\\) has systematic zeros.\n\nsparseL(pnm01)\n\n30×30 SparseArrays.SparseMatrixCSC{Float64, Int32} with 189 stored entries:\n⎡⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⎥\n⎢⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⎥\n⎣⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠓⎦\n\n\nWe say that “fill-in” has occurred when forming the sparse Cholesky decomposition. In this case there is a relatively minor amount of fill but in other cases there can be a substantial amount of fill. The computational methods are tuned to reduce the amount of fill.\n\n\n2.1.3 Precision of parameter estimates in the Pencillin model\nA parametric bootstrap sample of the parameter estimates\n\n\nCode\nbsrng = Random.seed!(9876789)\npnm01samp = parametricbootstrap(bsrng, 10_000, pnm01; hide_progress)\npnm01pars = DataFrame(pnm01samp.allpars);\n\n\ncan be used to create shortest 95% coverage intervals for the parameters in the model.\n\nDataFrame(shortestcovint(pnm01samp))\n\n4×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n21.4993\n24.4402\n\n\n2\nσ\nplate\n(Intercept)\n0.586766\n1.0985\n\n\n3\nσ\nsample\n(Intercept)\n0.627596\n2.55106\n\n\n4\nσ\nresidual\nmissing\n0.475331\n0.61701\n\n\n\n\n\n\nAs for model dsm01 the bootstrap parameter estimates of the fixed-effects parameter have approximately a “normal” or Gaussian shape, as shown in the kernel density plot (Figure 2.4)\n\n\nCode\ndraw(\n  data(@subset(pnm01pars, :type == \"β\")) *\n  mapping(\n    :value =&gt; \"Bootstrap samples of β\";\n    color=(:names =&gt; \"Names\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.4: Parametric bootstrap estimates of fixed-effects parameters in model pnm01\n\n\n\n\nand the shortest coverage interval on this parameter is close to the Wald interval\n\n\nCode\nshow(only(pnm01.beta) .+ [-2, 2] * only(pnm01.stderror))\n\n\n[21.483030272252066, 24.461414172196214]\n\n\nThe densities of the variance-components, on the scale of the standard deviation parameters, are diffuse but do not exhibit point masses at zero.\n\n\nCode\ndraw(\n  data(@subset(pnm01pars, :type == \"σ\")) *\n  mapping(\n    :value =&gt; \"Bootstrap samples of σ\";\n    color=(:group =&gt; \"Group\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.5: Parametric bootstrap estimates of variance components in model pnm01\n\n\n\n\nThe lack of precision in the estimate of \\(\\sigma_2\\), the standard deviation of the random effects for sample, is a consequence of only having 6 distinct levels of the sample factor. The plate factor, on the other hand, has 24 distinct levels. In general it is more difficult to estimate a measure of spread, such as the standard deviation, than to estimate a measure of location, such as a mean, especially when the number of levels of the factor is small. Six levels are about the minimum number required for obtaining sensible estimates of standard deviations for simple, scalar random effects terms."
  },
  {
    "objectID": "multiple.html#sec-NestedRE",
    "href": "multiple.html#sec-NestedRE",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.2 A model with nested random effects",
    "text": "2.2 A model with nested random effects\nIn this section we again consider a simple example, this time fitting a model with nested grouping factors for the random effects.\n\n2.2.1 The pastes data\nThe third example from Davies & Goldsmith (1972, Table 6.5, p. 138) is described as coming from\n\ndeliveries of a chemical paste product contained in casks where, in addition to sampling and testing errors, there are variations in quality between deliveries …As a routine, three casks selected at random from each delivery were sampled and the samples were kept for reference. …Ten of the delivery batches were sampled at random and two analytical tests carried out on each of the 30 samples.\n\nThe structure and summary of the data object are\n\npastes = dataset(:pastes)\n\nArrow.Table with 60 rows, 3 columns, and schema:\n :batch     String\n :cask      String\n :strength  Float64\n\n\n\npastes =\n  @transform(DataFrame(pastes), :sample = string(:batch, :cask))\ndescribe(pastes)\n\n4×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nbatch\n\nA\n\nJ\n0\nString\n\n\n2\ncask\n\na\n\nc\n0\nString\n\n\n3\nstrength\n60.0533\n54.2\n59.3\n66.0\n0\nFloat64\n\n\n4\nsample\n\nAa\n\nJc\n0\nString\n\n\n\n\n\n\nAs stated in the description in Davies & Goldsmith (1972), there are 30 samples, three from each of the 10 delivery batches. We have created a sample factor by concatenating the label of the batch factor with ‘a’, ‘b’ or ‘c’ to distinguish the three samples taken from that batch.\nWhen plotting the strength versus batch and cask in the data we should remember that we have two strength measurements on each of the 30 samples. It is tempting to use the cask designation (‘a’, ‘b’ and ‘c’) to determine, say, the plotting symbol within a batch. It would be fine to do this within a batch but the plot would be misleading if we used the same symbol for cask ‘a’ in different batches. There is no relationship between cask ‘a’ in batch ‘A’ and cask ‘a’ in batch ‘B’. The labels ‘a’, ‘b’ and ‘c’ are used only to distinguish the three samples within a batch; they do not have a meaning across batches.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=8, height=8)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\npp &lt;- $pastes\npp &lt;- within(pp, bb &lt;- reorder(batch, strength))\nprint(\n  dotplot(sample ~ strength | bb, pp, pch = 21, strip = FALSE,\n    strip.left = TRUE, layout = c(1, 10),\n    scales = list(y = list(relation = \"free\")),\n    ylab = \"Sample within batch\", type = c(\"p\", \"a\"),\n    xlab = \"Paste strength\", jitter.y = TRUE)\n)\n\"\"\";\n\n\n\n\n\nFigure 2.6: Strength of paste preparations according to sample within batch\n\n\n\n\nIn Figure 2.6 we plot the two strength measurements on each of the samples within each of the batches and join up the average strength for each sample. The perceptive reader will have noticed that the levels of the factors on the vertical axis in this figure, and in Figure 1.1, and Figure 2.1, have been reordered according to increasing average response. In all these cases there is no inherent ordering of the levels of the covariate such as batch or plate. Rather than confuse our interpretation of the plot by determining the vertical displacement of points according to a random ordering, we impose an ordering according to increasing mean response. This allows us to more easily check for structure in the data, including undesirable characteristics like increasing variability of the response with increasing mean level of the response.\nIn Figure 2.6 we order the samples within each batch separately then order the batches according to increasing mean strength.\nFigure 2.6 shows considerable variability in strength between samples relative to the variability within samples. There is some indication of variability between batches, in addition to the variability induced by the samples, but not a strong indication of a batch effect. For example, batches I and D, with low mean strength relative to the other batches, each contained one sample (I:b and D:c, respectively) that had high mean strength relative to the other samples. Also, batches H and C, with comparatively high mean batch strength, contain samples H:a and C:a with comparatively low mean sample strength. In Section 2.2.4 we will examine the need for incorporating batch-to-batch variability, in addition to sample-to-sample variability, in the statistical model.\n\n\n2.2.2 Nested Factors\nBecause each level of sample occurs with one and only one level of batch we say that sample is nested within batch. Some presentations of mixed-effects models, especially those related to multilevel modeling (Rasbash et al., 2000) or hierarchical linear models (Raudenbush & Bryk, 2002), leave the impression that one can only define random effects with respect to factors that are nested. This is the origin of the terms “multilevel”, referring to multiple, nested levels of variability, and “hierarchical”, also invoking the concept of a hierarchy of levels. To be fair, both those references do describe the use of models with random effects associated with non-nested factors, but such models tend to be treated as a special case.\nThe blurring of mixed-effects models with the concept of multiple, hierarchical levels of variation results in an unwarranted emphasis on “levels” when defining a model and leads to considerable confusion. It is perfectly legitimate to define models having random effects associated with non-nested factors. The reasons for the emphasis on defining random effects with respect to nested factors only are that such cases do occur frequently in practice and that some of the computational methods for estimating the parameters in the models can only be easily applied to nested factors.\nThis is not the case for the methods used MixedModels.jl. Indeed there is nothing special done for models with random effects for nested factors. When random effects are associated with multiple factors exactly the same computational methods are used whether the factors form a nested sequence or are partially crossed or are completely crossed.\nThere is, however, one aspect of nested grouping factors that we should emphasize, which is the possibility of a factor that is implicitly nested within another factor. Suppose, for example, that the factor sample had been defined as having three levels instead of 30 with the implicit assumption that sample is nested within batch. It may seem silly to try to distinguish 30 different batches with only three levels of a factor but, unfortunately, data are frequently organized and presented like this, especially in text books. The factor cask in the data is exactly such an implicitly nested factor. If we cross-tabulate cask and batch we get the impression that these factors are crossed, not nested. If we know that the cask should be considered as nested within the batch then we should create a new categorical variable giving the batch-cask combination, which is exactly what the sample factor is.\nIn a small data set like we can quickly detect a factor being implicitly nested within another factor and take appropriate action. In a large data set, such as from a multi-center study, it is often assumed that, say, subject identifiers are unique to each center. Frequently this is not the case. Especially when dealing with large data sets, assumptions about nested identifiers should be checked carefully.\n\n\n2.2.3 Fitting a model with nested random effects\nFitting a model with simple, scalar random effects for nested factors is done in exactly the same way as fitting a model with random effects for crossed grouping factors. We include random-effects terms for each factor, as in\n\npsm01 = let f = @formula strength ~ 1 + (1 | sample) + (1 | batch)\n  fit(MixedModel, f, pastes; contrasts)\nend\n\nMinimizing 35    Time: 0:00:00 ( 5.51 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_sample\nσ_batch\n\n\n\n\n(Intercept)\n60.0533\n0.6421\n93.52\n&lt;1e-99\n2.9041\n1.0951\n\n\nResidual\n0.8234\n\n\n\n\n\n\n\n\n\n\nNot only is the model specification similar for nested and crossed factors, the internal calculations are performed according to the methods described in for each model type. Comparing the patterns in the matrices \\({\\boldsymbol{\\Lambda}}\\), \\({\\mathbf{Z}}'{\\mathbf{Z}}\\) and \\({\\mathbf{L}}\\) and the block structure\n\nsparseL(psm01; full=true)\n\n42×42 SparseArrays.SparseMatrixCSC{Float64, Int32} with 153 stored entries:\n⎡⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠤⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠉⠑⠒⠤⢄⣀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠒⠤⢄⣀⠀⠀⠀⠑⢄⠀⎥\n⎣⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠓⎦\n\n\n\nBlockDescription(psm01)\n\n\n\n\nrows\nsample\nbatch\nfixed\n\n\n\n\n30\nDiagonal\n\n\n\n\n10\nSparse\nDiagonal\n\n\n\n2\nDense\nDense\nDense\n\n\n\n\n\nof psm01 to that of pnm01 shows that models with nested factors produce simple repeated structures along the diagonal of the sparse Cholesky factor, \\({\\mathbf{L}}\\). This type of structure has the desirable property that there is no “fill-in” during calculation of the Cholesky factor. In other words, the number of non-zeros in \\({\\mathbf{L}}\\) is the same as the number of non-zeros in the lower triangle of the matrix being factored, \\({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}}\\) (which, because \\({\\boldsymbol{\\Lambda}}\\) is diagonal, has the same structure as \\({\\mathbf{Z}}'{\\mathbf{Z}}\\)).\nFill-in of the Cholesky factor is not an important issue when we have a few dozen random effects, as we do here. It is an important issue when we have millions of random effects in complex configurations, as has been the case in some of the models that have been fit using MixedModels.\n\n\n2.2.4 Assessing parameter estimates in psm01\nThe parameter estimates are: \\(\\widehat{\\sigma_1}\\), the standard deviation of the random effects for sample; \\(\\widehat{\\sigma_2}\\), the standard deviation of the random effects for batch; \\(\\widehat{\\sigma}\\), the standard deviation of the residual noise term; and \\(\\widehat{\\beta_1}=\\), the overall mean response, which is labeled (Intercept) in these models.\nThe estimated standard deviation for sample is nearly three times as large as that for batch, which confirms what we saw in Figure 2.6. Indeed our conclusion from Figure 2.6 was that there may not be a significant batch-to-batch variability in addition to the sample-to-sample variability.\nPlots of the prediction intervals of the random effects (Figure 2.7)\n\n\nCode\ncaterpillar!(\n  Figure(resolution=(800, 300)),\n  ranefinfo(psm01, :batch),\n)\n\n\n\n\n\nFigure 2.7: Plot of batch prediction intervals from psm01\n\n\n\n\nconfirm this impression in that all the prediction intervals for the random effects for contain zero.\nFurthermore, kernel density estimates from a parametric bootstrap sample of the estimated standard deviations of the random effects and residuals\n\nRandom.seed!(4567654)\npsm01samp = parametricbootstrap(10_000, psm01; hide_progress)\npsm01pars = DataFrame(psm01samp.allpars);\n\n\n\nCode\ndraw(\n  data(@subset(psm01pars, :type == \"σ\")) *\n  mapping(\n    :value =&gt; \"Bootstrap samples of σ\";\n    color=(:group =&gt; \"Group\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.8: Kernel density plots of bootstrap estimates of σ for model psm01\n\n\n\n\nBecause there are several indications that \\(\\sigma_2\\) could reasonably be zero, resulting in a simpler model incorporating random effects for only, we perform a statistical test of this hypothesis.\n\n\n2.2.5 Testing \\(H_0:\\sigma_2=0\\) versus \\(H_a:\\sigma_2&gt;0\\)\nOne of the many famous statements attributed to Albert Einstein is “Everything should be made as simple as possible, but not simpler.” In statistical modeling this principal of parsimony is embodied in hypothesis tests comparing two models, one of which contains the other as a special case. Typically, one or more of the parameters in the more general model, which we call the alternative hypothesis, is constrained in some way, resulting in the restricted model, which we call the null hypothesis. Although we phrase the hypothesis test in terms of the parameter restriction, it is important to realize that we are comparing the quality of fits obtained with two nested models. That is, we are not assessing parameter values per se; we are comparing the model fit obtainable with some constraints on parameter values to that without the constraints. Because the more general model, \\(H_a\\), must provide a fit that is at least as good as the restricted model, \\(H_0\\), our purpose is to determine whether the change in the quality of the fit is sufficient to justify the greater complexity of model \\(H_a\\). This comparison is often reduced to a p-value, which is the probability of seeing a difference in the model fits as large as we did, or even larger, when, in fact, \\(H_0\\) is adequate. Like all probabilities, a p-value must be between 0 and 1. When the p-value for a test is small (close to zero) we prefer the more complex model, saying that we “reject \\(H_0\\) in favor of \\(H_a\\)”. On the other hand, when the p-value is not small we “fail to reject \\(H_0\\)”, arguing that there is a non-negligible probability that the observed difference in the model fits could reasonably be the result of random chance, not the inherent superiority of the model \\(H_a\\). Under these circumstances we prefer the simpler model, \\(H_0\\), according to the principal of parsimony.\nThese are the general principles of statistical hypothesis tests. To perform a test in practice we must specify the criterion for comparing the model fits, the method for calculating the p-value from an observed value of the criterion, and the standard by which we will determine if the p-value is “small” or not. The criterion is called the test statistic, the p-value is calculated from a reference distribution for the test statistic, and the standard for small p-values is called the level of the test.\nIn Section 1.3.2 we referred to likelihood ratio tests (LRTs) for which the test statistic is the difference in the deviance. That is, the LRT statistic is \\(d_0-d_a\\) where \\(d_a\\) is the deviance in the more general (\\(H_a\\)) model fit and \\(d_0\\) is the deviance in the constrained (\\(H_0\\)) model. An approximate reference distribution for an LRT statistic is the \\(\\chi^2_\\nu\\) distribution where \\(\\nu\\), the degrees of freedom, is determined by the number of constraints imposed on the parameters of \\(H_a\\) to produce \\(H_0\\).\nThe restricted model fit\n\npsm02 = let f = @formula strength ~ 1 + (1 | sample)\n  fit(MixedModel, f, pastes; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_sample\n\n\n\n\n(Intercept)\n60.0533\n0.5765\n104.16\n&lt;1e-99\n3.1037\n\n\nResidual\n0.8234\n\n\n\n\n\n\n\n\n\nis compared to model psm01 as\n\nMixedModels.likelihoodratiotest(psm02, psm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nstrength ~ 1 + (1 | sample)\n3\n248\n\n\n\n\n\nstrength ~ 1 + (1 | sample) + (1 | batch)\n4\n248\n0\n1\n0.5234\n\n\n\n\n\nwhich provides a p-value of 52%. Because typical standards for “small” p-values are 5% or 1%, a p-value over 50% would not be considered significant at any reasonable level.\nWe do need to be cautious in quoting this p-value, however, because the parameter value being tested, \\(\\sigma_2=0\\), is on the boundary of set of possible values, \\(\\sigma_2\\ge 0\\), for this parameter. The argument for using a \\(\\chi^2_1\\) distribution to calculate a p-value for the change in the deviance does not apply when the parameter value being tested is on the boundary. As shown in Pinheiro & Bates (2000, Sect. 2.5), the p-value from the \\(\\chi^2_1\\) distribution will be “conservative” in the sense that it is larger than a simulation-based p-value would be. In the worst-case scenario the \\(\\chi^2\\)-based p-value will be twice as large as it should be but, even if that were true, an effective p-value of 26% would not cause us to reject \\(H_0\\) in favor of \\(H_a\\).\n\n\n2.2.6 Assessing the reduced model, psm02\nComparing the coverage intervals for models psm01 and psm02\n\n\nCode\nDataFrame(shortestcovint(psm01samp))\n\n\n4×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n58.7732\n61.3025\n\n\n2\nσ\nsample\n(Intercept)\n1.93499\n3.61949\n\n\n3\nσ\nbatch\n(Intercept)\n0.0\n2.0894\n\n\n4\nσ\nresidual\nmissing\n0.608904\n1.02399\n\n\n\n\n\n\n\n\nCode\npsm02samp = parametricbootstrap(\n  Random.seed!(9753579),\n  10_000,\n  psm02;\n  hide_progress,\n)\nDataFrame(shortestcovint(psm02samp))\n\n\n3×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n58.9201\n61.1886\n\n\n2\nσ\nsample\n(Intercept)\n2.24908\n3.83207\n\n\n3\nσ\nresidual\nmissing\n0.61145\n1.02324\n\n\n\n\n\n\nThe confidence intervals on \\(\\sigma\\) and \\(\\beta_0\\) are similar for the two models. The confidence interval on \\(\\sigma_1\\) is slightly wider and incorporates larger values in model psm02 than in model psm01, because the variability that is attributed to batch in psm01 is incorporated into the variability due to sample in psm02."
  },
  {
    "objectID": "multiple.html#sec-partially",
    "href": "multiple.html#sec-partially",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.3 A model with partially crossed random effects",
    "text": "2.3 A model with partially crossed random effects\nEspecially in observational studies with multiple grouping factors, the configuration of the factors frequently ends up neither nested nor completely crossed. We describe such situations as having partially crossed grouping factors for the random effects.\nStudies in education, in which test scores for students over time are also associated with teachers and schools, usually result in partially crossed grouping factors. If students with scores in multiple years have different teachers for the different years, the student factor cannot be nested within the teacher factor. Conversely, student and teacher factors are not expected to be completely crossed. To have complete crossing of the student and teacher factors it would be necessary for each student to be observed with each teacher, which would be unusual. A longitudinal study of thousands of students with hundreds of different teachers inevitably ends up partially crossed.\nIn this section we consider an example with thousands of students and instructors where the response is the student’s evaluation of the instructor’s effectiveness. These data, like those from most large observational studies, are quite unbalanced.\n\n2.3.1 The insteval data\nThe data are from a special evaluation of lecturers by students at the Swiss Federal Institute for Technology–Zürich (ETH–Zürich), to determine who should receive the “best-liked professor” award. These data have been slightly simplified and identifying labels have been removed, so as to preserve anonymity.\nThe variables\n\ninsteval = dataset(:insteval)\n\nArrow.Table with 73421 rows, 7 columns, and schema:\n :s        String\n :d        String\n :dept     String\n :studage  String\n :lectage  String\n :service  String\n :y        Int8\n\n\n\ndescribe(DataFrame(insteval))\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ns\n\nS0001\n\nS2972\n0\nString\n\n\n2\nd\n\nI0001\n\nI2160\n0\nString\n\n\n3\ndept\n\nD01\n\nD15\n0\nString\n\n\n4\nstudage\n\n2\n\n8\n0\nString\n\n\n5\nlectage\n\n1\n\n6\n0\nString\n\n\n6\nservice\n\nN\n\nY\n0\nString\n\n\n7\ny\n3.20574\n1\n3.0\n5\n0\nInt8\n\n\n\n\n\n\nhave somewhat cryptic names. Factor s designates the student and d the instructor. The factor dept is the department for the course and service indicates whether the course was a service course taught to students from other departments.\nAlthough the response, y, is on a scale of 1 to 5,\n\n\nCode\ndraw(\n  data(Table(response=1:5, count=counts(insteval.y))) *\n  mapping(:response =&gt; \"Rating\", :count =&gt; \"Count\") *\n  visual(BarPlot);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.9: Histogram of instructor ratings in the insteval data\n\n\n\n\nit is sufficiently diffuse to warrant treating it as if it were a continuous response.\nAt this point we will fit models that have random effects for student, instructor, and department (or the combination of department and service) to these data.\n\niem01 = let f = @formula y ~ 1 + (1 | s) + (1 | d) + (1 | dept)\n  fit(MixedModel, f, insteval; contrasts)\nend\n\nMinimizing 97    Time: 0:00:00 ( 8.41 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_s\nσ_d\nσ_dept\n\n\n\n\n(Intercept)\n3.2519\n0.0279\n116.70\n&lt;1e-99\n0.3264\n0.5173\n0.0773\n\n\nResidual\n1.1777\n\n\n\n\n\n\n\n\n\n\n\nAll three estimated standard deviations of the random effects are less than \\(\\widehat{\\sigma}\\), with \\(\\widehat{\\sigma}_3\\), the estimated standard deviation of the random effects for the dept, less than one-tenth the estimated residual standard deviation.\nIt is not surprising that zero is within most of the prediction intervals on the random effects for this factor (Figure 2.10).\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 400)), ranefinfo(iem01, :dept))\n\n\n\n\n\nFigure 2.10: Prediction intervals on random effects for department in model iem01\n\n\n\n\nHowever, the p-value for the LRT of \\(H_0:\\sigma_3=0\\) versus \\(H_a:\\sigma_3&gt;0\\)\n\niem02 = let f = @formula y ~ 1 + (1 | s) + (1 | d)\n  fit(MixedModel, f, insteval; contrasts)\nend\nMixedModels.likelihoodratiotest(iem02, iem01)\n\nMinimizing 47    Time: 0:00:00 ( 7.04 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\ny ~ 1 + (1 | s) + (1 | d)\n4\n237778\n\n\n\n\n\ny ~ 1 + (1 | s) + (1 | d) + (1 | dept)\n5\n237770\n8\n1\n0.0043\n\n\n\n\n\nis highly significant. That is, we have very strong evidence that we should reject \\(H_0\\) in favor of \\(H_a\\).\nThe seeming inconsistency of these conclusions is due to the large sample size (\\(n=73421\\)). When a model is fit to a large sample even the most subtle of differences can be highly “statistically significant”. The researcher or data analyst must then decide if these terms have practical significance, beyond the apparent statistical significance.\nThe large sample size also helps to assure that the parameters have good normal approximations. We could profile this model fit but doing so would take a very long time and, in this particular case, the analysts are more interested in a model that uses fixed-effects parameters for the instructors.\n\n\n2.3.2 Structure of L for model iem01\nBefore leaving this model we examine the sparse Cholesky factor, \\({\\mathbf{L}}\\), which is of size \\(4116\\times4116\\).\n\nL = sparseL(iem01; full=true)\n\n4116×4116 SparseArrays.SparseMatrixCSC{Float64, Int32} with 741328 stored entries:\n⎡⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⎥\n⎢⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⎥\n⎢⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⎥\n⎣⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⎦\n\n\nEven as a sparse matrix L requires a considerable amount of memory, about 9 MB,\n\n(Base.summarysize(L), Base.summarysize(collect(L)))\n\n(8912564, 135531688)\n\n\nbut as a triangular dense matrix it requires over 10 times as much (about 135 MB). There are \\(4116^2\\) elements in the triangular matrix, each of which requires 8 bytes of storage.\n\n\n2.3.3 Effect of service courses\nIt is sometimes felt that it is more difficult to achieve favorable ratings from students in a service course (i.e. a course taught to students majoring in another program) as opposed to students taking a course in their major.\nThere are several ways in which service can be incorporated in a model like this. The simplest approach is to add service to the fixed-effects specification\n\niem03 =\n  let f = @formula y ~ 1 + service + (1 | s) + (1 | d) + (1 | dept)\n    fit(MixedModel, f, insteval; contrasts)\n  end\n\nMinimizing 112   Time: 0:00:01 ( 9.39 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_s\nσ_d\nσ_dept\n\n\n\n\n(Intercept)\n3.2826\n0.0284\n115.54\n&lt;1e-99\n0.3255\n0.5150\n0.0785\n\n\nservice: Y\n-0.0926\n0.0134\n-6.92\n&lt;1e-11\n\n\n\n\n\nResidual\n1.1775\n\n\n\n\n\n\n\n\n\n\n\nIn model iem03 the effect of service is considered to be constant across departments and is modeled with a single fixed-effects parameter, which is the difference in a typical rating in a service course to a non-service course. This parameter also affects the interpretation of the (Intercept) coefficient. With the service term in the model the (Intercept) becomes a typical rating at the reference level (i.e. non-service or service: N) because the default coding for the service term is zero at the reference level and one for service: Y.\nThe coding can be changed by specifying a non-default contrast for service. For example, the EffectsCoding and HelmerCoding contrasts will both assign -1 to the first level (N in this case) and +1 to the second level (Y).\n\n\n2.3.4 “Best-liked”\nA qqcaterpillar plot of the instructor (i.e. factor d in the data) random effects, Figure 2.11,\n\n\nCode\nqqcaterpillar!(\n  Figure(; resolution=(800, 650)),\n  ranefinfo(iem01, :d),\n)\n\n\n\n\n\nFigure 2.11: Caterpillar plot of the instructor BLUPS for model iem01 versus standard normal quantiles\n\n\n\n\nshows the differences in precision due to different numbers of observations for different instructors.\n\n\nCode\ndraw(\n  data(combine(groupby(DataFrame(insteval), :d), nrow =&gt; :n)) *\n  mapping(:n =&gt; \"Number of observations\") *\n  AlgebraOfGraphics.histogram(; bins=410);\n  figure=(; resolution=(800, 600)),\n)\n\n\n\n\n\nFigure 2.12: Histogram of the number of observations per instructor in the insteval data\n\n\n\n\nThe precision of the conditional distributions of the random effects, as measured by the width of the intervals, varies considerably between instructors.\nWe can determine that instructor I1258 has the largest mean of the conditional distributions of the random effects\n\nlast(\n  sort(DataFrame(ranefinfotable(ranefinfo(iem01, :d))), :cmode),\n  5,\n)\n\n5×4 DataFrame\n\n\n\nRow\nname\nlevel\ncmode\ncstddev\n\n\n\nString\nString\nFloat64\nFloat64\n\n\n\n\n1\n(Intercept)\nI0066\n1.03151\n0.105472\n\n\n2\n(Intercept)\nI0193\n1.04027\n0.190826\n\n\n3\n(Intercept)\nI0844\n1.05502\n0.169914\n\n\n4\n(Intercept)\nI1866\n1.06625\n0.123321\n\n\n5\n(Intercept)\nI1258\n1.17205\n0.188496\n\n\n\n\n\n\nbut the conditional distribution of this random effect clearly overlaps significantly with others."
  },
  {
    "objectID": "multiple.html#sec-MultSummary",
    "href": "multiple.html#sec-MultSummary",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.4 Chapter summary",
    "text": "2.4 Chapter summary\nA simple, scalar random effects term in an model formula is of the form , where is an expression whose value is the grouping factor of the set of random effects generated by this term. Typically, F is simply the name of a factor, as in most of the examples in this chapter. However, the grouping factor can be the value of an expression, such as in the last example.\nBecause simple, scalar random-effects terms can differ only in the description of the grouping factor we refer to configurations such as crossed or nested as applying to the terms or to the random effects, although it is more accurate to refer to the configuration as applying to the grouping factors.\nA model formula can include several such random effects terms. Because configurations such as nested or crossed or partially crossed grouping factors are a property of the data, the specification in the model formula does not depend on the configuration. We simply include multiple random effects terms in the formula specifying the model.\nOne apparent exception to this rule occurs with implicitly nested factors, in which the levels of one factor are only meaningful within a particular level of the other factor. In the pastes data, levels of the factor cask are only meaningful within a particular level of the factor batch. A model formula of would result in a fitted model that did not appropriately reflect the sources of variability in the data. Following the simple rule that the factor should be defined so that distinct experimental or observational units correspond to distinct levels of the factor will avoid such ambiguity.\nFor convenience, a model with multiple, nested random-effects terms can be specified as which internally is re-expressed as We will avoid terms of the form , preferring instead an explicit specification with simple, scalar terms based on unambiguous grouping factors.\nThe insteval data, described in Section 2.3.1, illustrate some of the characteristics of the real data to which mixed-effects models are now fit. There is a large number of observations associated with several grouping factors; two of which, student and instructor, have a large number of levels and are partially crossed. Such data are common in sociological and educational studies but until recently it has been very difficult to fit models that appropriately reflect such a structure. Much of the literature on mixed-effects models leaves the impression that multiple random effects terms can only be associated with nested grouping factors. The resulting emphasis on hierarchical or multilevel configurations is an artifact of the computational methods used to fit the models, not the models themselves.\nThe parameters of the models fit to small data sets have properties similar to those for the models in the previous chapter. That is, profile-based confidence intervals on the fixed-effects parameter, \\(\\beta_0\\), are symmetric about the estimate but overdispersed relative to those that would be calculated from a normal distribution and the logarithm of the residual standard deviation, \\(\\log(\\sigma)\\), has a good normal approximation. Profile-based confidence intervals for the standard deviations of random effects (\\(\\sigma_1\\), \\(\\sigma_2\\), etc.) are symmetric on a logarithmic scale except for those that could be zero.\nAnother observation from the last example is that, for data sets with a very large numbers of observations, a term in a model may be “statistically significant” even when its practical significance is questionable.\n\n\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical methods in research and production (4th ed.). Hafner.\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in S and S-Plus (pp. xvi + 528). New York, NY: Springer. https://doi.org/10.1007/b98882\n\n\nRasbash, J., Browne, W., Goldstein, H., Yang, M., & Plewis, I. (2000). A user’s guide to MLwiN. Multilevel Models Project, Institute of Education, University of London.\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (2nd ed.). Sage."
  },
  {
    "objectID": "longitudinal.html#the-elstongrizzle-data",
    "href": "longitudinal.html#the-elstongrizzle-data",
    "title": "3  Models for Longitudinal Data",
    "section": "3.1 The elstongrizzle data",
    "text": "3.1 The elstongrizzle data\nData from a dental study measuring the lengths of the ramus bone (mm) in 20 boys at 8, 8.5, 9, and 9.5 years of age were reported in Elston & Grizzle (1962) and in Davis (2002).\n\nelstongrizzle = dataset(:elstongrizzle)\n\nArrow.Table with 80 rows, 3 columns, and schema:\n :Subj  String\n :time  Float64\n :resp  Float64\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"      =&gt; \"Ramus bone lengths of boys from 8 to 9.5 years of age\"\n  \"references\" =&gt; \"@davis2002, Table 3.1, p. 52 and @elstongrizzle1962\"\n  \"sourcefile\" =&gt; \"EG.DAT\"\n\n\nConverting the table to a data frame provides the description\n\n\n3×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nSubj\n\nS01\n\nS20\n0\nString\n\n\n2\ntime\n8.75\n8.0\n8.75\n9.5\n0\nFloat64\n\n\n3\nresp\n50.0875\n45.0\n49.65\n55.5\n0\nFloat64\n\n\n\n\n\n\nA common way of plotting such longitudinal data is response versus time on a single axis with the observations for each individual joined by a line, Figure 3.1 (see also Figure 3.2, p. 52 of Davis (2002)).\n\n\nCode\ndraw(\n  data(egdf) *\n  mapping(\n    :time =&gt; \"Age (yr)\",\n    :resp =&gt; \"Ramus bone length (mm)\",\n    color=:Subj,\n  ) *\n  (visual(Scatter) + visual(Lines)),\n)\n\n\n\n\n\nFigure 3.1: Length of ramus bone versus age for a sample of 20 boys.\n\n\n\n\nUnfortunately, unless there are very few subjects, such figures, sometimes called “spaghetti plots”, are difficult to decipher.\nA preferred alternative is to plot response versus time with each subject’s data in a separate panel (Figure 3.2).\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=8, height=7)\nR\"\"\"\nprint(\n  lattice::xyplot(\n    resp ~ time|Subj,\n    $egdf,\n    type=c(\"g\",\"p\",\"r\"),\n    aspect=\"xy\",\n    index.cond=function(x,y) coef(lm(y ~ x)) %*% c(1,8),\n    xlab=\"Age (yr)\",\n    ylab=\"Ramus bone length (mm)\",\n    ),\n  )\n\"\"\";\n\n\n\n\n\nFigure 3.2: Length of ramus bone versus age for a sample of 20 boys. The panels are ordered rowwise, starting at the bottom left, by increasing bone length at age 8.\n\n\n\n\nTo aid comparisons between subjects the axes are the same in every panel and the order of the panels is chosen systematically - in Figure 3.2 the order is by increasing bone length at 8 years of age. This ordering makes it easier to examine the patterns in the rate of increase versus the initial bone length.\nAnd there doesn’t seem to be a strong relationship. Some subjects, e.g. S03 and S04, with shorter initial bone lengths have low growth rates. Others, e.g. S10 and S20, have low initial bone lengths and a high growth rate. Similarly, S06 and S11 have longer initial bone lengths and a low growth rate while S07 and S11 have longer initial bone lengths and a high growth rate."
  },
  {
    "objectID": "longitudinal.html#random-effects-for-slope-and-intercept",
    "href": "longitudinal.html#random-effects-for-slope-and-intercept",
    "title": "3  Models for Longitudinal Data",
    "section": "3.2 Random effects for slope and intercept",
    "text": "3.2 Random effects for slope and intercept\nAlthough it seems that there isn’t a strong correlation between initial bone length and growth rate in these data, a model with an overall linear trend and possibly correlated random effects for intercept and slope by subject estimates a strong negative correlation (-0.97) between these random effects.\n\ncontrasts[:Subj] = Grouping()\negm01 = let f = @formula resp ~ 1 + time + (1 + time | Subj)\n  fit(MixedModel, f, egdf; contrasts)\nend\nprintln(egm01)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time + (1 + time | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n\n\n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n            Column    Variance Std.Dev.   Corr.\nSubj     (Intercept)  90.340741 9.504775\n         time          1.162325 1.078112 -0.97\nResidual               0.194449 0.440964\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n\n\n─────────────────────────────────────────────────\n               Coef.  Std. Error      z  Pr(&gt;|z|)\n─────────────────────────────────────────────────\n(Intercept)  33.4975    2.26163   14.81    &lt;1e-48\ntime          1.896     0.256699   7.39    &lt;1e-12\n─────────────────────────────────────────────────\n\n\nThe reason for this seemingly unlikely result is that the (Intercept) term in the fixed effects and the random effects represents the bone length at age 0, which is not of interest here. Notice that the fixed-effects (Intercept) estimate is about 33.5 mm, which is far below the observed range of the data (45.0 to 55.5 mm.)\nExtrapolation from the observed range of ages, 8 years to 9.5 years, back to 0 years, will almost inevitably result is a negative correlation between slope and intercept.\nA caterpillar plot of the random effects for intercept and slope, Figure 3.3, shows both the negative correlation between intercept and slope conditional means and wide prediction intervals on the random effects for the intercept.\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 500)), ranefinfo(egm01, :Subj))\n\n\n\n\n\nFigure 3.3: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model egm01\n\n\n\n\n\n3.2.1 Centering the time values\nThe problem of estimates of intercepts representing extrapolation beyond the observed range of the data is a common one for longitudinal data. If the time covariate represents an age, as it does here, or, say, a year in the range 2000 to 2020, the intercept, which corresponds to an age or year of zero, is rarely of interest.\nThe way to avoid extrapolation beyond the range of the data is to center the time covariate at an age or date that is of interest. For example, we may wish to consider “time in study” instead of age as the time covariate.\nIn discussing Figure 3.2 we referred to the bone length at 8 years of age, which was the time of first measurement for each of the subjects, as the “initial” bone length. If the purpose of the experiment is to create a predictive model for the growth rate that can be applied to boys who enter this age range then we could center the time at 8 years.\nAlternatively, we could center at the average observed time, 8.75 years, or at some other value of interest.\nThe important thing is to make clear what the (Itercept) parameter estimates represent. The StandardizedPredictors.jl package allows for convenient representations of several standardizing transformations in a contrasts specification for the model. An advantage of this method of coding a transformation is that the coefficient names include a concise description of the transformation.\n(In model specifications in R and later in Julia the name contrasts has been come to be applied to ways of specifying the association between covariates in the data and parameters in a model. This is an extension of the original mathematical definition of contrasts amongst the levels of a categorical covariate.)\nA model with time centered at 8 years of age can be fit as\n\ncontrasts[:time] = Center(8)\negm02 = let f = @formula resp ~ 1 + time + (1 + time | Subj)\n  fit(MixedModel, f, egdf; contrasts)\nend\nprintln(egm02)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time(centered: 8) + (1 + time(centered: 8) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n               Column      Variance Std.Dev.   Corr.\nSubj     (Intercept)        6.096663 2.469142\n         time(centered: 8)  1.162301 1.078100 -0.23\nResidual                    0.194450 0.440965\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────────\n                     Coef.  Std. Error      z  Pr(&gt;|z|)\n───────────────────────────────────────────────────────\n(Intercept)        48.6655    0.558246  87.18    &lt;1e-99\ntime(centered: 8)   1.896     0.256696   7.39    &lt;1e-12\n───────────────────────────────────────────────────────\n\n\nComparing the parameter estimates from models egm01 and egm02, we find that the only differences are in the estimates for the (Intercept) terms in the fixed-effects parameters and the variance component parameters and in the correlation of the random effects. In terms of the predictions from the model and the likelihood at the parameter estimates, egm01 and egm02 are the same model.\nA caterpillar plot, Figure 3.4, for egm02 shows much smaller spread and more precision in the distribution of the random effects for (Intercept) but the same spread and precision for the time random effects, although these random effects are displayed in a different order.\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 550)), ranefinfo(egm02, :Subj))\n\n\n\n\n\nFigure 3.4: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model egm02\n\n\n\n\nA third option is to center the time covariate at the mean of the original time values.\n\ncontrasts[:time] = Center()\negm03 = let f = @formula resp ~ 1 + time + (1 + time | Subj)\n  fit(MixedModel, f, egdf; contrasts)\nend\nprintln(egm03)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time(centered: 8.75) + (1 + time(centered: 8.75) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n                Column        Variance Std.Dev.   Corr.\nSubj     (Intercept)           5.826542 2.413823\n         time(centered: 8.75)  1.162298 1.078099 +0.10\nResidual                       0.194450 0.440965\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────────────\n                        Coef.  Std. Error      z  Pr(&gt;|z|)\n──────────────────────────────────────────────────────────\n(Intercept)           50.0875    0.541994  92.41    &lt;1e-99\ntime(centered: 8.75)   1.896     0.256696   7.39    &lt;1e-12\n──────────────────────────────────────────────────────────\n\n\nBecause the default for the Center contrast is to center about the mean, this contrasts argument can be written\nNotice that in this model the estimated correlation of the random effects for Subj is positive."
  },
  {
    "objectID": "longitudinal.html#shrinkage-plots",
    "href": "longitudinal.html#shrinkage-plots",
    "title": "3  Models for Longitudinal Data",
    "section": "3.3 Shrinkage plots",
    "text": "3.3 Shrinkage plots\nOne way of assessing a random-effects term in a linear mixed model is with a caterpillar plot, which shows two important characteristics, location and spread, of the conditional distribution \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\).\nAnother plot of interest is to show the extent to which the conditional means have been “shrunk” towards the origin by the mixed-model, which represents a compromise between fidelity to the data, measured by the sum of squared residuals, and simplicity of the model.\nThe model with the highest fidelity to the data corresponds to a fixed-effects model with the random effects model matrix, \\({\\mathbf{Z}}\\), incorporated into the fixed-effects model matrix, \\({\\mathbf{X}}\\). There are technical problems (rank deficiency) with trying to estimate parameters in this model but such “unconstrained” random effects can be approximated as the conditional means for a very large \\({\\boldsymbol{\\Sigma}}\\) matrix. (In practice we use a large multiple, say 1000, of the identity matrix as the value of \\({\\boldsymbol{\\Sigma}}\\).)\nAt the other end of the spectrum is the limit as \\({\\boldsymbol{\\Sigma}}\\rightarrow\\mathbf{0}\\), which is the simplest model, involving only the fixed-effects parameters, but usually with a comparatively poor fit.\nA shrinkage plot shows the conditional means of the random effects from the model that was fit and those for a “large” \\({\\boldsymbol{\\Sigma}}\\).\n\n\nCode\nshrinkageplot!(Figure(; resolution=(600, 600)), egm02)\n\n\n\n\n\nFigure 3.5: Shrinkage plot of the random effects for model egm02. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nFigure 3.5 reinforces some of the conclusions from Figure 3.4. In particular, the random effects for the (Intercept) are reasonably precisely determined. We see this in Figure 3.4 because the intervals in the left panel are narrow. In Figure 3.5 there is little movement in the horizontal direction between the “unconstrained” within-subject estimates and the final random-effect locations."
  },
  {
    "objectID": "longitudinal.html#nonlinear-growth-curves",
    "href": "longitudinal.html#nonlinear-growth-curves",
    "title": "3  Models for Longitudinal Data",
    "section": "3.4 Nonlinear growth curves",
    "text": "3.4 Nonlinear growth curves\nAs seen in Figure 3.2 some of the growth curves are reasonably straight (e.g. S03, S11, and S15) whereas others are concave-up (e.g. S04, S13, and S17) or concave-down (e.g. S05, S07, and S19). One way of allowing for curvature in individual growth curves is to include a quadratic term for time in both the fixed and random effects.\nThe usual cautions about polynomial terms in regression models apply even more emphatically to linear mixed models.\n\nInterpretation of polynomial coefficients depends strongly upon the location of the zero point in the time axis.\nExtrapolation of polynomial models beyond the observed range of the time values is very risky.\n\nFor balanced data like egdf we usually center the time axis about the mean, producing\n\negm04 =\n  let f = @formula resp ~\n      1 + ctime + ctime^2 + (1 + ctime + ctime^2 | Subj)\n    dat = @transform(egdf, :ctime = :time - 8.75)\n    fit(MixedModel, f, dat; contrasts)\n  end\nprintln(egm04)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + ctime + :(ctime ^ 2) + (1 + ctime + :(ctime ^ 2) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -116.6187   233.2374   253.2374   256.4258   277.0577\n\nVariance components:\n\n\n\n            Column   Variance Std.Dev.   Corr.\nSubj     (Intercept)  6.048653 2.459401\n         ctime        1.168051 1.080764 +0.08\n         ctime ^ 2    0.056465 0.237624 -0.62 +0.66\nResidual              0.187198 0.432664\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────\n              Coef.  Std. Error      z  Pr(&gt;|z|)\n────────────────────────────────────────────────\n(Intercept)  50.1      0.555364  90.21    &lt;1e-99\nctime         1.896    0.256691   7.39    &lt;1e-12\nctime ^ 2    -0.04     0.200656  -0.20    0.8420\n────────────────────────────────────────────────\n\n\nWe see that the estimate for the population quadratic coefficient, -0.04, is small, relative to its standard error, 0.20, indicating that it is not significantly different from zero. This is not unexpected because some of the growth curves in Figure 3.2 are concave-up, while others are concave-down, and others don’t show a noticeable curvature.\nA shrinkage plot, Figure 3.6, shows that the random effects for the quadratic term (vertical axis in the bottom row of panels) are highly attenuated relative to the unconstrained, “per-subject”, values.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), egm04)\n\n\n\n\n\nFigure 3.6: Shrinkage plot of the random effects for model egm04. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nBoth of these results lead to the conclusion that linear growth, over the observed range of ages, should be adequate - a conclusion reinforced by a likelihood ratio test.\n\nMixedModels.likelihoodratiotest(egm03, egm04)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nresp ~ 1 + time(centered: 8.75) + (1 + time(centered: 8.75) | Subj)\n6\n234\n\n\n\n\n\nresp ~ 1 + ctime + :(ctime ^ 2) + (1 + ctime + :(ctime ^ 2) | Subj)\n10\n233\n1\n4\n0.8709"
  },
  {
    "objectID": "longitudinal.html#longitudinal-data-with-treatments",
    "href": "longitudinal.html#longitudinal-data-with-treatments",
    "title": "3  Models for Longitudinal Data",
    "section": "3.5 Longitudinal data with treatments",
    "text": "3.5 Longitudinal data with treatments\nOften the “subjects” on which longitudinal measurements are made are divided into different treatment groups. Many of the examples cited in Davis (2002) are of this type, including one from Box (1950)\n\nbox1950 = dataset(:box)\n\nArrow.Table with 135 rows, 4 columns, and schema:\n :Group  String\n :Subj   String\n :time   Int8\n :resp   Int16\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"      =&gt; \"Body weight by week of rats in three treatment groups\"\n  \"references\" =&gt; \"@davis2002, Table 2.12, p. 32 and Problem 2.4, p. 32, and @b…\n  \"sourcefile\" =&gt; \"BOX.DAT\"\n\n\n\nbxdf = DataFrame(box1950)\ndescribe(bxdf)\n\n4×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nGroup\n\nControl\n\nThyroxin\n0\nString\n\n\n2\nSubj\n\nS01\n\nS27\n0\nString\n\n\n3\ntime\n2.0\n0\n2.0\n4\n0\nInt8\n\n\n4\nresp\n100.807\n46\n100.0\n189\n0\nInt16\n\n\n\n\n\n\nThere are three treatment groups\n\nshow(levels(bxdf.Group))\n\n[\"Control\", \"Thioracil\", \"Thyroxin\"]\n\n\nand each “subject” (rat, in this case) is in only one of the treatment groups. This can be checked by comparing the number of unique Subj levels to the number of unique combinations of Subj and Group.\n\nnrow(unique(select(bxdf, :Group, :Subj))) ==\nlength(unique(bxdf.Subj))\n\ntrue\n\n\nBecause the number of combinations of Subj and Group is equal to the number of subjects, each subject occurs in only one group.\nThese data are balanced with respect to time (i.e. each rat is weighed at the same set of times) but not with respect to treatment, as can be seen by checking the number of rats in each treatment group.\n\ncombine(\n  groupby(unique(select(bxdf, :Group, :Subj)), :Group),\n  nrow =&gt; :n,\n)\n\n3×2 DataFrame\n\n\n\nRow\nGroup\nn\n\n\n\nString\nInt64\n\n\n\n\n1\nControl\n10\n\n\n2\nThioracil\n10\n\n\n3\nThyroxin\n7\n\n\n\n\n\n\n\n3.5.1 Within-group variation\nConsidering first the control group, whose trajectories can be plotted in a “spaghetti plot”, Figure 3.7\n\n\nCode\nbxaxes =\n  mapping(:time =&gt; \"Time in trial (wk)\", :resp =&gt; \"Body weight (g)\")\nbxgdf = groupby(bxdf, :Group)\ndraw(\n  data(bxgdf[(\"Control\",)]) *\n  bxaxes *\n  mapping(; color=:Subj) *\n  (visual(Scatter) + visual(Lines)),\n)\n\n\n\n\n\nFigure 3.7: Weight (g) of rats in the control group of bxdf versus time in trial (wk).\n\n\n\n\nor in separate panels ordered by initial weight, Figure 3.8.\n\n\nCode\nlet df = bxgdf[(\"Control\",)]\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(;\n      layout=:Subj =&gt;\n        sorter(sort!(filter(:time =&gt; iszero, df), :resp).Subj),\n    ) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.8: Weight (g) of rats in the control group versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nThe panels in Figure 3.8 show a strong linear trend with little evidence of systematic curvature.\nA multi-panel plot for the Thioracil group, Figure 3.9,\n\n\nCode\nlet\n  df = bxgdf[(\"Thioracil\",)]\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(\n      layout=:Subj =&gt;\n        sorter(sort!(filter(:time =&gt; iszero, df), :resp).Subj),\n    ) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.9: Weight (g) of rats in the Thioracil group versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nshows several animals (S18, S19, S21, and S24) whose rate of weight gain decreases as the trial goes on.\nBy contrast, in the Thyroxin group, Figure 3.10,\n\n\nCode\nlet\n  df = bxgdf[(\"Thyroxin\",)]\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(\n      layout=:Subj =&gt;\n        sorter(sort!(filter(:time =&gt; iszero, df), :resp).Subj),\n    ) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.10: Weight (g) of rats in the Thyroxin group versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nif there is any suggestion of curvature it would be concave-up.\n\n\n3.5.2 Models with interaction terms\nLongitudinal data in which the observational units, each rat in this case, are in different treatment groups, require careful consideration of the origin on the time axis. If, as here, the origin on the time axis is when the treatments of the different groups began and the subjects have been randomly assigned to the treatment groups, we do not expect differences between groups at time zero.\nUsually, when a model incorporates an effect for time and a time & Group interaction - checking for different underlying slopes of the response with respect to time for each level of Group, we will also include a “main effect” for Group. This is sometimes called the hierarchical principle regarding interactions - a significant higher-order interaction usually forces inclusion of any lower-order interactions or main effects contained in it.\nOne occasion where the hierarchical principle does not apply is when the main effect for Group would represent systematic differences in the response before the treatments began. Similarly in dose-response data; when a zero dose is included we could have a main effect for dose and a dose & Group interaction without a main effect for Group, because zero dose of a treatment is the same as zero dose of a placebo.\nWe can begin with a main effect for Group, as in\n\ndelete!(contrasts, :time)\nbxm01 =\n  let f = @formula resp ~\n      (1 + time + time^2) * Group + (1 + time + time^2 | Subj)\n    fit(MixedModel, f, bxdf; contrasts)\n  end\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.1086\n1.5367\n35.21\n&lt;1e-99\n4.0283\n\n\ntime\n24.0229\n1.5639\n15.36\n&lt;1e-52\n3.7540\n\n\ntime ^ 2\n0.6143\n0.3988\n1.54\n0.1235\n0.9973\n\n\nGroup: Thioracil\n0.9086\n2.1732\n0.42\n0.6759\n\n\n\nGroup: Thyroxin\n0.6302\n2.3948\n0.26\n0.7924\n\n\n\ntime & Group: Thioracil\n-1.6271\n2.2117\n-0.74\n0.4619\n\n\n\ntime & Group: Thyroxin\n-2.1861\n2.4372\n-0.90\n0.3697\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.9357\n0.5640\n-3.43\n0.0006\n\n\n\ntime ^ 2 & Group: Thyroxin\n0.7122\n0.6215\n1.15\n0.2518\n\n\n\nResidual\n2.8879\n\n\n\n\n\n\n\n\n\nbut we expect that a model without the main effect for Group,\n\nbxm02 =\n  let f = @formula resp ~\n      1 + (time + time^2) & Group + (1 + time + time^2 | Subj)\n    fit(MixedModel, f, bxdf; contrasts)\n  end\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n0.9382\n58.21\n&lt;1e-99\n4.0466\n\n\ntime & Group: Control\n24.1725\n1.5206\n15.90\n&lt;1e-56\n\n\n\ntime & Group: Thioracil\n22.2734\n1.5206\n14.65\n&lt;1e-47\n\n\n\ntime & Group: Thyroxin\n21.7977\n1.8081\n12.06\n&lt;1e-32\n\n\n\ntime ^ 2 & Group: Control\n0.5655\n0.3806\n1.49\n0.1374\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.2815\n0.3806\n-3.37\n0.0008\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.3393\n0.4510\n2.97\n0.0030\n\n\n\ntime\n\n\n\n\n3.7539\n\n\ntime ^ 2\n\n\n\n\n0.9977\n\n\nResidual\n2.8887\n\n\n\n\n\n\n\n\n\nwill be adequate, as confirmed by\n\nMixedModels.likelihoodratiotest(bxm02, bxm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nresp ~ 1 + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n14\n836\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) + Group + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n16\n835\n0\n2\n0.9135\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, the interpretations of some of the fixed-effects coefficients change between models bxm01 and bxm02. In model bxm01 the coefficient labelled time is the estimated slope at time zero for the Control group. In model bxm02 this coefficient is labelled time & Group: Control.\nIn model bxm01 the coefficient labelled time & Group: Thioracil is the change in the estimated slope at time zero between the Thioracil group and the Control group. In model bxm02 the coefficient with this label is the estimated slope at time zero in the Thioracil group.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIs there a way to write the formula for bxm02 to avoid this?\n\n\nWe see the effect of this changing interpretation in the p-values associated with these coefficients. In model bxm01 the only coefficients with low p-values are the (Intercept), the time, representing a typical rate of weight gain in the control group at time zero, and the change in the quadratic term from the Control group to the Thioracil group.\nA model without systematic differences between groups in the initial weight and the initial slope but with differences between groups in the quadratic coefficient is sensible. It would indicate that the groups are initially homogeneous both in weight and growth rate but, as the trial proceeds, the different treatments change the rate of growth.\n\nbxm03 =\n  let f = @formula resp ~\n      1 + time + time^2 & Group + (1 + time + time^2 | Subj)\n    fit(MixedModel, f, bxdf; contrasts)\n  end\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n0.9349\n58.41\n&lt;1e-99\n4.0128\n\n\ntime\n22.8534\n0.9627\n23.74\n&lt;1e-99\n3.8082\n\n\ntime ^ 2 & Group: Control\n0.7854\n0.3240\n2.42\n0.0154\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.3781\n0.3240\n-4.25\n&lt;1e-04\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1630\n0.3691\n3.15\n0.0016\n\n\n\ntime ^ 2\n\n\n\n\n0.9954\n\n\nResidual\n2.9094\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm03, bxm02)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n\n\n\n\n\nresp ~ 1 + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n14\n836\n1\n2\n0.5328\n\n\n\n\n\n\n\n3.5.3 Possible simplification of random effects\nA caterpillar plot created from the conditional means and standard deviations of the random effects by Subj for model bxm03, Figure 3.11, indicates that all of the random-effects terms generate some prediction intervals that do not contain zero.\n\n\nCode\ncaterpillar(bxm03)\n\n\n\n\n\nFigure 3.11: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model bxm03\n\n\n\n\nA shrinkage plot, Figure 3.12\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), bxm03)\n\n\n\n\n\nFigure 3.12: Shrinkage plot of the random effects for model bxm03. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nshows that the random effects are considerably shrunk towards the origin, relative to the “unconstrained” values from within-subject fits, but none of the panels shows them collapsing to a line.\nNevertheless, the estimated unconditional distribution of the random effects in model bxm03 is a degenerate distribution.\nThat is, the estimated within-subject covariance of the random effects is singular.\n\nissingular(bxm03)\n\ntrue\n\n\nOne way to see this is because the relative covariance factor, \\(\\boldsymbol{\\lambda}\\), of the within-subject random effects is\n\nMatrix(only(bxm03.λ))\n\n3×3 Matrix{Float64}:\n  1.37925   0.0       0.0\n  1.15546   0.614962  0.0\n -0.301075  0.162532  0.0\n\n\nand we see that the third column is all zeros.\nThus, in a three-dimensional space the conditional means of the random effects for each rat, lie on a plane, even though each of the two-dimensional projections in Figure 3.12 show a scatter.\nThree-dimensional plots of the conditional means of the random effects, Figure 3.13, can help to see that these points lie on a plane.\n\n\nCode\nlet\n  bpts = Point3f.(eachcol(only(bxm03.b)))\n  Upts = Point3f.(eachcol(svd(only(bxm03.λ)).U))\n  origin = Point3(zeros(Float32, 3))\n  xlabel, ylabel, zlabel = only(bxm03.reterms).cnames\n  zlabel = \"time²\"\n  perspectiveness = 0.5\n  aspect = :data\n  f = Figure(; resolution=(1200, 500))\n  u, v, w = -Upts[2]    # second principle direction flipped to get positive w\n  elevation = asin(w)\n  azimuth = atan(v, u)\n  ax1 =\n    Axis3(f[1, 1]; aspect, xlabel, ylabel, zlabel, perspectiveness)\n  ax2 = Axis3(\n    f[1, 2];\n    aspect,\n    xlabel,\n    ylabel,\n    zlabel,\n    perspectiveness,\n    elevation,\n    azimuth,\n  )\n  scatter!(ax1, bpts; marker='∘', markersize=20)\n  scatter!(ax2, bpts; marker='∘', markersize=20)\n  for p in Upts\n    seg = [origin, p]\n    lines!(ax1, seg)\n    lines!(ax2, seg)\n  end\n  f\nend\n\n\n\n\n\nFigure 3.13: Two views of the conditional means of the random effects from model bxm03. The lines from the origin are the principal axes of the unconditional distribution of the random effects. The panel on the right is looking in along the negative of the second principle axis (red line in left panel).\n\n\n\n\nIn each of the panels the three orthogonal lines from the origin are the three principle axes of the unconditional distribution of the random effects, corresponding to the columns of\n\nsvd(only(bxm03.λ)).U\n\n3×3 Matrix{Float64}:\n -0.723674   0.568542   0.391224\n -0.675882  -0.698487  -0.235156\n  0.139569  -0.434598   0.889744\n\n\nThe panel on the right is oriented so the viewpoint is along the negative of the second principle axis, showing that there is considerable variation in the first principle direction and zero variation in the third principle direction.\nWe see that the distribution of the random effects in model bxm03 is degenerate but it is not clear how to simplify the model.\nCoverage intervals from a parametric bootstrap sample for this model\n\n\nCode\nbxm03samp = parametricbootstrap(\n  Xoshiro(8642468),\n  10_000,\n  bxm03;\n  hide_progress=true,\n)\nbxm03pars = DataFrame(bxm03samp.allpars)\nDataFrame(shortestcovint(bxm03samp))\n\n\n12×5 DataFrame\n\n\n\nRow\ntype\ngroup\nnames\nlower\nupper\n\n\n\nString\nString?\nString?\nFloat64\nFloat64\n\n\n\n\n1\nβ\nmissing\n(Intercept)\n52.7311\n56.4209\n\n\n2\nβ\nmissing\ntime\n20.9169\n24.7163\n\n\n3\nβ\nmissing\ntime ^ 2 & Group: Control\n0.154471\n1.4371\n\n\n4\nβ\nmissing\ntime ^ 2 & Group: Thioracil\n-2.0224\n-0.72444\n\n\n5\nβ\nmissing\ntime ^ 2 & Group: Thyroxin\n0.449056\n1.928\n\n\n6\nσ\nSubj\n(Intercept)\n2.48407\n5.48889\n\n\n7\nσ\nSubj\ntime\n2.26462\n5.44344\n\n\n8\nρ\nSubj\n(Intercept), time\n0.381016\n1.0\n\n\n9\nσ\nSubj\ntime ^ 2\n0.545848\n1.39446\n\n\n10\nρ\nSubj\n(Intercept), time ^ 2\n-1.0\n-0.444577\n\n\n11\nρ\nSubj\ntime, time ^ 2\n-0.898147\n-0.184607\n\n\n12\nσ\nresidual\nmissing\n2.32538\n3.2721\n\n\n\n\n\n\nshows that the coverage intervals for both of the correlation parameters involving the (Intercept) extend out to one of the limits of the allowable range [-1, 1] of correlations.\nA kernel density plot, Figure 3.14, of the parametric bootstrap estimates of the correlation coefficients reinforces this conclusion.\n\n\nCode\ndraw(\n  data(@subset(bxm03pars, :type == \"ρ\")) *\n  mapping(\n    :value =&gt; \"Bootstrap replicates of correlation estimates\";\n    color=(:names =&gt; \"Variables\"),\n  ) *\n  AlgebraOfGraphics.density();\n)\n\n\n\n\n\nFigure 3.14: Kernel density plots of parametric bootstrap estimates of correlation estimates from model bxm03\n\n\n\n\nEven on the scale of Fisher’s z transformation, Figure 3.15, these estimates are highly skewed.\n\n\nCode\nlet\n  dat = @transform(\n    @subset(bxm03pars, :type == \"ρ\"),\n    :z = atanh(clamp(:value, -0.99999, 0.99999))\n  )\n  mp = mapping(\n    :z =&gt; \"Fisher's z transformation of correlation estimates\";\n    color=(:names =&gt; \"Variables\"),\n  )\n  draw(\n    data(dat) * mp * AlgebraOfGraphics.density();\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\nFigure 3.15: Kernel density plots of Fisher’s z transformation of parametric bootstrap estimates of correlation estimates from model bxm03\n\n\n\n\nBecause of these high correlations, trying to deal with the degenerate random effects distribution by simply removing the random effects for time ^ 2 reduces the model too much.\n\nbxm04 =\n  let f =\n      @formula resp ~ 1 + time + time^2 & Group + (1 + time | Subj)\n    fit(MixedModel, f, bxdf; contrasts)\n  end\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n1.2587\n43.38\n&lt;1e-99\n5.5780\n\n\ntime\n22.8534\n1.0454\n21.86\n&lt;1e-99\n3.6245\n\n\ntime ^ 2 & Group: Control\n0.7633\n0.2526\n3.02\n0.0025\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.3807\n0.2526\n-5.47\n&lt;1e-07\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1984\n0.2890\n4.15\n&lt;1e-04\n\n\n\nResidual\n3.6290\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm04, bxm03)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time | Subj)\n9\n867\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n30\n3\n&lt;1e-05\n\n\n\n\n\nas does eliminating within-subject correlations between the random-effects for the time^2 random effect and the other random effects.\n\nbxm05 =\n  let f = @formula resp ~\n      1 +\n      time +\n      time^2 & Group +\n      (1 + time | Subj) +\n      (0 + time^2 | Subj)\n    fit(MixedModel, f, bxdf; contrasts)\n  end\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n1.0433\n52.34\n&lt;1e-99\n4.6060\n\n\ntime\n22.8534\n0.8167\n27.98\n&lt;1e-99\n2.5577\n\n\ntime ^ 2 & Group: Control\n0.8262\n0.3471\n2.38\n0.0173\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.4171\n0.3471\n-4.08\n&lt;1e-04\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1605\n0.4054\n2.86\n0.0042\n\n\n\ntime ^ 2\n\n\n\n\n0.9240\n\n\nResidual\n3.0374\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm05, bxm03)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time | Subj) + (0 + :(time ^ 2) | Subj)\n10\n850\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n13\n2\n0.0017\n\n\n\n\n\n\n\n3.5.4 Some consequences of changing the random-effects structure\nThe three models bxm03, bxm04, and bxm05 have the same fixed-effects structure. The random effects specification varies from the most complicated (bxm03), which produces a singular estimate of the covariance, to the simplest (bxm04) structure to an intermediate structure (bxm05).\nThe likelihood ratio tests give evidence for preferring bxm03, the most complex of these models, but also one with a degenerate distribution of the random effects.\nIf we assume that the purpose of the experiment is to compare the effects of the two treatments versus the Control group on the weight gain of the rats, then interest would focus on the fixed-effects parameters and, in particular, on those associated with the groups. The models give essentially the same predictions of weight versus time for a “typical” animal in each group, Figure 3.16.\n\n\nCode\nlet\n  times = Float32.((0:256) / 64)\n  times² = abs2.(times)\n  z257 = zeros(Float32, 257)\n  tmat = hcat(\n    ones(Float32, 257 * 3),\n    repeat(times, 3),\n    vcat(times², z257, z257),\n    vcat(z257, times², z257),\n    vcat(z257, z257, times²),\n  )\n  grp = repeat([\"Control\", \"Thioracil\", \"Thyroxin\"]; inner=257)\n  draw(\n    data(\n      append!(\n        append!(\n          DataFrame(;\n            times=tmat[:, 2],\n            wt=tmat * Float32.(bxm03.beta),\n            Group=grp,\n            model=\"bxm03\",\n          ),\n          DataFrame(;\n            times=tmat[:, 2],\n            wt=tmat * Float32.(bxm04.beta),\n            Group=grp,\n            model=\"bxm04\",\n          ),\n        ),\n        DataFrame(;\n          times=tmat[:, 2],\n          wt=tmat * Float32.(bxm05.beta),\n          Group=grp,\n          model=\"bxm05\",\n        ),\n      ),\n    ) *\n    mapping(\n      :times =&gt; \"Time in trial (wk)\",\n      :wt =&gt; \"Weight (gm)\";\n      color=:Group,\n      col=:model,\n    ) *\n    visual(Lines);\n    axis=(width=350, height=350),\n  )\nend\n\n\n\n\n\nFigure 3.16: Typical weight curves from models bxm03, bxm04, and bxm05 for each of the three treatment groups.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOther points to make:\n1. Standard errors are different, largest for `bxm05`, smallest for `bxm04`\n2. Real interest should center on the differences in the quadratic coef - trt vs control\n3. Not sure how to code that up\n4. More complex models require more iterations and more work per iteration\n5. Probably go with `bxm03` in this case, even though it gives a degenerate dist'n of random effects.  The idea is that the random effects are absorbing a form of rat-to-rat variability.  The residual standard deviation does reflect this.\n\n\n\n\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear curves. Biometrics, 6(4), 362. https://doi.org/10.2307/3001781\n\n\nDavis, C. S. (2002). Statistical methods for the analysis of repeated measurements. In Springer Texts in Statistics (pp. xxiv + 415). New York, NY: Springer. https://doi.org/10.1007/b97287\n\n\nElston, R. C., & Grizzle, J. E. (1962). Estimation of time-response curves and their confidence bands. Biometrics, 18, 148–159. https://doi.org/10.2307/2527453"
  },
  {
    "objectID": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "href": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "title": "4  A large-scale designed experiment",
    "section": "4.1 Trial-level data from the LDT",
    "text": "4.1 Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = dataset(:ELP_ldt_trial)\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  Int16\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"     =&gt; \"Trial-level data from Lexical Discrimination Task in the Engl…\n  \"reference\" =&gt; \"Balota et al. (2007), The English Lexicon Project, Behavior R…\n  \"source\"    =&gt; \"https://osf.io/n63s2\"\n\n\n\nldttrial =\n  transform!(DataFrame(ldttrial), :seq =&gt; (x -&gt; x .&gt; 2000) =&gt; :s2)\ndescribe(ldttrial)\n\n6×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n\n\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day. The s2 column, added to the data frame using @transform!, is a Boolean value indicating if the trial is in the second session."
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialexplore",
    "href": "largescaledesigned.html#sec-ldtinitialexplore",
    "title": "4  A large-scale designed experiment",
    "section": "4.2 Initial data exploration",
    "text": "4.2 Initial data exploration\nFrom the basic summary of ldttrial we can see that there are some questionable response times — such as negative values and values over 32 seconds. Because of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\n4.2.1 Summaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‘i’ in the names of these summaries (and an ‘s’ in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),               # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(\n    :wrdlen = Int8(length(:item)),\n    :ipropacc = :iacc / :ni\n  )\nend\n\n80962×7 DataFrame80937 rows omitted\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\n\n\n\n\n1\na\n35\n0\n26\n743.0\n1\n0.742857\n\n\n2\ne\n35\n0\n19\n824.0\n1\n0.542857\n\n\n3\naah\n34\n0\n21\n770.5\n3\n0.617647\n\n\n4\naal\n34\n0\n32\n702.5\n3\n0.941176\n\n\n5\nAaron\n33\n0\n31\n625.0\n5\n0.939394\n\n\n6\nAarod\n33\n0\n23\n810.0\n5\n0.69697\n\n\n7\naback\n34\n0\n15\n710.0\n5\n0.441176\n\n\n8\nahack\n34\n0\n34\n662.0\n5\n1.0\n\n\n9\nabacus\n34\n0\n17\n671.5\n6\n0.5\n\n\n10\nalacus\n34\n0\n29\n640.0\n6\n0.852941\n\n\n11\nabandon\n34\n0\n32\n641.0\n7\n0.941176\n\n\n12\nacandon\n34\n0\n33\n725.5\n7\n0.970588\n\n\n13\nabandoned\n34\n0\n31\n667.5\n9\n0.911765\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n80951\nzoology\n33\n0\n32\n623.0\n7\n0.969697\n\n\n80952\npoology\n33\n0\n32\n757.0\n7\n0.969697\n\n\n80953\nzoom\n35\n0\n34\n548.0\n4\n0.971429\n\n\n80954\nzool\n35\n0\n30\n633.0\n4\n0.857143\n\n\n80955\nzooming\n33\n0\n29\n617.0\n7\n0.878788\n\n\n80956\nsooming\n33\n0\n30\n721.0\n7\n0.909091\n\n\n80957\nzooms\n33\n0\n30\n598.0\n5\n0.909091\n\n\n80958\ncooms\n33\n0\n31\n660.0\n5\n0.939394\n\n\n80959\nzucchini\n34\n0\n29\n781.5\n8\n0.852941\n\n\n80960\nhucchini\n34\n0\n32\n727.5\n8\n0.941176\n\n\n80961\nZurich\n34\n0\n21\n731.5\n6\n0.617647\n\n\n80962\nZurach\n34\n0\n26\n811.0\n6\n0.764706\n\n\n\n\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nni\n33.9166\n30\n34.0\n37\n0\nInt64\n\n\n3\nimiss\n0.0169215\n0\n0.0\n2\n0\nInt64\n\n\n4\niacc\n29.0194\n0\n31.0\n37\n0\nInt64\n\n\n5\nimedianrt\n753.069\n458.0\n737.5\n1691.0\n0\nFloat64\n\n\n6\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n7\nipropacc\n0.855616\n0.0\n0.911765\n1.0\n0\nFloat64\n\n\n8\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n\n\n\n\nThis table shows that some of the items were never identified correctly. These are\n\nfilter(:iacc =&gt; iszero, byitem)\n\n9×8 DataFrame\n\n\n\nRow\nitem\nni\nimiss\niacc\nimedianrt\nwrdlen\nipropacc\nisword\n\n\n\nString\nInt64\nInt64\nInt64\nFloat64\nInt8\nFloat64\nBool\n\n\n\n\n1\nbaobab\n34\n0\n0\n616.5\n6\n0.0\ntrue\n\n\n2\nhaulage\n34\n0\n0\n708.5\n7\n0.0\ntrue\n\n\n3\nleitmotif\n35\n0\n0\n688.0\n9\n0.0\ntrue\n\n\n4\nmiasmal\n35\n0\n0\n774.0\n7\n0.0\ntrue\n\n\n5\npeahen\n34\n0\n0\n684.0\n6\n0.0\ntrue\n\n\n6\nplosive\n34\n0\n0\n663.0\n7\n0.0\ntrue\n\n\n7\nplugugly\n33\n0\n0\n709.0\n8\n0.0\ntrue\n\n\n8\nposhest\n34\n0\n0\n740.0\n7\n0.0\ntrue\n\n\n9\nservo\n33\n0\n0\n697.0\n5\n0.0\ntrue\n\n\n\n\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a “left join”. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nUnion{Missing, Int8}\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nUnion{Missing, Bool}\n\n\n\n\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n8×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.31\n1\n409.0\n816\n0\nInt16\n\n\n2\nseq\n1687.21\n1\n1687.0\n3374\n0\nInt16\n\n\n3\nacc\n0.85604\nfalse\n1.0\ntrue\n1370\nUnion{Missing, Bool}\n\n\n4\nrt\n846.325\n-16160\n732.0\n32061\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.407128\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99835\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.499995\nfalse\n0.0\ntrue\n0\nBool\n\n\n\n\n\n\n\n\n\n\n\n\nNamed argument “error”\n\n\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\n\nA barchart of the word length counts, Figure 4.1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\nlet wlen = 1:21\n  draw(\n    data((; wrdlen=wlen, count=counts(byitem.wrdlen, wlen))) *\n    mapping(:wrdlen =&gt; \"Length of word\", :count) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\nFigure 4.1: Histogram of word lengths in the items used in the lexical decision task.\n\n\n\n\nTo examine trends in accuracy by word length we use a scatterplot smoother on the binary response, as described in Section 6.1.1. The resulting plot, Figure 4.2, shows the accuracy of identifying words is more-or-less constant at around 84%, but accuracy decreases with increasing word length for the nonwords.\n\n\nCode\ndraw(\n  data(filter(:acc =&gt; !ismissing, ldttrial)) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :acc =&gt; \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.2: Smoothed curves of accuracy versus word length in the lexical decision task.\n\n\n\n\nFigure 4.2 may be a bit misleading because the largest discrepancies in proportion of accurate identifications of words and nonwords occur for the longest words, of which there are few. Over 95% of the words are between 4 and 13 characters in length\n\ncount(x -&gt; 4 ≤ x ≤ 13, byitem.wrdlen) / nrow(byitem)\n\n0.9654899829549666\n\n\nIf we restrict the smoother curves to this range, as in Figure 4.3,\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc), 4 ≤ :wrdlen ≤ 13)) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :acc =&gt; \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.3: Smoothed curves of accuracy versus word length in the range 4 to 13 characters in the lexical decision task.\n\n\n\n\nthe differences are less dramatic.\nAnother way to visualize these results is by plotting the proportion accurate versus word-length separately for words and non-words with the area of each marker proportional to the number of observations for that combinations (Figure 4.4).\n\n\nCode\nlet\n  itemsummry = combine(\n    groupby(byitem, [:wrdlen, :isword]),\n    :ni =&gt; sum,\n    :imiss =&gt; sum,\n    :iacc =&gt; sum,\n  )\n  @transform!(\n    itemsummry,\n    :iacc_mean = :iacc_sum / (:ni_sum - :imiss_sum)\n  )\n  @transform!(itemsummry, :msz = sqrt((:ni_sum - :imiss_sum) / 800))\n  draw(\n    data(itemsummry) * mapping(\n      :wrdlen =&gt; \"Word length\",\n      :iacc_mean =&gt; \"Proportion accurate\";\n      color=:isword,\n      markersize=:msz,\n    );\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\nFigure 4.4: Proportion of accurate trials in the LDT versus word length separately for words and non-words. The area of the marker is proportional to the number of observations represented.\n\n\n\n\nThe pattern in the range of word lengths with non-negligible counts (there are points in the plot down to word lengths of 1 and up to word lengths of 21 but these points are very small) is that the accuracy for words is nearly constant at about 84% and the accuracy fof nonwords is slightly higher until lengths of 13, at which point it falls off a bit.\n\n\n4.2.2 Summaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),               # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n814×6 DataFrame789 rows omitted\n\n\n\nRow\nsubj\nns\nsmiss\nsacc\nsmedianrt\nspropacc\n\n\n\nInt16\nInt64\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n3374\n0\n3158\n554.0\n0.935981\n\n\n2\n2\n3372\n1\n3031\n960.0\n0.898873\n\n\n3\n3\n3372\n3\n3006\n813.0\n0.891459\n\n\n4\n4\n3374\n1\n3062\n619.0\n0.907528\n\n\n5\n5\n3374\n0\n2574\n677.0\n0.762893\n\n\n6\n6\n3374\n0\n2927\n855.0\n0.867516\n\n\n7\n7\n3374\n4\n2877\n918.5\n0.852697\n\n\n8\n8\n3372\n1\n2731\n1310.0\n0.809905\n\n\n9\n9\n3374\n13\n2669\n657.0\n0.791049\n\n\n10\n10\n3374\n0\n2722\n757.0\n0.806758\n\n\n11\n11\n3374\n0\n2894\n632.0\n0.857736\n\n\n12\n12\n3374\n4\n2979\n692.0\n0.882928\n\n\n13\n13\n3374\n2\n2980\n1114.0\n0.883225\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n803\n805\n3374\n5\n2881\n534.0\n0.853883\n\n\n804\n806\n3374\n1\n3097\n841.5\n0.917902\n\n\n805\n807\n3374\n3\n2994\n704.0\n0.887374\n\n\n806\n808\n3374\n2\n2751\n630.5\n0.815353\n\n\n807\n809\n3372\n4\n2603\n627.0\n0.771945\n\n\n808\n810\n3374\n1\n3242\n603.5\n0.960877\n\n\n809\n811\n3374\n2\n2861\n827.0\n0.847955\n\n\n810\n812\n3372\n6\n3012\n471.0\n0.893238\n\n\n811\n813\n3372\n4\n2932\n823.0\n0.869514\n\n\n812\n814\n3374\n1\n3070\n773.0\n0.909899\n\n\n813\n815\n3374\n1\n3024\n602.0\n0.896266\n\n\n814\n816\n3374\n0\n2950\n733.0\n0.874333\n\n\n\n\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n6×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nns\n3373.41\n3370\n3374.0\n3374\n0\nInt64\n\n\n3\nsmiss\n1.68305\n0\n1.0\n22\n0\nInt64\n\n\n4\nsacc\n2886.33\n1727\n2928.0\n3286\n0\nInt64\n\n\n5\nsmedianrt\n760.992\n205.0\n735.0\n1804.0\n0\nFloat64\n\n\n6\nspropacc\n0.855613\n0.511855\n0.868031\n0.973918\n0\nFloat64\n\n\n\n\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure 4.5, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc =&gt; \"Proportion accurate\",\n    :smedianrt =&gt; \"Median response time (ms)\",\n  ) *\n  (smooth() + visual(Scatter));\n)\n\n\n\n\n\nFigure 4.5: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(&lt;(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(filter(:spropacc =&gt; &gt;(0.9), bysubj).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant’s mean response.\nAs described in Section 4.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ≤ :rt ≤ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsubj\n409.802\n1\n410.0\n816\n0\nInt16\n\n\n2\nseq\n1684.56\n1\n1684.0\n3374\n0\nInt16\n\n\n3\nacc\n0.859884\nfalse\n1.0\ntrue\n0\nBool\n\n\n4\nrt\n838.712\n200\n733.0\n3000\n0\nInt16\n\n\n5\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n6\ns2\n0.40663\nfalse\n0.0\ntrue\n0\nBool\n\n\n7\nwrdlen\n7.99244\n1\n8.0\n21\n0\nInt8\n\n\n8\nisword\n0.500126\nfalse\n1.0\ntrue\n0\nBool\n\n\n9\nspropacc\n0.857169\n0.511855\n0.869295\n0.973918\n0\nFloat64\n\n\n\n\n\n\n\n\n4.2.3 Choice of response scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure 4.6, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt =&gt; \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.6: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\nThe density of the response speed, in responses per second, is shown in Figure 4.7.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :rt =&gt;\n      (\n        x -&gt; 1000 / x\n      ) =&gt; \"Response speed (s⁻¹) for correct responses\",\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.7: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\nFigure 4.6 and Figure 4.7 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(\n  r -&gt; !ismissing(r.acc) && 200 &lt; r.rt &lt; 333,\n  eachrow(ldttrial),\n) / count(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure 4.8 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; \"Response time (ms)\";\n    :color =&gt; :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure 4.8: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\nand Figure 4.9 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000 / x) =&gt; \"Speed of response (s⁻¹)\";\n    :color =&gt; :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure 4.9: Scatterplot smooths of response speed versus word length in the LDT.\n\n\n\n\nFor the most part the smoother lines in Figure 4.9 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study.\nFigure 4.10 shows a “violin plot” - the empirical density of the response speed by word length separately for words and nonwords. The lines on the plot are fit by linear regression.\n\n\nCode\nlet\n  plt = data(@subset(pruned, :wrdlen &gt; 3, :wrdlen &lt; 14))\n  plt *= mapping(\n    :wrdlen =&gt; \"Word length\",\n    :rt =&gt; (x -&gt; 1000 / x) =&gt; \"Speed of response (s⁻¹)\",\n    color=:isword,\n    side=:isword,\n  )\n  plt *= (visual(Violin) + linear(; interval=:confidence))\n  draw(plt, axis=(; limits=(nothing, (0.0, 2.8))))\nend\n\n\n\n\n\nFigure 4.10: Empirical density of response speed versus word length by word/non-word status, with lines fit by linear regression to each group."
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialmodel",
    "href": "largescaledesigned.html#sec-ldtinitialmodel",
    "title": "4  A large-scale designed experiment",
    "section": "4.3 Models with scalar random effects",
    "text": "4.3 Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\n4.3.1 Establish the contrasts\nBecause there are a large number of items in the data set it is important to assign a Grouping() contrast to item (and, less importantly, to subj). For the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts[:isword] = EffectsCoding(; base=false);\ncontrasts[:wrdlen] = Center(8);\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let f = @formula 1000/rt ~\n    1 + isword * wrdlen + (1 | item) + (1 | subj)\n  fit(MixedModel, f, pruned; contrasts)\nend\n\nMinimizing 53    Time: 0:00:03 (75.43 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nThe predicted response speed by word length and word/nonword status can be summarized as\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm01)\n\n10×6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.46555\n0.00903111\n1.45652\n1.47458\n\n\n2\n6\nfalse\n1.38947\n0.00898124\n1.38049\n1.39845\n\n\n3\n8\nfalse\n1.31338\n0.00896459\n1.30442\n1.32235\n\n\n4\n10\nfalse\n1.2373\n0.00898133\n1.22832\n1.24628\n\n\n5\n12\nfalse\n1.16121\n0.00903129\n1.15218\n1.17025\n\n\n6\n4\ntrue\n1.6351\n0.0090311\n1.62607\n1.64413\n\n\n7\n6\ntrue\n1.5367\n0.00898124\n1.52772\n1.54569\n\n\n8\n8\ntrue\n1.43831\n0.00896459\n1.42934\n1.44727\n\n\n9\n10\ntrue\n1.33991\n0.00898133\n1.33092\n1.34889\n\n\n10\n12\ntrue\n1.24151\n0.00903127\n1.23248\n1.25054\n\n\n\n\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let f = @formula 1000 / rt ~\n    1 + isword * wrdlen + (1 | item) + (1 | subj)\n  dat = filter(:spropacc =&gt; &gt;(0.8), pruned)\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 53    Time: 0:00:02 (52.78 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3611\n0.0088\n153.98\n&lt;1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.73\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n&lt;1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\n\neffects(Dict(:isword =&gt; [false, true], :wrdlen =&gt; 4:2:12), elm02)\n\n10×6 DataFrame\n\n\n\nRow\nwrdlen\nisword\n1000 / rt\nerr\nlower\nupper\n\n\n\nInt64\nBool\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n4\nfalse\n1.45036\n0.00892466\n1.44144\n1.45929\n\n\n2\n6\nfalse\n1.37297\n0.00887101\n1.3641\n1.38184\n\n\n3\n8\nfalse\n1.29557\n0.00885308\n1.28672\n1.30443\n\n\n4\n10\nfalse\n1.21818\n0.0088711\n1.20931\n1.22705\n\n\n5\n12\nfalse\n1.14078\n0.00892486\n1.13186\n1.14971\n\n\n6\n4\ntrue\n1.62735\n0.00892465\n1.61842\n1.63627\n\n\n7\n6\ntrue\n1.52702\n0.008871\n1.51815\n1.53589\n\n\n8\n8\ntrue\n1.4267\n0.00885308\n1.41784\n1.43555\n\n\n9\n10\ntrue\n1.32637\n0.00887109\n1.3175\n1.33524\n\n\n10\n12\ntrue\n1.22605\n0.00892483\n1.21712\n1.23497\n\n\n\n\n\n\nTo compare the conditional means of the random effects for item in these two models we incorporate them into the byitem table.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ncondmeans = leftjoin!(\n  leftjoin!(\n    rename!(DataFrame(raneftables(elm01)[:item]), [:item, :elm01]),\n    rename!(DataFrame(raneftables(elm02)[:item]), [:item, :elm02]);\n    on=:item,\n  ),\n  select(byitem, :item, :isword; copycols=false);\n  on=:item,\n)\ndraw(\n  data(condmeans) * mapping(\n    :elm01 =&gt; \"Conditional means of item random effects for model elm01\",\n    :elm02 =&gt; \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  );\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\nFigure 4.11: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure 4.11.\n\n\nFigure 4.11 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(condmeans, :elm01, :elm02)))\n\n2×2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nsaveoptsum(\"./optsums/elm01.json\", elm01);\n\n\nelm01a = restoreoptsum!(\n  let f = @formula 1000 / rt ~\n      1 + isword * wrdlen + (1 | item) + (1 | subj)\n    MixedModel(f, pruned; contrasts)\n  end,\n  \"./optsums/elm01.json\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n&lt;1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n&lt;1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n&lt;1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n&lt;1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(dataset(:ELP_ldt_item))\ndescribe(elpldtitem)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nitem\n\nAarod\n\nzuss\n0\nString\n\n\n2\nOrtho_N\n1.53309\n0\n1.0\n25\n0\nInt8\n\n\n3\nBG_Sum\n13938.4\n11\n13026.0\n59803\n177\nUnion{Missing, Int32}\n\n\n4\nBG_Mean\n1921.25\n5.5\n1907.0\n6910.0\n177\nUnion{Missing, Float32}\n\n\n5\nBG_Freq_By_Pos\n2043.08\n0\n1928.0\n6985\n4\nUnion{Missing, Int16}\n\n\n6\nitemno\n40481.5\n1\n40481.5\n80962\n0\nInt32\n\n\n7\nisword\n0.5\nfalse\n0.5\ntrue\n0\nBool\n\n\n8\nwrdlen\n7.9988\n1\n8.0\n21\n0\nInt8\n\n\n9\npairno\n20241.0\n1\n20241.0\n40481\n0\nInt32\n\n\n\n\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(dataset(:ELP_ldt_subj))\ndescribe(elpldtsubj)\n\n20×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nAny\nAny\nInt64\nType\n\n\n\n\n1\nsubj\n409.311\n1\n409.5\n816\n0\nInt16\n\n\n2\nuniv\n\nKansas\n\nWayne State\n0\nString\n\n\n3\nsex\n\nf\n\nm\n8\nUnion{Missing, String}\n\n\n4\nDOB\n\n1938-06-07\n\n1984-11-14\n0\nDate\n\n\n5\nMEQ\n44.4932\n19.0\n44.0\n75.0\n8\nUnion{Missing, Float32}\n\n\n6\nvision\n5.51169\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n7\nhearing\n5.86101\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n8\neducatn\n8.89681\n1\n12.0\n28\n0\nInt8\n\n\n9\nncorrct\n29.8505\n5\n30.0\n40\n18\nUnion{Missing, Int8}\n\n\n10\nrawscor\n31.9925\n13\n32.0\n40\n18\nUnion{Missing, Int8}\n\n\n11\nvocabAge\n17.8123\n10.3\n17.8\n21.0\n19\nUnion{Missing, Float32}\n\n\n12\nshipTime\n3.0861\n0\n3.0\n9\n1\nUnion{Missing, Int8}\n\n\n13\nreadTime\n2.50215\n0.0\n2.0\n15.0\n1\nUnion{Missing, Float32}\n\n\n14\npreshlth\n5.48708\n0\n6.0\n7\n1\nUnion{Missing, Int8}\n\n\n15\npasthlth\n4.92989\n0\n5.0\n7\n1\nUnion{Missing, Int8}\n\n\n16\nS1start\n\n2001-03-16T13:49:27\n2001-10-16T11:38:28.500\n2003-07-29T18:48:44\n0\nDateTime\n\n\n17\nS2start\n\n2001-03-19T10:00:35\n2001-10-19T14:24:19.500\n2003-07-30T13:07:45\n0\nDateTime\n\n\n18\nMEQstrt\n\n2001-03-22T18:32:00\n2001-10-23T11:26:13\n2003-07-30T14:30:49\n7\nUnion{Missing, DateTime}\n\n\n19\nfilename\n\n101DATA.LDT\n\nData998.LDT\n0\nString\n\n\n20\nfrstLang\n\nEnglish\n\nother\n8\nUnion{Missing, String}\n\n\n\n\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure 4.12,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n\n\nFigure 4.12: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\nshows definite distinctions between subjects because the widths of the prediction intervals are small compared to the range of the conditional modes. Also, there is at least one outlier with a conditional mode over 1.0.\nFigure 4.13 is the corresponding caterpillar plot for model elm02 fit to the data with inaccurate responders eliminated.\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm02, :subj),\n)\n\n\n\n\n\nFigure 4.13: Conditional means and 95% prediction intervals for subject random effects in elm02.\n\n\n\n\n\n\n\n\nBalota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The English lexicon project. Behavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/bf03193014\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statistical Society: Series B (Methodological), 26(2), 211–243. https://doi.org/10.1111/j.2517-6161.1964.tb00553.x"
  },
  {
    "objectID": "largescaleobserved.html#structure-of-the-data",
    "href": "largescaleobserved.html#structure-of-the-data",
    "title": "5  A large-scale observational study",
    "section": "5.1 Structure of the data",
    "text": "5.1 Structure of the data\nOne of the purposes of this chapter is to study which dimensions of the data have the greatest effect on the amount of memory used to represent the model and the time required to fit a model.\nThe two datasets examined in Chapter 4, from the English Lexicon Project (Balota et al., 2007), consist of trials or instances associated with subject and item factors. The subject and item factors are “incompletely crossed” in that each item occurred in trials with several subjects, but not all subjects, and each subject responded to several different items, but not to all items.\nSimilarly in the movie ratings, each instance of a rating is associated with a user and with a movie, and these factors are incompletely crossed.\n\nratings = dataset(:ratings)\n\nArrow.Table with 27753444 rows, 4 columns, and schema:\n :userId     Int32\n :movieId    Int32\n :rating     Float32\n :timestamp  Int32\n\nwith metadata given by a Base.ImmutableDict{String, String} with 1 entry:\n  \"url\" =&gt; \"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n\n\nConvert this Arrow table to a Table and drop the timestamp column, which we won’t be using. (For small data sets dropping such columns is not important but with over 27 million ratings it does help to drop unnecessary columns to save some memory space.)\n\nratings =\n  Table(getproperties(ratings, (:userId, :movieId, :rating)))\n\nTable with 3 columns and 27753444 rows:\n      userId  movieId  rating\n    ┌────────────────────────\n 1  │ 1       307      3.5\n 2  │ 1       481      3.5\n 3  │ 1       1091     1.5\n 4  │ 1       1257     4.5\n 5  │ 1       1449     4.5\n 6  │ 1       1590     2.5\n 7  │ 1       1591     1.5\n 8  │ 1       2134     4.5\n 9  │ 1       2478     4.0\n 10 │ 1       2840     3.0\n 11 │ 1       2986     2.5\n 12 │ 1       3020     4.0\n 13 │ 1       3424     4.5\n 14 │ 1       3698     3.5\n 15 │ 1       3826     2.0\n 16 │ 1       3893     3.5\n 17 │ 2       170      3.5\n 18 │ 2       849      3.5\n 19 │ 2       1186     3.5\n 20 │ 2       1235     3.0\n 21 │ 2       1244     3.0\n 22 │ 2       1296     4.5\n 23 │ 2       1663     3.0\n ⋮  │   ⋮        ⋮       ⋮\n\n\nInformation on the movies is available in the movies dataset, which we supplement with the mean rating for each movie in the ratings table.\n\nmovies = Table(\n  disallowmissing!(\n    leftjoin!(\n      combine(\n        groupby(DataFrame(ratings), :movieId),\n        :rating =&gt; mean =&gt; :mnrtng,\n      ),\n      DataFrame(dataset(:movies));\n      on=:movieId,\n    );\n    error=false,\n  )\n)\n\nTable with 25 columns and 53889 rows:\n      movieId  mnrtng   nrtngs  title                 imdbId  tmdbId  Action  ⋯\n    ┌──────────────────────────────────────────────────────────────────────────\n 1  │ 1        3.88665  68469   Toy Story (1995)      114709  862     false   ⋯\n 2  │ 2        3.24658  27143   Jumanji (1995)        113497  8844    false   ⋯\n 3  │ 3        3.17398  15585   Grumpier Old Men (1…  113228  15602   false   ⋯\n 4  │ 4        2.87454  2989    Waiting to Exhale (…  114885  31357   false   ⋯\n 5  │ 5        3.07729  15474   Father of the Bride…  113041  11862   false   ⋯\n 6  │ 6        3.84421  28683   Heat (1995)           113277  949     true    ⋯\n 7  │ 7        3.37135  15301   Sabrina (1995)        114319  11860   false   ⋯\n 8  │ 8        3.12248  1539    Tom and Huck (1995)   112302  45325   false   ⋯\n 9  │ 9        3.00753  4449    Sudden Death (1995)   114576  9091    true    ⋯\n 10 │ 10       3.43163  33086   GoldenEye (1995)      113189  710     true    ⋯\n 11 │ 11       3.66028  19669   American President,…  112346  9087    false   ⋯\n 12 │ 12       2.66965  4524    Dracula: Dead and L…  112896  12110   false   ⋯\n 13 │ 13       3.33965  1952    Balto (1995)          112453  21032   false   ⋯\n 14 │ 14       3.429    6838    Nixon (1995)          113987  10858   false   ⋯\n 15 │ 15       2.73098  3154    Cutthroat Island (1…  112760  1408    true    ⋯\n 16 │ 16       3.80236  21165   Casino (1995)         112641  524     false   ⋯\n 17 │ 17       3.95013  24552   Sense and Sensibili…  114388  4584    false   ⋯\n 18 │ 18       3.41207  6255    Four Rooms (1995)     113101  5       false   ⋯\n 19 │ 19       2.64201  24913   Ace Ventura: When N…  112281  9273    false   ⋯\n 20 │ 20       2.89448  4658    Money Train (1995)    113845  11517   true    ⋯\n 21 │ 21       3.56837  25699   Get Shorty (1995)     113161  8012    false   ⋯\n 22 │ 22       3.30114  11136   Copycat (1995)        112722  1710    false   ⋯\n 23 │ 23       3.1588   4871    Assassins (1995)      112401  9691    true    ⋯\n ⋮  │    ⋮        ⋮       ⋮              ⋮              ⋮       ⋮       ⋮     ⋱\n\n\nIn contrast to data from a designed experiment, like the English Lexicon Project, the data from this observational study are extremely unbalanced with respect to the observational grouping factors, userId and movieId. The movies table includes an nrtngs column that gives the number of ratings for each movie, which varies from 1 to nearly 100,000.\n\nextrema(movies.nrtngs)\n\n(1, 97999)\n\n\nThe number of ratings per user is also highly skewed\n\nusers = Table(\n  combine(\n    groupby(DataFrame(ratings), :userId),\n    nrow =&gt; :urtngs,\n    :rating =&gt; mean =&gt; :umnrtng,\n  ),\n)\n\nTable with 3 columns and 283228 rows:\n      userId  urtngs  umnrtng\n    ┌────────────────────────\n 1  │ 1       16      3.3125\n 2  │ 2       15      3.66667\n 3  │ 3       11      3.54545\n 4  │ 4       736     3.39742\n 5  │ 5       72      4.26389\n 6  │ 6       42      3.54762\n 7  │ 7       15      3.2\n 8  │ 8       31      3.35484\n 9  │ 9       1       5.0\n 10 │ 10      121     4.19008\n 11 │ 11      19      3.73684\n 12 │ 12      18      2.63889\n 13 │ 13      20      4.15\n 14 │ 14      174     3.94828\n 15 │ 15      162     4.0\n 16 │ 16      41      4.34146\n 17 │ 17      2       4.0\n 18 │ 18      92      3.68478\n 19 │ 19      262     3.43893\n 20 │ 20      4       2.75\n 21 │ 21      15      3.13333\n 22 │ 22      26      3.5\n 23 │ 23      17      3.58824\n ⋮  │   ⋮       ⋮        ⋮\n\n\n\nextrema(users.urtngs)\n\n(1, 23715)\n\n\nOne way of visualizing the imbalance in the number of ratings per movie or per user is as an empirical cumulative distribution function (ecdf) plot, which is a “stair-step” plot where the vertical axis is the proportion of observations less than or equal to the corresponding value on the horizontal axis. Because the distribution of the number of ratings per movie or per user is so highly skewed in the low range we use a logarithmic horizontal axis in Figure 5.1.\n\n\nCode\nlet\n  f = Figure(; resolution=(1000, 400))\n  xscale = log10\n  xminorticksvisible = true\n  xminorgridvisible = true\n  yminorticksvisible = true\n  xminorticks = IntervalsBetween(10)\n  ylabel = \"Relative cumulative frequency\"\n  nrtngs = sort(movies.nrtngs)\n  ecdfplot(\n    f[1, 1],\n    nrtngs;\n    npoints=last(nrtngs),\n    axis=(\n      xlabel=\"Number of ratings per movie (logarithmic scale)\",\n      xminorgridvisible,\n      xminorticks,\n      xminorticksvisible,\n      xscale,\n      ylabel,\n      yminorticksvisible,\n    ),\n  )\n  urtngs = sort(users.urtngs)\n  ecdfplot(\n    f[1, 2],\n    urtngs;\n    npoints=last(urtngs),\n    axis=(\n      xlabel=\"Number of ratings per user (logarithmic scale)\",\n      xminorgridvisible,\n      xminorticks,\n      xminorticksvisible,\n      xscale,\n      yminorticksvisible,\n    ),\n  )\n  f\nend\n\n\n\n\n\nFigure 5.1: Empirical distribution plots of the number of ratings per movie and per user. The horizontal axes are on a logarithmic scale.\n\n\n\n\nIn this collection of over 27 million ratings, nearly 20% of the movies are rated only once and nearly half of the movies were rated 6 or fewer times.\n\ncount(≤(6), movies.nrtngs) / length(movies.nrtngs)\n\n0.4967989756722151\n\n\nThe ecdf plot of the number of ratings per user shows a similar pattern to that of the movies — a few users with a very large number of ratings and many users with just a few ratings.\nFor example, about 20% of the users rated 10 movies or fewer; the median number of movies rated is around 30; but the maximum is close to 24,000 (which is a lot of movies - over 20 years of 3 movies per day every day - if this user actually watched all of them).\nMovies with very few ratings provide little information about overall trends or even about the movie being rated. We can imagine that the “shrinkage” of random effects for movies with just a few ratings pulls their adjusted rating strongly towards the overall average.\nFurthermore, the distribution of ratings for movies with only one rating is systematically lower than the distribution of ratings for movies rated at least five times.\nFirst, add nrtngs and urtngs as columns of the ratings table.\n\nratings = Table(\n  disallowmissing!(\n    leftjoin!(\n      leftjoin!(\n        DataFrame(ratings),\n        DataFrame(getproperties(movies, (:movieId, :nrtngs)));\n        on=:movieId,\n      ),\n      DataFrame(getproperties(users, (:userId, :urtngs)));\n      on=:userId,\n    ),\n  ),\n)\n\nTable with 5 columns and 27753444 rows:\n      userId  movieId  rating  nrtngs  urtngs\n    ┌────────────────────────────────────────\n 1  │ 1       307      3.5     7958    16\n 2  │ 1       481      3.5     6037    16\n 3  │ 1       1091     1.5     6138    16\n 4  │ 1       1257     4.5     5902    16\n 5  │ 1       1449     4.5     6867    16\n 6  │ 1       1590     2.5     8511    16\n 7  │ 1       1591     1.5     6508    16\n 8  │ 1       2134     4.5     7020    16\n 9  │ 1       2478     4.0     7797    16\n 10 │ 1       2840     3.0     6047    16\n 11 │ 1       2986     2.5     6060    16\n 12 │ 1       3020     4.0     7783    16\n 13 │ 1       3424     4.5     7265    16\n 14 │ 1       3698     3.5     8269    16\n 15 │ 1       3826     2.0     8898    16\n 16 │ 1       3893     3.5     5259    16\n 17 │ 2       170      3.5     9574    15\n 18 │ 2       849      3.5     9878    15\n 19 │ 2       1186     3.5     8643    15\n 20 │ 2       1235     3.0     9937    15\n 21 │ 2       1244     3.0     10249   15\n 22 │ 2       1296     4.5     7470    15\n 23 │ 2       1663     3.0     9581    15\n ⋮  │   ⋮        ⋮       ⋮       ⋮       ⋮\n\n\nthen create a bar chart of the two distributions\n\n\nCode\nlet\n  fiveplus = zeros(Int, 10)\n  onlyone = zeros(Int, 10)\n  for (i, n) in\n      zip(round.(Int, Float32(2) .* ratings.rating), ratings.nrtngs)\n    if n &gt; 4\n      fiveplus[i] += 1\n    elseif isone(n)\n      onlyone[i] += 1\n    end\n  end\n  fiveprop = fiveplus ./ sum(fiveplus)\n  oneprop = onlyone ./ sum(onlyone)\n  draw(\n    data((;\n      props=vcat(fiveprop, oneprop),\n      rating=repeat(0.5:0.5:5.0; outer=2),\n      nratings=repeat([\"≥5\", \"only 1\"]; inner=10),\n    )) *\n    mapping(\n      :rating =&gt; nonnumeric,\n      :props =&gt; \"Proportion of ratings\";\n      color=:nratings =&gt; \"Ratings/movie\",\n      dodge=:nratings,\n    ) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\nFigure 5.2: Distribution of ratings for movies with only one rating compared to movies with at least 5 ratings\n\n\n\n\nSimilarly, users who rate very few movies add little information, even about the movies that they rated, because there isn’t sufficient information to contrast a specific rating with a typical rating for the user.\nOne way of dealing with the extreme imbalance in the number of observations per user or per movie is to set a threshold on the number of observations for a user or a movie to be included in the data used to fit the model. For example, a companion data set on grouplens.org, available in the ml-25m.zip archive, included only users who had rated at least 20 movies.\nTo be able to select ratings on according to the number of ratings per user and the number of ratings per movie, we left-joined the movies.nrtngs and users.urtngs columns into the ratings data frame.\n\ndescribe(DataFrame(ratings))\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nAbstract…\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nuserId\n141942.0\n1\n142022.0\n283228\n0\nInt32\n\n\n2\nmovieId\n18488.0\n1\n2716.0\n193886\n0\nInt32\n\n\n3\nrating\n3.53045\n0.5\n3.5\n5.0\n0\nFloat32\n\n\n4\nnrtngs\n17238.2\n1\n10633.0\n97999\n0\nInt32\n\n\n5\nurtngs\n559.946\n1\n294.0\n23715\n0\nInt64\n\n\n\n\n\n\n\n\n\n\n\n\nSeemingly inconsistent medians of “nrtngs” and “urtngs”\n\n\n\n\n\nThe medians in this table of nrtngs and urtngs are much higher than the values from the movies and users tables because a movie with 98,000 ratings occurs 98,000 times in this table whereas it occurs only once in the movies table."
  },
  {
    "objectID": "largescaleobserved.html#sec-lrgobsmods",
    "href": "largescaleobserved.html#sec-lrgobsmods",
    "title": "5  A large-scale observational study",
    "section": "5.2 Models fit with lower bounds on ratings per user and per movie",
    "text": "5.2 Models fit with lower bounds on ratings per user and per movie\nWe fit a simple model to this dataset using different thresholds on the number of ratings per movie and the number of ratings per user. These fits were performed on compute servers with generous amounts of memory (128 GiB/node) and numbers of compute cores (48/node). A sample fit is shown in Section 5.2.2.\nThe results are summarized in the following table\n\n\nCode\nsizespeed = Table(dataset(:sizespeed))\n\n\nTable with 10 columns and 21 rows:\n      mc  uc  nratings  nusers  nmvie  modelsz  L22sz    nv  fittime  evtime\n    ┌────────────────────────────────────────────────────────────────────────\n 1  │ 1   5   27716748  266980  53883  24.3439  21.6318  20  7352.56  303.921\n 2  │ 1   10  27565774  243658  53852  24.3029  21.607   23  7463.83  297.491\n 3  │ 1   20  26544038  174605  53781  24.1394  21.55    26  8321.86  305.485\n 4  │ 2   5   27706599  266980  43734  16.961   14.2504  20  4938.6   245.938\n 5  │ 2   10  27555656  243658  43734  16.9449  14.2504  23  5594.63  202.176\n 6  │ 2   20  26533987  174605  43730  16.8358  14.2478  37  7451.41  187.425\n 7  │ 5   5   27671580  266979  30824  9.78469  7.07894  23  2703.42  113.091\n 8  │ 5   10  27520713  243658  30824  9.7686   7.07894  26  3523.98  139.028\n 9  │ 5   20  26499391  174605  30823  9.66167  7.07848  18  2255.44  123.423\n 10 │ 10  5   27624556  266979  23716  6.89142  4.19057  23  2016.23  81.2037\n 11 │ 10  10  27473781  243658  23716  6.87534  4.19057  23  1937.04  77.2586\n 12 │ 10  20  26452968  174605  23716  6.76892  4.19057  18  1560.85  81.4125\n 13 │ 15  5   27585626  266979  20400  5.79716  3.10063  23  1640.61  68.2099\n 14 │ 15  10  27434948  243658  20400  5.78108  3.10063  24  1773.55  70.9519\n 15 │ 15  20  26414555  174605  20400  5.6747   3.10063  19  1358.37  61.5033\n 16 │ 20  5   27551352  266978  18366  5.20626  2.51315  19  1266.38  63.8407\n 17 │ 20  10  27400762  243658  18366  5.1902   2.51315  35  2438.19  70.5935\n 18 │ 20  20  26380766  174605  18366  5.08385  2.51315  21  1489.64  70.6748\n 19 │ 50  5   27394411  266974  13360  4.00751  1.32985  26  1425.35  55.2993\n 20 │ 50  10  27244313  243656  13360  3.9915   1.32985  30  1660.57  52.5368\n 21 │ 50  20  26226280  174604  13360  3.88534  1.32985  20  1196.71  52.6929\n\n\nIn this table, mc is the “movie cutoff” (i.e. the threshold on the number of ratings per movie); uc is the user cutoff (threshold on the number of ratings per user); nratings, nusers and nmvie are the number of ratings, users and movies in the resulting trimmed data set; modelsz is the size (in GiB) of the model fit; L22sz is the size of the [2,2] block of the L matrix in that model; fittime is the time (in seconds) required to fit the model; nev is the number of function evaluations until convergence; and evtime is the time (s) per function evaluation.\nThe “[2,2] block of the L matrix” is described in Section 5.2.2.\n\n5.2.1 Dimensions of the model versus cut-off values\nFirst we consider the effects of the minimum number of ratings per user and per movie on the dimensions of the data set, as shown in Figure 5.3.\n\n\nCode\nlet\n  f = Figure(resolution=(800, 1000))\n  xlabel = \"Minimum number of ratings per movie\"\n  mc = refarray(sizespeed.mc)\n  xticks = (1:7, string.(refpool(sizespeed.mc)))\n  uc = refarray(sizespeed.uc)\n  Legend(\n    f[1, 1],\n    lelements,\n    llabels,\n    ltitle;\n    orientation=:horizontal,\n    tellwidth=false,\n    tellheight=true,\n  )\n  barplot!(\n    Axis(f[2, 1]; xticks, ylabel=\"Number of users\"),\n    mc,\n    sizespeed.nusers;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(f[3, 1]; xticks, ylabel=\"Number of movies\"),\n    mc,\n    sizespeed.nmvie;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(f[4, 1]; xlabel, xticks, ylabel=\"Number of observations\"),\n    mc,\n    sizespeed.nratings;\n    dodge=uc,\n    color=uc,\n  )\n  f\nend\n\n\n\n\n\nFigure 5.3: Bar plot of data set dimensions by minimum number of ratings per user and per movie.\n\n\n\n\nUnsurprisingly, Figure 5.3 shows that increasing the minimum number of ratings per user decreases the number of users, does not noticeably affect the number of movies, and results in small decreases in the total number of ratings. Conversely, increasing the minimum number of ratings per movie does not noticeably affect the number of users, causes dramatic reductions in the number of movies and has very little effect on the total number of ratings.\n\n\n5.2.2 Memory footprint of the model representation\nTo explain what “the [2,2] block of the L matrix” is and why its size is important, we provide a brief overview of the evaluation of the “profiled” log-likelihood for a LinearMixedModel representation.\nTo make the discussion concrete we consider one of the models represented in this table, with cut-offs of 20 ratings per user and 20 ratings per movie. This, and any of the models shown in the table, can be restored in a few minutes from the saved optsum values, as opposed to taking up to two hours to perform the fit.\n\n\nCode\nfunction ratingsoptsum(\n  mcutoff::Integer,\n  ucutoff::Integer;\n  data=Table(ratings),\n  form=@formula(rating ~ 1 + (1 | userId) + (1 | movieId)),\n  contrasts=Dict(:movieId =&gt; Grouping(), :userId =&gt; Grouping()),\n)\n  optsumfnm = optsumdir(\n    \"mvm$(lpad(mcutoff, 2, '0'))u$(lpad(ucutoff, 2, '0')).json\",\n  )\n  isfile(optsumfnm) ||\n    throw(ArgumentError(\"File $optsumfnm is not available\"))\n  return restoreoptsum!(\n    LinearMixedModel(\n      form,\n      filter(\n        r -&gt; (r.nrtngs ≥ mcutoff) & (r.urtngs ≥ ucutoff),\n        data,\n      );\n      contrasts,\n    ),\n    optsumfnm,\n  )\nend\nmvm20u20 = ratingsoptsum(20, 20)\nprintln(mvm20u20)\n\n\nLinear mixed model fit by maximum likelihood\n rating ~ 1 + (1 | userId) + (1 | movieId)\n     logLik       -2 logLik         AIC           AICc            BIC      \n\n\n -33644689.0320  67289378.0639  67289386.0639  67289386.0639  67289446.4165\n\nVariance components:\n\n\n\n            Column   Variance Std.Dev. \nuserId   (Intercept)  0.184529 0.429568\nmovieId  (Intercept)  0.242959 0.492909\nResidual              0.732964 0.856133\n Number of obs: 26380766; levels of grouping factors: 174605, 18366\n\n  Fixed-effects parameters:\n\n\n──────────────────────────────────────────────────\n               Coef.  Std. Error       z  Pr(&gt;|z|)\n──────────────────────────────────────────────────\n(Intercept)  3.43345   0.0038669  887.91    &lt;1e-99\n──────────────────────────────────────────────────\n\n\nCreating the model representation and restoring the optimal parameter values can take a couple of minutes because the objective is evaluated twice — at the initial parameter values and at the final parameter values — during the call to restoreoptsum!.\nEach evaluation of the objective, which requires setting the value of the parameter \\({\\boldsymbol\\theta}\\) in the numerical representation of the model, updating the blocked Cholesky factor, \\(\\mathbf{L}\\), and evaluating the scalar objective value from this factor, takes a little over a minute (71 seconds) on a server node and probably longer on a laptop.\nThe lower triangular L factor is large but sparse. It is stored in six blocks of dimensions and types as shown in\n\nBlockDescription(mvm20u20)\n\n\n\n\nrows\nuserId\nmovieId\nfixed\n\n\n\n\n174605\nDiagonal\n\n\n\n\n18366\nSparse\nDiag/Dense\n\n\n\n2\nDense\nDense\nDense\n\n\n\n\n\nThis display gives the types of two blocked matrices: A which is derived from the data and does not depend on the parameters, and L, which is derived from A and the \\({\\boldsymbol\\theta}\\) parameter. The only difference in their structures is in the [2,2] block, which is diagonal in A and a dense, lower triangular matrix in L.\nThe memory footprint (bytes) of each of the blocks is\n\n\nCode\nlet\n  block = String[]\n  for i in 1:3\n    for j in 1:i\n      push!(block, \"[$i,$j]\")\n    end\n  end\n  Table((;\n    block,\n    Abytes=Base.summarysize.(mvm20u20.A),\n    Lbytes=Base.summarysize.(mvm20u20.L),\n  ))\nend\n\n\nTable with 3 columns and 6 rows:\n     block  Abytes     Lbytes\n   ┌─────────────────────────────\n 1 │ [1,1]  1396888    1396888\n 2 │ [2,1]  317267776  317267776\n 3 │ [2,2]  146976     2698479688\n 4 │ [3,1]  2793720    2793720\n 5 │ [3,2]  293896     293896\n 6 │ [3,3]  72         72\n\n\nresulting in total memory footprints (GiB) of\n\n\nCode\nNamedTuple{(:A, :L)}(\n  Base.summarysize.(getproperty.(Ref(mvm20u20), (:A, :L))) ./ 2^30,\n)\n\n\n(A = 0.29979219287633896, L = 2.8128103613853455)\n\n\nThat is, L requires roughly 10 times the amount of storage as does A, and that difference is entirely due to the different structure of the [2,2] block.\nThis phenomenon of the Cholesky factor requiring more storage than the sparse matrix being factored is described as fill-in.\nNote that although the dimensions of the [2,1] block are larger than those of the [2,2] block its memory footprint is smaller because it is a sparse matrix. The matrix is over 99% zeros or, equivalently, less than 1% nonzeros,\n\nlet\n  L21 = mvm20u20.L[2]  # blocks are stored in a one-dimensional array\n  nnz(L21) / length(L21)\nend\n\n0.008226519768989443\n\n\nwhich makes the sparse representation much smaller than the dense representation.\nThis fill-in of the [2,2] block leads to a somewhat unintuitive conclusion. The memory footprint of the model representation depends strongly on the number of movies, less strongly on the number of users and almost not at all on the number of ratings. The first two parts of this conclusion are illustrated in Figure 5.4.\n\n\nCode\nlet\n  f = Figure(resolution=(800, 1000))\n  xlabel = \"Minimum number of ratings per movie\"\n  mc = refarray(sizespeed.mc)\n  xticks = (1:7, string.(refpool(sizespeed.mc)))\n  uc = refarray(sizespeed.uc)\n  Legend(\n    f[1, 1],\n    lelements,\n    llabels,\n    ltitle;\n    orientation=:horizontal,\n    tellwidth=false,\n    tellheight=true,\n  )\n  barplot!(\n    Axis(f[2, 1]; xticks, ylabel=\"Memory requirement (GiB)\"),\n    mc,\n    sizespeed.modelsz;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(f[3, 1]; xticks, ylabel=\"Size of L[2,2] (GiB)\"),\n    mc,\n    sizespeed.L22sz;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(f[4, 1]; xlabel, xticks, ylabel=\"L[2,2] memory fraction\"),\n    mc,\n    sizespeed.L22prop;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  f\nend\n\n\n\n\n\nFigure 5.4: Bar plot of memory footprint of the model representation by minimum number of ratings per user and per movie.\n\n\n\n\nFigure 5.4 shows that when all the movies are included in the data to which the model is fit (i.e. mc == 1) the total memory footprint is over 20 GiB, and nearly 90% of that memory is that required for the [2,2] block of L. Even when requiring a minimum of 50 ratings per movie, the [2,2] block of L is over 30% of the memory footprint.\nIn a sense this is good news because the amount of storage required for the [2,2] block can be nearly cut in half by taking advantage of the fact that it is a triangular matrix. The rectangular full packed format looks especially promising for this purpose.\nIn general, for models with scalar random effects for two incompletely crossed grouping factors, the memory footprint depends strongly on the smaller of the number of levels of the grouping factors, less strongly on the larger number, and almost not at all on the number of observations.\n\n\n5.2.3 Speed of log-likelihood evalation\nThe time required to fit a model to large data sets is dominated by the time required to evaluate the log-likelihood during the optimization of the parameter estimates. The time for one evaluation is given in the evtime column of sizespeed. Also given is the number of evaluations to convergence, nev, and the time to fit the model, fittime The reason for considering evtime in addition to fittime and nev is because the evtime for one model, relative to other models, is reasonably stable across computers whereas nev, and hence, fittime, can be affected by seemingly trivial variations in function values resulting from different implementations of low-level calculations, such as the BLAS (Basic Linear Algebra Subroutines).\nThat is, we can’t expect to reproduce nev exactly when fitting the same model on different computers or with slightly different versions of software but the pattern in evtime with respect to uc and mc can be expected to reproducible.\n\n\nCode\nlet\n  f = Figure(resolution=(800, 1000))\n  xlabel = \"Minimum number of ratings per movie\"\n  mc = refarray(sizespeed.mc)\n  xticks = (1:7, string.(refpool(sizespeed.mc)))\n  uc = refarray(sizespeed.uc)\n  Legend(\n    f[1, 1],\n    lelements,\n    llabels,\n    ltitle;\n    orientation=:horizontal,\n    tellwidth=false,\n    tellheight=true,\n  )\n  barplot!(\n    Axis(\n      f[2, 1];\n      xticks,\n      ylabel=\"Log-likelihood evaluation time (m)\",\n    ),\n    mc,\n    sizespeed.evtime ./ 60;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(\n      f[3, 1];\n      xticks,\n      ylabel=\"Number of evaluations to convergence\",\n    ),\n    mc,\n    sizespeed.nv;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  barplot!(\n    Axis(\n      f[4, 1];\n      xlabel,\n      xticks,\n      ylabel=\"Time (hr) to fit the model\",\n    ),\n    mc,\n    sizespeed.fittime ./ 3600;\n    xticks,\n    dodge=uc,\n    color=uc,\n  )\n  f\nend\n\n\n\n\n\nFigure 5.5: Bar plot of function log-likelihood evaluation time by minimum number of ratings per user and per movie.\n\n\n\n\nFigure 5.5 shows that the average evaluation time for the log-likelihood function depends strongly on the number of movies and less strongly on the number of users.\nHowever the middle panel shows that the number of iterations to convergence is highly variable. Most of these models required between 20 and 25 evaluations but some required almost 50 evaluations.\nThe derivation of the log-likelihood for linear mixed-effects models is given in Section B.7, which provides a rather remarkable result: the profiled log-likelihood for a linear mixed-effects model can be evaluated from Cholesky factor of a blocked, positive-definite symmetric matrix.\nThere are two blocked matrices, A and L, stored in the model representation and, for large models such as we are considering these are the largest fields in"
  },
  {
    "objectID": "largescaleobserved.html#model-size-and-speed-for-different-thresholds",
    "href": "largescaleobserved.html#model-size-and-speed-for-different-thresholds",
    "title": "5  A large-scale observational study",
    "section": "5.3 Model size and speed for different thresholds",
    "text": "5.3 Model size and speed for different thresholds\n\n\nCode\ndraw(\n  data(sizespeed) *\n  mapping(\n    :L22sz =&gt; \"Size (GiB) of 2,2 block of L\",\n    :modelsz =&gt; \"Size (GiB) of model representation\";\n    color=:uc,\n  ) *\n  visual(Scatter),\n)\n\n\n\n\n\nFigure 5.6: Amount of storage for L[2,2] versus amount of storage for the entire model. Colors are determined by the minimum number of ratings per user in the data to which the model was fit.\n\n\n\n\nLinear regression of the modelsz versus the number of users and L22sz.\n\nsizemod =\n  lm(@formula(Float64(modelsz) ~ 1 + nusers + L22sz), sizespeed)\ncoeftable(sizemod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nt\nPr(&gt;\nt\n)\n\n\n\n\n(Intercept)\n2.32966\n0.0134151\n173.66\n&lt;1e-29\n2.30147\n2.35784\n\n\nnusers\n1.3747e-6\n5.68925e-8\n24.16\n&lt;1e-14\n1.25517e-6\n1.49423e-6\n\n\nL22sz\n1.00125\n0.000321437\n3114.92\n&lt;1e-52\n1.00058\n1.00193\n\n\n\n\n\nprovides an \\(r^2\\) value very close to one, indicating an almost perfect fit.\n\nr²(sizemod)\n\n0.9999981449902416\n\n\n\n\n\n\nBalota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The English lexicon project. Behavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/bf03193014\n\n\nHarper, F. M., & Konstan, J. A. (2016). The MovieLens datasets. ACM Transactions on Interactive Intelligent Systems, 5(4), 1–19. https://doi.org/10.1145/2827872"
  },
  {
    "objectID": "glmmbernoulli.html#sec-contraception",
    "href": "glmmbernoulli.html#sec-contraception",
    "title": "6  Generalized Linear Mixed Models for Binary Responses",
    "section": "6.1 Artificial contraception use in regions of Bangladesh",
    "text": "6.1 Artificial contraception use in regions of Bangladesh\nOne of the test data sets from the Center for Multilevel Modelling, University of Bristol is derived from the 1989 Bangladesh Fertility Survey, (Huq & Cleland, 1990). The data are a subsample of 1934 women selected from 60 of the 64 political districts or zila, available as the contra data set in the MixedModels package.\n\ncontra = Table(dataset(:contra))\n\nTable with 5 columns and 1934 rows:\n      dist  urban  livch  age     use\n    ┌────────────────────────────────\n 1  │ D01   Y      3+     18.44   N\n 2  │ D01   Y      0      -5.56   N\n 3  │ D01   Y      2      1.44    N\n 4  │ D01   Y      3+     8.44    N\n 5  │ D01   Y      0      -13.56  N\n 6  │ D01   Y      0      -11.56  N\n 7  │ D01   Y      3+     18.44   N\n 8  │ D01   Y      3+     -3.56   N\n 9  │ D01   Y      1      -5.56   N\n 10 │ D01   Y      3+     1.44    N\n 11 │ D01   Y      0      -11.56  Y\n 12 │ D01   Y      0      -2.56   N\n 13 │ D01   Y      1      -4.56   N\n 14 │ D01   Y      3+     5.44    N\n 15 │ D01   Y      3+     -0.56   N\n 16 │ D01   Y      3+     4.44    Y\n 17 │ D01   Y      0      -5.56   N\n 18 │ D01   Y      3+     -0.56   Y\n 19 │ D01   Y      1      -6.56   Y\n 20 │ D01   Y      2      -3.56   N\n 21 │ D01   Y      0      -4.56   N\n 22 │ D01   Y      0      -9.56   N\n 23 │ D01   Y      3+     2.44    N\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮\n\n\nwith summary\n\n\nCode\nlet df = DataFrame(contra)\n  describe(df, :mean, :min, :median, :max, :nunique, :eltype)\nend\n\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnunique\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nUnion…\nDataType\n\n\n\n\n1\ndist\n\nD01\n\nD61\n60\nString\n\n\n2\nurban\n\nN\n\nY\n2\nString\n\n\n3\nlivch\n\n0\n\n3+\n4\nString\n\n\n4\nage\n0.00204757\n-13.56\n-1.56\n19.44\n\nFloat64\n\n\n5\nuse\n\nN\n\nY\n2\nString\n\n\n\n\n\n\nThe response of interest is use, whether the woman chooses to use artificial contraception, which is a binary response with only two possible values, N and Y. The covariates include the district (dist) in which the woman resides, the number of live children she currently has (livch, coded as 0, 1, 2, and 3+), her age, and urban, also coded as N and Y, indicating rural or urban.\nNote that the mean of these age values is close to zero but not exactly zero. This occurs when the values have been centered about the sample mean then rounded. In this case it appears that the ages were recorded as a whole number of years and the mean was rounded to two decimal places ending in .56. Thus, all the negative values end in .56 and the positive values end in .44. The mean of these rounded, centered values is not exactly zero because of the rounding after centering.\nCentering the ages allows for meaningful interpretation of intercepts and fixed effects for other covariates, because they refer to an age within the range of the observed ages. Regretably, the information on what the centering age (i.e. the original mean age) was does not seem to be available.\n\n6.1.1 Plotting the binary response\nProducing informative graphical displays of a binary response as it relates to covariates is somewhat more challenging that the corresponding plots for responses on a continuous scale. If we were to plot the 1934 responses as 0/1 values versus, for example, the woman’s centered age, we would end up with a rather uninformative plot, because all the points would fall on one of two horizontal lines, at \\(y=0\\) and \\(y=1\\).\nOne approach to illustrating the structure of the data more effectively is to add scatterplot smoother lines to the plot, as in Figure 6.1,\n\n\nCode\ndraw(\n  data(contra) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :use =&gt; ==(\"Y\") =&gt; \"Frequency of contraceptive use\";\n    col=:urban =&gt; renamer([\"N\" =&gt; \"Rural\", \"Y\" =&gt; \"Urban\"]),\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 6.1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\nto show the trend in the response with respect to the covariate. Once we have the smoother lines in such a plot we can omit the data points themselves, as we did here, because they add very little information.\nThe first thing to notice about the plot is that the proportion of women using contraception is not linear in age, which, on reflection, makes sense. A woman in the middle of this age range (probably corresponding to an age around 25) is more likely to use artificial contraception than is a girl in her early teens or a woman in her mid-forties. We also see that women in an urban setting are more likely to use contraception than those in a rural setting and that women with no live children are less likely to use contraception than are women who already have children. There do not seem to be strong differences between women who have 1, 2 or 3 or more children compared to the differences between women with children and those without children.\nInterestingly, the quadratic pattern with respect to age does not seem to have been noticed in other analyses of these data. Comparisons of model fits through different software systems, as provided by the Center for Multilevel Modelling, incorporate only a linear term in age, even though the pattern is clearly nonlinear. The lesson here is similar to what we have seen in other examples; careful plotting of the data should, whenever possible, precede attempts to fit models to the data.\n\n\n6.1.2 Initial GLMM fit to the contraception data\nFitting a generalized linear mixed model (GLMM) is very similar to fitting a linear mixed model (LMM). We call the fit function with the first three arguments being the model type, MixedModel, then a formula specifying the response, fixed-effects terms and random-effects terms, then a data table. For GLMMs we also specify a fourth positional argument which is a distribution - in this case Bernoulli().\nEstablishing the contrasts and fitting a preliminary model with random effects for dist, main effects for livch, age, age^2, and urban, plus interaction terms for age & urban and age^2 & urban can be done as\n\ncontrasts[:livch] = EffectsCoding(; base=\"0\")\ncontrasts[:urban] = HelmertCoding()\ncontrasts[:dist] = Grouping()\n\ncom01 = let\n  f = @formula use ~\n    1 + livch + (age + abs2(age)) * urban + (1 | dist)\n  fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ)\nend\n\nMinimizing 822   Time: 0:00:01 ( 1.34 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.0127\n0.1058\n-0.12\n0.9041\n0.4787\n\n\nlivch: 1\n0.1532\n0.0982\n1.56\n0.1187\n\n\n\nlivch: 2\n0.2562\n0.1045\n2.45\n0.0142\n\n\n\nlivch: 3+\n0.2591\n0.1022\n2.53\n0.0113\n\n\n\nage\n0.0006\n0.0095\n0.06\n0.9482\n\n\n\nabs2(age)\n-0.0047\n0.0008\n-6.12\n&lt;1e-09\n\n\n\nurban: Y\n0.3770\n0.0804\n4.69\n&lt;1e-05\n\n\n\nage & urban: Y\n-0.0067\n0.0069\n-0.98\n0.3258\n\n\n\nabs2(age) & urban: Y\n-0.0004\n0.0007\n-0.51\n0.6078\n\n\n\n\n\n\nNotice that in the formula language defined by the StatsModels package, an interaction term is written with the & operator. Crossing of terms, which generates main effects and interactions, is written with the * operator (as in the formula language in R). An interaction of a numeric variable with itself is performed by multiplication so the coefficient labelled age & age in the table is the quadratic term in age. (Notice that, in this formula language, age * age, which may easily be interpreted as age^2, expands to age + age^2.) Thus, what is written in the formula as a three-way interaction age * age * urban becomes an urban contrast plus linear and quadratic terms for age and each of their interactions with the urban contrast, which will be coded as -1 for N and +1 for Y.\nA fifth positional argument can be used to specify the link function, described in Section 6.2, but in most cases the canonical link function for the distribution is used. In the case of the Bernoulli distribution the canonical link is the logit link.\nAs for LMMs, the named argument contrasts specifies the contrasts to apply to some of the covariates in a key-value dictionary. Another named argument, nAGQ, specifies the number of quadrature points to use in an adaptive Gauss-Hermite quadrature rule for evaluating the deviance (see Section C.6 for details). A small, odd number, such as nAGQ=9 defined in the first code block of this chapter, is the preferred choice.\nThe interpretation of the coefficients in this model is somewhat different from the linear mixed models coefficients that we examined previously, but many of the model-building steps are similar. A rough assessment of the utility of a particular term in the fixed-effects part of the model can be obtained from examining the estimates of the coefficients associated with it and their standard errors, which are the basis for the z (z-statistic) and p (p-value) columns in the table. However, these p-values are even more approximate than those provided for LMMs. To perform a more accurate test of whether a particular term is useful we omit it from the model, refit and compare the reduced model fit to the original according to the change in deviance.\nWe will examine the terms in the model first and discuss the interpretation of the coefficients in Section 6.2.\n\n\n6.1.3 Model building for the contra data\nWe noted that Figure 6.1 shows similar patterns for women with children, whether they have one, two, or three or more children. We have set the contrasts for the livch factor to be offsets relative to the reference level, in this case women who do not have any live children. Although the coefficients labeled livch: 1, livch: 2, and livch: 3+ are all large relative to their standard errors, they are reasonably close to each other. This confirms our earlier impression that the main distinction is between women with children and those without and, for those who do have children, the number of children is not an important distinction.\nAfter incorporating a new variable ch — an indicator of whether the woman has any children — in the data,\n\ncontrasts[:ch] = HelmertCoding();\ncontra = Table(contra; ch=contra.livch .!= \"0\")\n\nTable with 6 columns and 1934 rows:\n      dist  urban  livch  age     use  ch\n    ┌───────────────────────────────────────\n 1  │ D01   Y      3+     18.44   N    true\n 2  │ D01   Y      0      -5.56   N    false\n 3  │ D01   Y      2      1.44    N    true\n 4  │ D01   Y      3+     8.44    N    true\n 5  │ D01   Y      0      -13.56  N    false\n 6  │ D01   Y      0      -11.56  N    false\n 7  │ D01   Y      3+     18.44   N    true\n 8  │ D01   Y      3+     -3.56   N    true\n 9  │ D01   Y      1      -5.56   N    true\n 10 │ D01   Y      3+     1.44    N    true\n 11 │ D01   Y      0      -11.56  Y    false\n 12 │ D01   Y      0      -2.56   N    false\n 13 │ D01   Y      1      -4.56   N    true\n 14 │ D01   Y      3+     5.44    N    true\n 15 │ D01   Y      3+     -0.56   N    true\n 16 │ D01   Y      3+     4.44    Y    true\n 17 │ D01   Y      0      -5.56   N    false\n 18 │ D01   Y      3+     -0.56   Y    true\n 19 │ D01   Y      1      -6.56   Y    true\n 20 │ D01   Y      2      -3.56   N    true\n 21 │ D01   Y      0      -4.56   N    false\n 22 │ D01   Y      0      -9.56   N    false\n 23 │ D01   Y      3+     2.44    N    true\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮     ⋮\n\n\n\n\nCode\nlet df = DataFrame(contra)\n  describe(df, :mean, :min, :median, :max, :nunique, :eltype)\nend\n\n\n6×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnunique\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nUnion…\nDataType\n\n\n\n\n1\ndist\n\nD01\n\nD61\n60\nString\n\n\n2\nurban\n\nN\n\nY\n2\nString\n\n\n3\nlivch\n\n0\n\n3+\n4\nString\n\n\n4\nage\n0.00204757\n-13.56\n-1.56\n19.44\n\nFloat64\n\n\n5\nuse\n\nN\n\nY\n2\nString\n\n\n6\nch\n0.725957\nfalse\n1.0\ntrue\n\nBool\n\n\n\n\n\n\nwe fit a reduced model.\n\ncom02 =\n  let f = @formula use ~ 1 + ch + age * age * urban + (1 | dist)\n    fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ)\n  end\n\nMinimizing 435   Time: 0:00:00 ( 0.93 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.2194\n0.1155\n-1.90\n0.0575\n0.4773\n\n\nch: true\n0.4342\n0.0741\n5.86\n&lt;1e-08\n\n\n\nage\n0.0035\n0.0082\n0.43\n0.6674\n\n\n\nurban: Y\n0.3733\n0.0801\n4.66\n&lt;1e-05\n\n\n\nage & age\n-0.0048\n0.0008\n-6.27\n&lt;1e-09\n\n\n\nage & urban: Y\n-0.0068\n0.0069\n-0.99\n0.3228\n\n\n\nage & age & urban: Y\n-0.0004\n0.0007\n-0.49\n0.6273\n\n\n\n\n\n\nComparing this model to the previous model\n\nMixedModels.likelihoodratiotest(com02, com01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nuse ~ 1 + ch + age + urban + age & age + age & urban + age & age & urban + (1 | dist)\n8\n2371\n\n\n\n\n\nuse ~ 1 + livch + age + :(abs2(age)) + urban + age & urban + :(abs2(age)) & urban + (1 | dist)\n10\n2371\n0\n2\n0.7823\n\n\n\n\n\nindicates that the reduced model is adequate.\nApparently neither the second-order interaction age & urban nor the third-order interaction age & age & urban is significant and we fit a model without these terms.\n\ncom03 =\n  let f = @formula use ~ 1 + urban + ch + age * age + (1 | dist)\n    fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ)\n  end\n\nMinimizing 137   Time: 0:00:00 ( 0.91 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.2303\n0.1140\n-2.02\n0.0434\n0.4774\n\n\nurban: Y\n0.3462\n0.0599\n5.78\n&lt;1e-08\n\n\n\nch: true\n0.4303\n0.0737\n5.84\n&lt;1e-08\n\n\n\nage\n0.0063\n0.0078\n0.80\n0.4246\n\n\n\nage & age\n-0.0046\n0.0007\n-6.47\n&lt;1e-10\n\n\n\n\n\n\nA likelihood ratio test\n\nMixedModels.likelihoodratiotest(com03, com02)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nuse ~ 1 + urban + ch + age + age & age + (1 | dist)\n6\n2373\n\n\n\n\n\nuse ~ 1 + ch + age + urban + age & age + age & urban + age & age & urban + (1 | dist)\n8\n2371\n2\n2\n0.4119\n\n\n\n\n\nindicates that these terms can safely be eliminated.\nA plot of the smoothed observed proportions versus centered age by urban and ch, Figure 6.2,\n\n\nCode\ndraw(\n  data(contra) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :use =&gt; ==(\"Y\") =&gt; \"Frequency of contraceptive use\";\n    col=:urban =&gt; renamer([\"N\" =&gt; \"Rural\", \"Y\" =&gt; \"Urban\"]),\n    color=:ch =&gt; \"Children\",\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 6.2: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey. The livch factor has been collapsed to children/nochildren.\n\n\n\n\nindicates that all four groups have a quadratic trend with respect to age but the location of the peak proportion is shifted for those without children relative to those with children. Incorporating an interaction of age and ch allows for such a shift.\n\ncom04 =\n  let f =\n      @formula use ~ 1 + urban + ch * age + abs2(age) + (1 | dist)\n    fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ)\n  end\n\nMinimizing 135   Time: 0:00:00 ( 0.96 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.84\n0.0046\n0.4756\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n&lt;1e-08\n\n\n\nch: true\n0.6054\n0.1035\n5.85\n&lt;1e-08\n\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n&lt;1e-11\n\n\n\nch: true & age\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\nComparing this fitted model to the previous one\n\nMixedModels.likelihoodratiotest(com03, com04)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(&gt;χ²)\n\n\n\n\nuse ~ 1 + urban + ch + age + age & age + (1 | dist)\n6\n2373\n\n\n\n\n\nuse ~ 1 + urban + ch + age + :(abs2(age)) + ch & age + (1 | dist)\n7\n2365\n8\n1\n0.0047\n\n\n\n\n\nconfirms the usefulness of this term.\nA series of such model fits led to a model with random effects for the combinations of dist and urban, because differences between urban and rural women in the same district were comparable to differences between districts, even after accounting for an effect of urban at the fixed-effects (or population) level.\n\ncom05 =\n  let f = @formula use ~\n      1 + urban + ch * age + abs2(age) + (1 | dist & urban)\n    fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ)\n  end\n\nMinimizing 148   Time: 0:00:00 ( 0.97 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist & urban\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n&lt;1e-05\n\n\n\nch: true\n0.6064\n0.1045\n5.80\n&lt;1e-08\n\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2472\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n&lt;1e-10\n\n\n\nch: true & age\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nIn more detail,\n\n\nCode\nprintln(com05)\n\n\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 9)\n  use ~ 1 + urban + ch + age + :(abs2(age)) + ch & age + (1 | dist & urban)\n  Distribution: \n\n\nBernoulli{Float64}\n  Link: LogitLink()\n\n   logLik    deviance     AIC       AICc        BIC    \n -1177.2418\n\n\n  2353.8242  2368.4836  2368.5418  2407.4550\n\nVariance components:\n                Column   Variance Std.Dev. \ndist & urban (Intercept)  0.331932 0.576135\n\n Number of obs: 1934; levels of grouping factors: 102\n\nFixed-effects parameters:\n────────────────────────────────────────────────────────\n                     Coef.   Std. Error      z  Pr(&gt;|z|)\n────────────────────────────────────────────────────────\n\n\n(Intercept)     -0.341466   0.126906     -2.69    0.0071\nurban: Y         0.393592   0.0859089     4.58    &lt;1e-05\nch: true         0.606443   0.104529      5.80    &lt;1e-08\nage             -0.0129094  0.0111548    -1.16    0.2472\nabs2(age)       -0.0056246  0.000844099  -6.66    &lt;1e-10\nch: true & age   0.0332096  0.0128227     2.59    0.0096\n────────────────────────────────────────────────────────\n\n\nNotice that, although there are 60 distinct districts, there are only 102 distinct combinations of dist and urban represented in the data. In 15 of the 60 districts there are no rural women in the sample and in 3 districts there are no urban women in the sample, as shown in a frequency table\n\n\nCode\nfreqtable(contra, :urban, :dist)\n\n\n2×60 Named Matrix{Int64}\nurban ╲ dist │ D01  D02  D03  D04  D05  D06  …  D56  D57  D58  D59  D60  D61\n─────────────┼──────────────────────────────────────────────────────────────\nN            │  54   20    0   19   37   58  …   24   23   20   10   22   31\nY            │  63    0    2   11    2    7  …   21    4   13    0   10   11"
  },
  {
    "objectID": "glmmbernoulli.html#sec-glmmlink",
    "href": "glmmbernoulli.html#sec-glmmlink",
    "title": "6  Generalized Linear Mixed Models for Binary Responses",
    "section": "6.2 Link functions and interpreting coefficients",
    "text": "6.2 Link functions and interpreting coefficients\nTo this point the only difference we have encountered between fitting generalized linear mixed models (GLMMs) and linear mixed models (LMMs) is the need to specify the distribution family in a call to fit. The formula specification is identical and the assessment of the significance of terms using likelihood ratio tests is similar. This is intentional. We have emphasized the use of likelihood ratio tests on terms, whether fixed-effects or random-effects terms, exactly so the approach will be general.\nHowever, the interpretation of the coefficient estimates in the different types of models is different. In a linear mixed model the conditional mean (or “expected value”) of the response given the random effects is simply the value of the linear predictor, \\({\\boldsymbol{\\mu}}={\\boldsymbol{\\eta}}={\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\mathbf{b}}\\) . That is, if we assume that we know the values of the fixed-effects parameters, \\({\\boldsymbol{\\beta}}\\), and the random effects, \\({\\mathbf{b}}\\), then the expected response for a particular combination of covariate values is a linear combination of these coefficients where the particular linear combination is determined by values of the covariates for that observation. Individual coefficients can be interpreted as slopes of the fitted response with respect to a numeric covariate or as shifts between levels of a categorical covariate.\nIt is worthwhile emphasizing this relationship, and how it is used to form predictions from the model, by illustrating the process on a grid of covariate values. To this end, we will take a brief excursion into creating grids of covariate values and evaluating linear predictors.\n\n6.2.1 Creating grids of covariate values\nSuppose that we wish to plot the “population” linear predictor values from model com05. That is, we will consider only the fixed-effects terms in the model formula and ignore the random effects. We want to create a plot like Figure 6.2 by evaluating the linear predictor for a range of age values for each of the combinations of ch and urban.\nFirst we construct a table of covariate values, newdata, containing the Cartesian product of values of these covariates, created using a generator expression returning NamedTuples, which is the standard row-wise represention of tabular data.\n\nnewdata = Table(\n  (; age, ch, urban) for age in -10:3:20, ch in [false, true],\n  urban in [\"N\", \"Y\"]\n)\n\nTable with 3 columns and 44 rows:\n      age  ch     urban\n    ┌──────────────────\n 1  │ -10  false  N\n 2  │ -7   false  N\n 3  │ -4   false  N\n 4  │ -1   false  N\n 5  │ 2    false  N\n 6  │ 5    false  N\n 7  │ 8    false  N\n 8  │ 11   false  N\n 9  │ 14   false  N\n 10 │ 17   false  N\n 11 │ 20   false  N\n 12 │ -10  true   N\n 13 │ -7   true   N\n 14 │ -4   true   N\n 15 │ -1   true   N\n 16 │ 2    true   N\n 17 │ 5    true   N\n 18 │ 8    true   N\n 19 │ 11   true   N\n 20 │ 14   true   N\n 21 │ 17   true   N\n 22 │ 20   true   N\n 23 │ -10  false  Y\n ⋮  │  ⋮     ⋮      ⋮\n\n\nNext we isolate the fixed-effects terms from the formula for model com05 by selecting its rhs property (the right-hand side of the formula) then filtering out any random-effects terms in the expression.\n\nfeform = filter(\n  t -&gt; !isa(t, MixedModels.AbstractReTerm),\n  com05.formula.rhs,\n);\n\n\n\n\n\n\n\nNote\n\n\n\nAdd extractors for different parts of the formula to MixedModels.jl\n\n\nFinally, we evaluate the model columns for this reduced formula. These are returned as an array of matrices, in this case containing only one matrix.\n\nnewX = only(StatsModels.modelcols(feform, newdata))\n\n44×6 Matrix{Float64}:\n 1.0  -1.0  -1.0  -10.0  100.0   10.0\n 1.0  -1.0  -1.0   -7.0   49.0    7.0\n 1.0  -1.0  -1.0   -4.0   16.0    4.0\n 1.0  -1.0  -1.0   -1.0    1.0    1.0\n 1.0  -1.0  -1.0    2.0    4.0   -2.0\n 1.0  -1.0  -1.0    5.0   25.0   -5.0\n 1.0  -1.0  -1.0    8.0   64.0   -8.0\n 1.0  -1.0  -1.0   11.0  121.0  -11.0\n 1.0  -1.0  -1.0   14.0  196.0  -14.0\n 1.0  -1.0  -1.0   17.0  289.0  -17.0\n 1.0  -1.0  -1.0   20.0  400.0  -20.0\n 1.0  -1.0   1.0  -10.0  100.0  -10.0\n 1.0  -1.0   1.0   -7.0   49.0   -7.0\n ⋮                                ⋮\n 1.0   1.0  -1.0   20.0  400.0  -20.0\n 1.0   1.0   1.0  -10.0  100.0  -10.0\n 1.0   1.0   1.0   -7.0   49.0   -7.0\n 1.0   1.0   1.0   -4.0   16.0   -4.0\n 1.0   1.0   1.0   -1.0    1.0   -1.0\n 1.0   1.0   1.0    2.0    4.0    2.0\n 1.0   1.0   1.0    5.0   25.0    5.0\n 1.0   1.0   1.0    8.0   64.0    8.0\n 1.0   1.0   1.0   11.0  121.0   11.0\n 1.0   1.0   1.0   14.0  196.0   14.0\n 1.0   1.0   1.0   17.0  289.0   17.0\n 1.0   1.0   1.0   20.0  400.0   20.0\n\n\nThe predicted linear predictor, \\({\\boldsymbol{\\eta}}\\), for the newdata table and the fixed-effects estimate for model com05 is the product of this model matrix and the fixef coefficients.\n\nnewdata = Table(newdata; η=newX * fixef(com05))\n\nTable with 4 columns and 44 rows:\n      age  ch     urban  η\n    ┌─────────────────────────────\n 1  │ -10  false  N      -1.44277\n 2  │ -7   false  N      -1.29427\n 3  │ -4   false  N      -1.24702\n 4  │ -1   false  N      -1.30101\n 5  │ 2    false  N      -1.45624\n 6  │ 5    false  N      -1.71271\n 7  │ 8    false  N      -2.07043\n 8  │ 11   false  N      -2.52939\n 9  │ 14   false  N      -3.08959\n 10 │ 17   false  N      -3.75103\n 11 │ 20   false  N      -4.51372\n 12 │ -10  true   N      -0.894077\n 13 │ -7   true   N      -0.546322\n 14 │ -4   true   N      -0.299809\n 15 │ -1   true   N      -0.15454\n 16 │ 2    true   N      -0.110513\n 17 │ 5    true   N      -0.167729\n 18 │ 8    true   N      -0.326188\n 19 │ 11   true   N      -0.58589\n 20 │ 14   true   N      -0.946834\n 21 │ 17   true   N      -1.40902\n 22 │ 20   true   N      -1.97245\n 23 │ -10  false  Y      -0.655587\n ⋮  │  ⋮     ⋮      ⋮        ⋮\n\n\nPlotting the result, Figure 6.3,\n\n\nCode\ndraw(\n  data(newdata) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :η =&gt; \"Linear predictor value from model com05\";\n    col=:urban =&gt; renamer([\"N\" =&gt; \"Rural\", \"Y\" =&gt; \"Urban\"]),\n    color=:ch =&gt; \"Children\",\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 6.3: Linear predictor versus centered age from model com05\n\n\n\n\nshows that these curves follow the general trends of the data plot. However, the vertical axis is not on a probability scale. Indeed most of the values of the linear predictor are negative.\nThis brings us to the topic of link functions.\n\n\n6.2.2 The logit link function for binary responses\nThe probability model for a binary response is the Bernoulli distribution, which is a very simple probability distribution in that its “support” — the set of possible values of the random variable — is just 0 or 1. If the probability of the response 1 is \\(p\\) then the probability of 0 must be \\(1-p\\). It is easy to establish that the expected value is also \\(p\\). For consistency across distribution families we write this expected response as \\(\\mu\\) instead of \\(p\\). We should, however, keep in mind that, for this distribution, \\(\\mu\\) corresponds to a probability and hence must satisfy \\(0\\le\\mu\\le 1\\).\nIn general we don’t want to have restrictions on the values of the linear predictor so we equate the linear predictor to a function of \\(\\mu\\) that has an unrestricted range. In the case of the Bernoulli distribution with the canonical link function we equate the linear predictor to the log odds or logit of the positive response. That is \\[\n\\eta = \\log\\left(\\frac{\\mu}{1-\\mu}\\right) .\n\\tag{6.2}\\]\nTo understand why this is called the “log odds” recall that \\(\\mu\\) corresponds to a probability in \\([0,1]\\). The corresponding odds ratio, \\(\\frac{\\mu}{1-\\mu}\\), is in \\([0,\\infty)\\) and the logarithm of the odds ratio, \\(\\mathrm{logit}(\\mu)\\), is in \\((-\\infty, \\infty)\\).\nThe inverse of the logit link function, \\[\n\\mu = \\frac{1}{1+\\exp(-\\eta)} ,\n\\tag{6.3}\\] is called the logistic function and is shown in Figure 6.4.\n\n\nCode\nlet\n  fig = Figure(; resolution=(800, 400))\n  ax = Axis(fig[1, 1]; xlabel=\"η\", ylabel=\"μ\")\n  lines!(ax, -5.5 .. 5.5, η -&gt; inv(1 + exp(-η)))\n  fig\nend\n\n\n\n\n\nFigure 6.4: The logistic function, which is the inverse to the logit link function.\n\n\n\n\nThe inverse link takes a value on the unrestricted range, \\((-\\infty,\\infty)\\), and maps it to the probability range, \\([0,1]\\). It happens this function is also the cumulative distribution function for the standard logistic distribution, available in Distributions.jl as cdf(Logistic(), η). In some presentations the relationship between the logit link and the logistic distribution is emphasized but that often leads to questions of why we should focus on the logistic distribution. Also, it is not clear how this approach would generalize to other distributions such as the Poisson or the Gamma distributions.\n\n\n6.2.3 Canonical link functions\nA way of deriving the logit link that does generalize to a class of common distributions, in what is called the exponential family, is to consider the logarithm of the probability function (for discrete distributions) or the probability density function (for continuous distributions). The probability function for the Bernoulli distribution is \\(\\mu\\) for \\(y=1\\) and \\(1-\\mu\\) for \\(y=0\\). If we write this in a somewhat contrived way as \\(\\mu^y+(1-\\mu)^{1-y}\\) for \\(y\\in\\{0,1\\}\\) then the logarithm of the probability function becomes \\[\n\\log\\left(\\mu^y+(1-\\mu)^{1-y}\\right) = \\log(1-\\mu) +\ny\\,\\log\\left(\\frac{\\mu}{1-\\mu}\\right) .\n\\tag{6.4}\\] Notice that the logit link function is the multiple of \\(y\\) in the last term.\nThe characteristic of distributions in the exponential family is that the logarithm of the probability mass function or probability density function, whichever is appropriate, can be expressed as a sum of up to three terms: one that involves \\(y\\) only, one that involves the parameters only, and the product of \\(y\\) and a function of the parameters. This function is the canonical link for the distribution.\nIn the case of the Poisson distribution the probability function is \\(\\frac{e^{-\\mu}\\mu^y}{y!}\\) for \\(y\\in\\{0,1,2,\\dots\\}\\) so the log probability function is \\[\n-\\log(y!)-\\mu+y\\log(\\mu) .\n\\tag{6.5}\\] and the canonical link function is \\(\\log(\\mu)\\).\n\n\n6.2.4 Interpreting coefficient estimates\nReturning to the interpretation of the estimated coefficients in model com05 we apply exactly the same interpretation as for a linear mixed model but taking into account that slopes or differences in levels are with respect to the logit or log-odds function. If we wish to express results in the probability scale then we should apply the logistic function to whatever combination of coefficients is of interest to us.\n\nlogistic(η) = inv(one(η) + exp(-η))\nnewdata = Table(newdata; μ=logistic.(newdata.η))\n\nTable with 5 columns and 44 rows:\n      age  ch     urban  η          μ\n    ┌────────────────────────────────────────\n 1  │ -10  false  N      -1.44277   0.191117\n 2  │ -7   false  N      -1.29427   0.21513\n 3  │ -4   false  N      -1.24702   0.223217\n 4  │ -1   false  N      -1.30101   0.213996\n 5  │ 2    false  N      -1.45624   0.189043\n 6  │ 5    false  N      -1.71271   0.152812\n 7  │ 8    false  N      -2.07043   0.112005\n 8  │ 11   false  N      -2.52939   0.0738236\n 9  │ 14   false  N      -3.08959   0.0435388\n 10 │ 17   false  N      -3.75103   0.0229542\n 11 │ 20   false  N      -4.51372   0.0108389\n 12 │ -10  true   N      -0.894077  0.290269\n 13 │ -7   true   N      -0.546322  0.366718\n 14 │ -4   true   N      -0.299809  0.425604\n 15 │ -1   true   N      -0.15454   0.461442\n 16 │ 2    true   N      -0.110513  0.4724\n 17 │ 5    true   N      -0.167729  0.458166\n 18 │ 8    true   N      -0.326188  0.419168\n 19 │ 11   true   N      -0.58589   0.357579\n 20 │ 14   true   N      -0.946834  0.279522\n 21 │ 17   true   N      -1.40902   0.196389\n 22 │ 20   true   N      -1.97245   0.122126\n 23 │ -10  false  Y      -0.655587  0.341732\n ⋮  │  ⋮     ⋮      ⋮        ⋮          ⋮\n\n\nproducing the population predictions on the probability scale, as shown in Figure 6.5.\n\n\nCode\ndraw(\n  data(newdata) *\n  mapping(\n    :age =&gt; \"Centered age (yr)\",\n    :μ =&gt; \"Probability of contraceptive use\";\n    col=:urban =&gt; renamer([\"N\" =&gt; \"Rural\", \"Y\" =&gt; \"Urban\"]),\n    color=:ch =&gt; \"Children\",\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 6.5: Predicted probability of contraception use versus centered age from model com05.\n\n\n\n\nOn the probability scale we can compare the predictions to the observed frequencies shown with scatterplot smoother lines in Figure 6.2.\nConsider the predictions on both the linear predictor and probability (or expected value) scales for women with centered ages of 2.0.\n\nfilter(r -&gt; r.age == 2, newdata)\n\nTable with 5 columns and 4 rows:\n     age  ch     urban  η          μ\n   ┌───────────────────────────────────────\n 1 │ 2    false  N      -1.45624   0.189043\n 2 │ 2    true   N      -0.110513  0.4724\n 3 │ 2    false  Y      -0.669053  0.338709\n 4 │ 2    true   Y      0.676671   0.662995\n\n\nThe predicted probability of woman with centered age of 2, with children, living in an urban environment using artificial contraception is about 2/3, which is reasonably close to the smoothed frequency for that combination of covariates in Figure 6.2.\nSimilarly, a woman of centered age of 2 without children living in a rural environment has a predicted probability of using artificial contraception of a little less than 20%, which also corresponds to the smoother line for that combination (blue line in the left panel) in Figure 6.2."
  },
  {
    "objectID": "glmmbernoulli.html#sec-glmmrandomeff",
    "href": "glmmbernoulli.html#sec-glmmrandomeff",
    "title": "6  Generalized Linear Mixed Models for Binary Responses",
    "section": "6.3 Interpretation of random effects",
    "text": "6.3 Interpretation of random effects\nWe should also be aware that the random effects are defined on the linear predictor scale and not on the probability scale. A normal probability plot of the conditional modes of the random effects for model com05, Figure 6.6\n\n\nCode\nqqcaterpillar(com05)\n\n\n\n\n\nFigure 6.6: Caterpillar plot of the conditional modes of the random-effects for model com05\n\n\n\n\nshows that the smallest random effects are approximately -1 and the largest are approximately 1.\nThe numerical values and the identifier of the combination of dist and urban for these extreme values can be obtained from the first few rows and the last few rows of the sorted random-effects table.\n\nsrtdre = let retbl = only(raneftables(com05))\n  retbl[sortperm(last.(retbl))]\nend\nfirst(srtdre, 6)\n\nTable with 2 columns and 6 rows:\n     dist & urban  (Intercept)\n   ┌──────────────────────────\n 1 │ (\"D01\", \"N\")  -0.957369\n 2 │ (\"D11\", \"N\")  -0.931987\n 3 │ (\"D24\", \"N\")  -0.603423\n 4 │ (\"D01\", \"Y\")  -0.580204\n 5 │ (\"D27\", \"N\")  -0.57665\n 6 │ (\"D55\", \"Y\")  -0.548266\n\n\n\nlast(srtdre, 6)\n\nTable with 2 columns and 6 rows:\n     dist & urban  (Intercept)\n   ┌──────────────────────────\n 1 │ (\"D43\", \"N\")  0.644083\n 2 │ (\"D04\", \"Y\")  0.644674\n 3 │ (\"D42\", \"N\")  0.665121\n 4 │ (\"D58\", \"N\")  0.677422\n 5 │ (\"D14\", \"Y\")  0.695323\n 6 │ (\"D34\", \"N\")  1.08592\n\n\nThe largest random effect is for rural settings in D34. There were 26 women in the sample from rural D34\n\nD34N = filter(r -&gt; (r.dist == \"D34\") & (r.urban == \"N\"), contra)\n\nTable with 6 columns and 26 rows:\n      dist  urban  livch  age     use  ch\n    ┌───────────────────────────────────────\n 1  │ D34   N      0      -11.56  Y    false\n 2  │ D34   N      1      3.44    Y    true\n 3  │ D34   N      2      4.44    Y    true\n 4  │ D34   N      3+     -3.56   Y    true\n 5  │ D34   N      0      -8.56   Y    false\n 6  │ D34   N      2      8.44    N    true\n 7  │ D34   N      3+     8.44    Y    true\n 8  │ D34   N      3+     4.44    Y    true\n 9  │ D34   N      2      -3.56   Y    true\n 10 │ D34   N      3+     8.44    Y    true\n 11 │ D34   N      2      -3.56   N    true\n 12 │ D34   N      1      -9.56   N    true\n 13 │ D34   N      3+     -2.56   Y    true\n 14 │ D34   N      3+     2.44    Y    true\n 15 │ D34   N      2      0.44    Y    true\n 16 │ D34   N      1      4.44    N    true\n 17 │ D34   N      2      -7.56   Y    true\n 18 │ D34   N      3+     11.44   Y    true\n 19 │ D34   N      0      -12.56  N    false\n 20 │ D34   N      3+     9.44    N    true\n 21 │ D34   N      1      -2.56   Y    true\n 22 │ D34   N      3+     -2.56   Y    true\n 23 │ D34   N      2      13.44   Y    true\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮     ⋮\n\n\nof whom 20 used contraception, an unusually large proportion for a rural setting.\nBut this happens when you have relatively small survey sizes - we expect considerable variation.\nAlso there is considerable variability in the lengths of the prediction intervals in Figure 6.6 because the data are unbalanced with respect to district and rural/urban districts.\nConsider the cross-tabulation of counts of interviewees by district and urban/rural status presented at the end of Section 6.1.3. The data contains responses from 54 rural women in district D01 but only 21 rural women from D11. Thus the bottom line in Figure 6.6, for (\"D21\", \"N\"), and based on 54 responses, is shorter than the line second from the bottom, for (\"D11\", \"N\") and based on 21 women only.\n\n6.3.1 Conversion of random effects to relative odds\nThe exponential of the random effect is the relative odds of a woman in a particular urban/district combination using artificial birth control compared to her counterpart (same age, same with/without children status, same urban/rural status) in a typical district. The thus, the relative odds of a rural woman in district D01 using artifical contraception relative to the general population of women her age is\n\nexp(last(first(srtdre)))\n\n0.3839015779363896\n\n\nor about 40%.\n\n\n\n\nHuq, N. M., & Cleland, J. (1990). Bangladesh fertility survey 1989 (main report). National Institute of Population Research; Training."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler,\nB., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., &\nTreiman, R. (2007). The English lexicon project.\nBehavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/bf03193014\n\n\nBates, D., Maechler, M., Bolker, B. M., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4. Journal of Statistical\nSoftware, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear\ncurves. Biometrics, 6(4), 362. https://doi.org/10.2307/3001781\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations.\nJournal of the Royal Statistical Society: Series B\n(Methodological), 26(2), 211–243. https://doi.org/10.1111/j.2517-6161.1964.tb00553.x\n\n\nBox, G. E. P., & Tiao, G. C. (1973). Bayesian inference in\nstatistical analysis. Addison-Wesley.\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible\nhigh-performance data visualization for Julia. Journal\nof Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical\nmethods in research and production (4th ed.). Hafner.\n\n\nDavis, C. S. (2002). Statistical methods for the analysis of repeated\nmeasurements. In Springer Texts in Statistics (pp. xxiv + 415).\nNew York, NY: Springer. https://doi.org/10.1007/b97287\n\n\nElston, R. C., & Grizzle, J. E. (1962). Estimation of time-response\ncurves and their confidence bands. Biometrics, 18,\n148–159. https://doi.org/10.2307/2527453\n\n\nHarper, F. M., & Konstan, J. A. (2016). The MovieLens\ndatasets. ACM Transactions on Interactive Intelligent\nSystems, 5(4), 1–19. https://doi.org/10.1145/2827872\n\n\nHuq, N. M., & Cleland, J. (1990). Bangladesh fertility survey\n1989 (main report). National Institute of Population Research;\nTraining.\n\n\nKamiński, B. (2023). Julia for data analysis. Manning.\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in\nS and S-Plus (pp. xvi + 528). New York,\nNY: Springer. https://doi.org/10.1007/b98882\n\n\nPowell, M. J. (2009). The BOBYQA algorithm for bound constrained\noptimization without derivatives. Cambridge NA Report NA2009/06,\nUniversity of Cambridge, Cambridge, 26.\n\n\nRasbash, J., Browne, W., Goldstein, H., Yang, M., & Plewis, I.\n(2000). A user’s guide to MLwiN. Multilevel Models\nProject, Institute of Education, University of London.\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear\nmodels: Applications and data analysis methods (2nd ed.). Sage.\n\n\nSakamoto, Y., Ishiguro, M., & Kitagawa, G. (1986). Akaike\ninformation criterion statistics (p. 290). Reidel.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. Annals of\nStatistics, 6, 461–464.\n\n\nTierney, L., & Kadane, J. B. (1986). Accurate approximations for\nposterior moments and marginal densities. Journal of the American\nStatistical Association, 81(393), 82–86. https://doi.org/10.1080/01621459.1986.10478240\n\n\nWickham, H. (2011). The split-apply-combine strategy for data analysis.\nJournal of Statistical Software, 40(1), 1–29. https://doi.org/10.18637/jss.v040.i01"
  },
  {
    "objectID": "datatables.html#the-arrow-format-for-data-tables",
    "href": "datatables.html#the-arrow-format-for-data-tables",
    "title": "Appendix A — Working with data tables",
    "section": "A.1 The Arrow format for data tables",
    "text": "A.1 The Arrow format for data tables\nAn increasing popular representation of data frames is as Arrow tables. Polars uses the Arrow format internally as does the DuckDB database. Recently it was announced that the 2.0 release of pandas will allow for Arrow tables.\nArrow specifies a language-independent tabular memory format that provides many desirable properties, such as provision for missing data and compact representation of categorical data vectors, for data science. The memory format also defines a file format for storing and exchanging data tables. Because the memory format is essentially the same as the file format, Arrow files can be memory-mapped providing very fast read speeds.\nFurthermore, the Arrow Project provides a reference implementation of the Arrow format and tools for manipulating data in that format as a C++ library, which is used by implementations in several other languages, including C, C#, Go, Java, JavaScript, MATLAB, Python, R, and Ruby. The Julia implementation in Arrow.jl does not call functions in the C++ library. Instead it implements the format in Julia code in such a way that Arrow vectors behave like native Julia vectors. That is, Arrow vectors are a subtype of AbstractVector. A similar approach is taken in the implementations of the Arrow format for the Rust language.\nThe data sets in the MixedModels package and the auxillary data sets used in this book are stored in the Arrow file format and retrieved as Arrow.Tables. There are many examples throughout this book of loading such data sets.\n\ncontra = dataset(:contra)\n\nArrow.Table with 1934 rows, 5 columns, and schema:\n :dist   String\n :urban  String\n :livch  String\n :age    Float64\n :use    String\n\n\nOften, for ease of access and for display, we convert the Arrow table to a Table, which, contrary to convention, is a type defined in TypedTables.jl and not in Tables.jl.\nBefore going into detail about the properties and use of the Table type, let us first discuss the role of Tables.jl."
  },
  {
    "objectID": "datatables.html#sec-Tablesjl",
    "href": "datatables.html#sec-Tablesjl",
    "title": "Appendix A — Working with data tables",
    "section": "A.2 Tables.jl provides an “interface for tables”",
    "text": "A.2 Tables.jl provides an “interface for tables”\nAn important characteristic of any system for working with data tables is whether the table is stored in memory column-wise or row-wise.\nAs described above, most implementations of data frames for data science store the data column-wise.\nIn relational database management systems (RDBMS), such as PostgreSQL, SQLite, and a multitude of commerical systems, a data table, called a relation, is typically stored row-wise. Such systems typically use SQL, the structured query language, to define and access the data in tables, which is why that acronym appears in many of the names. There are exceptions to the row-wise rule, such as DuckDB, an SQL-based RDBMS, that, as mentioned above, represents relations as Arrow tables.\nMany external representations of data tables, such as in comma-separated-value (CSV) files, are row-oriented. Furthermore, it is often convenient to generate a data table a row at a time.\nThus it becomes convenient to have a “clearing house” that can accept either row tables or column tables and provided the desired form to a downstream package, such as StatsModels.jl. Tables.jl does exactly this. It is not an implementation of data tables itself, but rather it defines “an interface for tables in Julia”, allowing a row-oriented table to be accessed column-wise and vice-versa.\nIt defines a prototype column-oriented table, Tables.ColumnTable, as a NamedTuple of vectors.\n\nTables.ColumnTable\n\nNamedTuple{names, T} where {N, names, T&lt;:Tuple{Vararg{AbstractVector, N}}}\n\n\nand a prototype row-oriented table as a vector of NamedTuples.\n\nTables.RowTable\n\n\nAbstractVector{T} where T&lt;:NamedTuple (alias for AbstractArray{T, 1} where T&lt;:NamedTuple)\n\n\n\nThe actual implementation of a row-table or column-table type may be different from these prototypes but it must provide access methods as if it were one of these types. Tables.jl provides the “glue” to treat a particular data table type as if it were row-oriented, by calling Tables.rows or Tables.rowtable on it, or column-oriented, by calling Tables.columntable on it."
  },
  {
    "objectID": "datatables.html#the-table-type-from-typedtables",
    "href": "datatables.html#the-table-type-from-typedtables",
    "title": "Appendix A — Working with data tables",
    "section": "A.3 The Table type from TypedTables",
    "text": "A.3 The Table type from TypedTables\nTypedTables.jl is a lightweight package (about 1500 lines of source code) that provides a concrete implementation of column-tables, called simply Table, as a NamedTuple of vectors.\nA Table that is constructed from another type of column-table, such as an Arrow.Table or a DataFrame or an explicit NamedTuple of vectors, is simply a wrapper around the original table’s contents. On the other hand, constructing a Table from a row table first creates a ColumnTable, then wraps it.\n\ncontratbl = Table(contra)\n\nTable with 5 columns and 1934 rows:\n      dist  urban  livch  age     use\n    ┌────────────────────────────────\n 1  │ D01   Y      3+     18.44   N\n 2  │ D01   Y      0      -5.56   N\n 3  │ D01   Y      2      1.44    N\n 4  │ D01   Y      3+     8.44    N\n 5  │ D01   Y      0      -13.56  N\n 6  │ D01   Y      0      -11.56  N\n 7  │ D01   Y      3+     18.44   N\n 8  │ D01   Y      3+     -3.56   N\n 9  │ D01   Y      1      -5.56   N\n 10 │ D01   Y      3+     1.44    N\n 11 │ D01   Y      0      -11.56  Y\n 12 │ D01   Y      0      -2.56   N\n 13 │ D01   Y      1      -4.56   N\n 14 │ D01   Y      3+     5.44    N\n 15 │ D01   Y      3+     -0.56   N\n 16 │ D01   Y      3+     4.44    Y\n 17 │ D01   Y      0      -5.56   N\n 18 │ D01   Y      3+     -0.56   Y\n 19 │ D01   Y      1      -6.56   Y\n 20 │ D01   Y      2      -3.56   N\n 21 │ D01   Y      0      -4.56   N\n 22 │ D01   Y      0      -9.56   N\n 23 │ D01   Y      3+     2.44    N\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮\n\n\n\ntypeof(contratbl)\n\nTable{NamedTuple{(:dist, :urban, :livch, :age, :use), Tuple{String, String, String, Float64, String}}, 1, NamedTuple{(:dist, :urban, :livch, :age, :use), Tuple{Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}, Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}, Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}, Arrow.DictEncoded{Float64, Int8, Arrow.Primitive{Float64, Vector{Float64}}}, Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}}}}\n\n\n(The output from that expression is a very long string. You need to scroll to the right over the output to see all the output.)\nThis type of table is said to be “strongly typed”, meaning that the data type itself contains a wealth of detail about the exact form of the table, allowing the Julia compiler to generate efficient code for operations on the table. That is the positive aspect of being so specific about the names of the columns and the details of the type of data in each column. However, it also means that this mechanism is not suitable for tables with a large number, say hundreds or thousands, of columns, which can overburden the compiler.\n\nA.3.1 Accessing columns or rows in a Table\nThe methods for accessing columns or rows in a Table are simple.\nA column is accessed by its name as a “property”, either using the getproperty extractor function or, more commonly, with the dot (.) operator, returning a vector.\n\ncontratbl.urban\n\n1934-element Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}:\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n ⋮\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n\n\nThe column names are Symbols, not strings, usually typed as a : followed by the name, as shown in\n\ncolumnnames(contratbl)\n\n(:dist, :urban, :livch, :age, :use)\n\n\nThe : form for creating the Symbol requires that the column name be a valid variable name in Julia. If, for example, a column name contains a blank, the : form must be replaced by an expression like var\"&lt;name&gt;\", which invokes what is called a “string macro”.\n\ncontratbl.var\"urban\"\n\n1934-element Arrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}:\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n \"Y\"\n ⋮\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n \"N\"\n\n\nA row is accessed by its index, either using the getindex function or, more commonly, with the index in square brackets, returning a NamedTuple for a singleton index or another Table for a vector-valued index.\n\ncontratbl[1]\n\n(dist = \"D01\", urban = \"Y\", livch = \"3+\", age = 18.44, use = \"N\")\n\n\n\ncontratbl[2:5]\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D01   Y      0      -5.56   N\n 2 │ D01   Y      2      1.44    N\n 3 │ D01   Y      3+     8.44    N\n 4 │ D01   Y      0      -13.56  N\n\n\n(Notice that the row numbers are not part of the table. Extracting a subset of the rows produces a table with row numbers starting at one, regardless of what the original row numbers were.)\nBut there is much more to the indexing than simply extracting a subset of rows - it provides an iterator interface to Table.\n\n\nA.3.2 A trivial example done several ways\nSuppose we wish to select the rows from district D49 as a table. We could create a Boolean vector and use it to index into contratbl\n\ncontratbl[contratbl.dist .== \"D49\"]\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D49   N      0      -12.56  N\n 2 │ D49   N      0      -9.56   N\n 3 │ D49   N      0      -10.56  N\n 4 │ D49   N      3+     2.44    N\n\n\n\n\n\n\n\n\n.== is the vectorized form of the equality comparison, ==\n\n\n\n\n\nWhen comparing a vector, like contratbl.dist to a single string or number, like \"D47\" we must “vectorize” the operation, which is done here using dot vectorization\n\n\n\nOr we could filter the rows of the table by applying a function to each row to determine if the dist field has the value \"D49\".\n\nisD49dist(row) = row.dist == \"D49\" # a 'one-liner' function definition\nfilter(isD49dist, contratbl)\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D49   N      0      -12.56  N\n 2 │ D49   N      0      -9.56   N\n 3 │ D49   N      0      -10.56  N\n 4 │ D49   N      3+     2.44    N\n\n\nOr we could write the filter function as an anonymous function\n\nfilter(r -&gt; r.dist == \"D49\", contratbl)\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D49   N      0      -12.56  N\n 2 │ D49   N      0      -9.56   N\n 3 │ D49   N      0      -10.56  N\n 4 │ D49   N      3+     2.44    N\n\n\nOr we could write the filter function as the composition of a function that extracts the first value from the row, which is the dist value, and a function that compares that value to \"D49\".\n\nfilter(==(\"D49\") ∘ first, contratbl)\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D49   N      0      -12.56  N\n 2 │ D49   N      0      -9.56   N\n 3 │ D49   N      0      -10.56  N\n 4 │ D49   N      3+     2.44    N\n\n\n\n\n\n\n\n\nThe function composition operator\n\n\n\n\n\nThe function composition operator, ∘, typed as \\circ&lt;tab&gt;, is described in this manual section,\n\n\n\nOr we could write a generator expression\n\nTable(r for r in contratbl if r.dist == \"D49\")\n\nTable with 5 columns and 4 rows:\n     dist  urban  livch  age     use\n   ┌────────────────────────────────\n 1 │ D49   N      0      -12.56  N\n 2 │ D49   N      0      -9.56   N\n 3 │ D49   N      0      -10.56  N\n 4 │ D49   N      3+     2.44    N\n\n\nThe point is that all of these variations are from the base Julia language and simply rely on the fact that contratbl can be treated as an iterator over the rows of the table.\nAs shown in the last code block, the process of iterating over the rows of a Table can be applied in reverse, constructing a Table from an iterator or a generator expression that returns NamedTuples. In Chapter 6 a newdata table is constructed from the Cartesian product of vectors of covariates as the newdata table.\n\nnewdata = Table(\n  (; age=a, ch=c, urban=u)  # NamedTuple from iterator product\n  for a in -10:3:20, c in [false, true], u in [\"N\", \"Y\"]\n)\n\nTable with 3 columns and 44 rows:\n      age  ch     urban\n    ┌──────────────────\n 1  │ -10  false  N\n 2  │ -7   false  N\n 3  │ -4   false  N\n 4  │ -1   false  N\n 5  │ 2    false  N\n 6  │ 5    false  N\n 7  │ 8    false  N\n 8  │ 11   false  N\n 9  │ 14   false  N\n 10 │ 17   false  N\n 11 │ 20   false  N\n 12 │ -10  true   N\n 13 │ -7   true   N\n 14 │ -4   true   N\n 15 │ -1   true   N\n 16 │ 2    true   N\n 17 │ 5    true   N\n 18 │ 8    true   N\n 19 │ 11   true   N\n 20 │ 14   true   N\n 21 │ 17   true   N\n 22 │ 20   true   N\n 23 │ -10  false  Y\n ⋮  │  ⋮     ⋮      ⋮\n\n\n\n\n\n\n\n\nExpression for creating a NamedTuple\n\n\n\n\n\nIn general a Tuple is written as a comma-separated set of values within parentheses.\n\ntypeof((1, true, 'R'))\n\nTuple{Int64, Bool, Char}\n\n\nThis looks like the arguments to a function call without the function name, which is not accidental - internally the structure is exactly that of the arguments to a function call. Just as we can optionally separate the positional arguments from the named arguments with ; in a function call, we can indicate that we are generating a NamedTuple by prefacing the named values with ; as shown in this example.\nIn this expression the ; is not necessary but there is another form where we just give the name of the variable, like simply specifying contrasts in the function call like fit(MixedModel, form, data; contrasts), where the ; indicates that the following arguments are named arguments so that contrasts by itself is equivalent to contrasts=contrasts specifying both the name and the value.\nThus the newdata table could be constructed as\n\nnewdata = Table(\n  (; age, ch, urban) for age in -10:3:20, ch in [false, true],\n  urban in [\"N\", \"Y\"]\n)\n\nTable with 3 columns and 44 rows:\n      age  ch     urban\n    ┌──────────────────\n 1  │ -10  false  N\n 2  │ -7   false  N\n 3  │ -4   false  N\n 4  │ -1   false  N\n 5  │ 2    false  N\n 6  │ 5    false  N\n 7  │ 8    false  N\n 8  │ 11   false  N\n 9  │ 14   false  N\n 10 │ 17   false  N\n 11 │ 20   false  N\n 12 │ -10  true   N\n 13 │ -7   true   N\n 14 │ -4   true   N\n 15 │ -1   true   N\n 16 │ 2    true   N\n 17 │ 5    true   N\n 18 │ 8    true   N\n 19 │ 11   true   N\n 20 │ 14   true   N\n 21 │ 17   true   N\n 22 │ 20   true   N\n 23 │ -10  false  Y\n ⋮  │  ⋮     ⋮      ⋮\n\n\n\n\n\nTypedTables.jl leverages the power of the base Julia language and its implementation of concepts such as iterators to provide data manipulation without needing to re-implement each concept from scratch.\n\n\nA.3.3 Adding or removing columns from a Table\nBecause TypeTables.Table wraps a NamedTuple of vectors, which is an immutable type, a Table’s column names and types cannot be changed. However, it is easy and fast to create a new Table from an existing Table. (The reason this operation is fast is because it does not copy the contents of the vectors in the table, it just creates a new NamedTuple and wrapper referencing the existing contents.)\nDuring the creation of a new Table columns can be added or removed.\nFor example, in Chapter 6 we added a Boolean column, ch, indicating if livch is not \"0\", to the contra table using an expression like\n\ncontratbl = Table(contratbl; ch=contratbl.livch .== \"0\")\n\nTable with 6 columns and 1934 rows:\n      dist  urban  livch  age     use  ch\n    ┌───────────────────────────────────────\n 1  │ D01   Y      3+     18.44   N    false\n 2  │ D01   Y      0      -5.56   N    true\n 3  │ D01   Y      2      1.44    N    false\n 4  │ D01   Y      3+     8.44    N    false\n 5  │ D01   Y      0      -13.56  N    true\n 6  │ D01   Y      0      -11.56  N    true\n 7  │ D01   Y      3+     18.44   N    false\n 8  │ D01   Y      3+     -3.56   N    false\n 9  │ D01   Y      1      -5.56   N    false\n 10 │ D01   Y      3+     1.44    N    false\n 11 │ D01   Y      0      -11.56  Y    true\n 12 │ D01   Y      0      -2.56   N    true\n 13 │ D01   Y      1      -4.56   N    false\n 14 │ D01   Y      3+     5.44    N    false\n 15 │ D01   Y      3+     -0.56   N    false\n 16 │ D01   Y      3+     4.44    Y    false\n 17 │ D01   Y      0      -5.56   N    true\n 18 │ D01   Y      3+     -0.56   Y    false\n 19 │ D01   Y      1      -6.56   Y    false\n 20 │ D01   Y      2      -3.56   N    false\n 21 │ D01   Y      0      -4.56   N    true\n 22 │ D01   Y      0      -9.56   N    true\n 23 │ D01   Y      3+     2.44    N    false\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮     ⋮\n\n\nIf later we decide that we can do without this column we can drop it using getproperties whose second argument should be a Tuple of Symbols of the names of the columns to retain.\n\ncontratbl = Table(\n  getproperties(contratbl, (:dist, :urban, :livch, :age, :use)),\n)\n\nTable with 5 columns and 1934 rows:\n      dist  urban  livch  age     use\n    ┌────────────────────────────────\n 1  │ D01   Y      3+     18.44   N\n 2  │ D01   Y      0      -5.56   N\n 3  │ D01   Y      2      1.44    N\n 4  │ D01   Y      3+     8.44    N\n 5  │ D01   Y      0      -13.56  N\n 6  │ D01   Y      0      -11.56  N\n 7  │ D01   Y      3+     18.44   N\n 8  │ D01   Y      3+     -3.56   N\n 9  │ D01   Y      1      -5.56   N\n 10 │ D01   Y      3+     1.44    N\n 11 │ D01   Y      0      -11.56  Y\n 12 │ D01   Y      0      -2.56   N\n 13 │ D01   Y      1      -4.56   N\n 14 │ D01   Y      3+     5.44    N\n 15 │ D01   Y      3+     -0.56   N\n 16 │ D01   Y      3+     4.44    Y\n 17 │ D01   Y      0      -5.56   N\n 18 │ D01   Y      3+     -0.56   Y\n 19 │ D01   Y      1      -6.56   Y\n 20 │ D01   Y      2      -3.56   N\n 21 │ D01   Y      0      -4.56   N\n 22 │ D01   Y      0      -9.56   N\n 23 │ D01   Y      3+     2.44    N\n ⋮  │  ⋮      ⋮      ⋮      ⋮      ⋮"
  },
  {
    "objectID": "datatables.html#the-dataframes-package",
    "href": "datatables.html#the-dataframes-package",
    "title": "Appendix A — Working with data tables",
    "section": "A.4 The DataFrames package",
    "text": "A.4 The DataFrames package\nThe JuliaData organization manages the development of several packages related to data science and data management, including DataFrames.jl, a comprehensive system for working with column-oriented data tables in Julia. Kamiński (2023), written by the primary author of that package, provides an in-depth introduction to data science facilities, in particular the DataFrames package, in Julia.\nThis package is particularly well-suited to more advanced data manipulation such as the split-apply-combine strategy (Wickham, 2011) and “joins” of data tables.\n\n\n\n\nKamiński, B. (2023). Julia for data analysis. Manning.\n\n\nWickham, H. (2011). The split-apply-combine strategy for data analysis. Journal of Statistical Software, 40(1), 1–29. https://doi.org/10.18637/jss.v040.i01"
  },
  {
    "objectID": "linalg.html#matrix-vector-representation-of-linear-models",
    "href": "linalg.html#matrix-vector-representation-of-linear-models",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.1 Matrix-vector representation of linear models",
    "text": "B.1 Matrix-vector representation of linear models\nA linear statistical model is often written as an expression for each element of the \\(n\\)-dimensional response vector, \\({\\mathbf{y}}\\), as, e.g. \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_p x_{i,p} + \\epsilon_i, \\quad i=1,\\dots, n\n\\tag{B.1}\\] and some additional description like “where the \\(\\epsilon_i,i=1,\\dots,n\\) are independently and identically distributed as \\({\\mathcal{N}}(0, \\sigma^2)\\)”.\nAn alternative is to write the model in terms of the \\(n\\)-dimensional response vector, \\({\\mathbf{y}}\\), an \\(n\\times p\\) model matrix, \\({\\mathbf{X}}\\), and a \\(p\\)-dimensional coefficient vector, \\({\\boldsymbol{\\beta}}\\), as \\[\n{\\mathcal{Y}}\\sim{\\mathcal{N}}\\left({\\mathbf{X}}{\\boldsymbol{\\beta}},\\sigma^2{\\mathbf{I}}\\right),\n\\tag{B.2}\\] where \\({\\mathcal{N}}\\) denotes the multivariate Gaussian distribution with mean \\({\\boldsymbol{\\mu}}={\\mathbf{X}}{\\boldsymbol{\\beta}}\\) and variance-covariance matrix \\({\\boldsymbol{\\Sigma}}=\\sigma^2{\\mathbf{I}}\\). (In what follows we will refer to the variance-covariance matrix as simply the covariance matrix.)\nBefore considering properties of and computational methods for the model Equation B.2 we will describe some of the properties of the multivariate Gaussian distribution."
  },
  {
    "objectID": "linalg.html#sec-multivariateGaussian",
    "href": "linalg.html#sec-multivariateGaussian",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.2 The multivariate Gaussian distribution",
    "text": "B.2 The multivariate Gaussian distribution\nJust as a univariate Gaussian distribution can be written by specifying the (scalar) mean, \\(\\mu\\), and the variance, \\(\\sigma^2\\), as \\({\\mathcal{N}}(\\mu, \\sigma^2)\\), a multivariate Gaussian distribution is characterized by its \\(n\\)-dimensional mean vector, \\({\\boldsymbol{\\mu}}\\), and its \\(n\\times n\\) variance-covariance matrix, \\({\\boldsymbol{\\Sigma}}\\), as \\({\\mathcal{N}}({\\boldsymbol{\\mu}}, {\\boldsymbol{\\Sigma}})\\).\nThe density function for a univariate Gaussian distribution is the familiar “bell curve” \\[\nf(x; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)\n\\tag{B.3}\\] and probabilities defined by this density are most easily evaluated by standardizing the deviation, \\(x-\\mu\\), as \\(z=\\frac{x-\\mu}{\\sigma}\\), which is why \\(\\sigma\\) is called the standard deviation.\nTo be able to evaluate \\(\\sigma\\), the variance, \\(\\sigma^2\\), must be positive, or at least non-negative. If \\(\\sigma^2=0\\) then all the probability is concentrated at a single point, \\(x=\\mu\\), and we no longer have a probability density, in the usual way of thinking of one. The density shrinks to a point mass and the distribution is said to be degenerate.\nSimilar constraints apply to a covariance matrix, \\({\\boldsymbol{\\Sigma}}\\). Because the covariance of the i’th and j’th elements does not depend upon the order in which we write them, \\({\\boldsymbol{\\Sigma}}\\) must be symmetric. That is, \\[\n{\\boldsymbol{\\Sigma}}' = {\\boldsymbol{\\Sigma}}.\n\\tag{B.4}\\] Furthermore, to define a proper multivariate density, \\({\\boldsymbol{\\Sigma}}\\) must be positive definite, which means that for any non-zero vector, \\({\\mathbf{x}}\\), the quadratic form defined by \\({\\boldsymbol{\\Sigma}}\\) must be positive. That is \\[\n{\\mathbf{x}}'{\\boldsymbol{\\Sigma}}{\\mathbf{x}}&gt;0,\\quad\\forall\\,{\\mathbf{x}}\\ne\\mathbf{0} .\n\\tag{B.5}\\] (the symbol \\(\\forall\\) means “for all”). Positive definiteness implies that the precision matrix, \\({\\boldsymbol{\\Sigma}}^{-1}\\), exists and is also positive definite. It also implies that there are “matrix square roots” of \\({\\boldsymbol{\\Sigma}}\\) in the sense that there are matrices \\(\\mathbf{A}\\) such that \\(\\mathbf{A}'\\mathbf{A}={\\boldsymbol{\\Sigma}}\\). (The reason for writing \\(\\mathbf{A}'\\mathbf{A}\\) and not simply the square of \\(\\mathbf{A}\\) is that \\(\\mathbf{A}\\) is not required to be symmetric but \\(\\mathbf{A}'\\mathbf{A}\\) will be symmetric, even in \\(\\mathbf{A}\\) is not.)\nOne such “square root” of a positive definite \\({\\boldsymbol{\\Sigma}}\\) is the Cholesky factor, which corresponds to \\(n\\times n\\) upper-triangular matrix, \\({\\mathbf{R}}\\), such that \\[\n{\\boldsymbol{\\Sigma}}={\\mathbf{R}}'{\\mathbf{R}}.\n\\tag{B.6}\\] This factor is usually called \\({\\mathbf{R}}\\) because it appears without the transpose as the right-hand multiplicant in Equation B.6. An alternative expression is written with the lower-triangular \\(\\mathbf{L}\\) on the left as \\[\n{\\boldsymbol{\\Sigma}}=\\mathbf{L}\\mathbf{L}',\n\\tag{B.7}\\] with the obvious relationship that \\(\\mathbf{L}={\\mathbf{R}}'\\). To add to the confusion, the cholesky function in the LinearAlgebra package produces a factorization where the lower-triangular factor on the left is called L and the upper-triangular factor on the right is called U.\nThe factor \\({\\mathbf{R}}\\) or \\(\\mathbf{L}\\) can be evaluated directly from the elements of \\({\\boldsymbol{\\Sigma}}\\). For example, the non-zeros in the first two rows of \\(\\mathbf{L}\\) are evaluated as \\[\n\\begin{aligned}\n\\mathbf{L}_{1,1}&=\\sqrt{{\\boldsymbol{\\Sigma}}_{1,1}}\\\\\n\\mathbf{L}_{2,1}&={\\boldsymbol{\\Sigma}}_{2,1}/\\mathbf{L}_{1,1}\\\\\n\\mathbf{L}_{2,2}&=\\sqrt{{\\boldsymbol{\\Sigma}}_{2,2}-\\mathbf{L}_{2,1}^2}\n\\end{aligned}\n\\tag{B.8}\\] Evaluating the diagonal elements involves taking a square root. By convention we choose the positive square root for the Cholesky factor with the result that the diagonal elements of \\(\\mathbf{L}\\) are all positive.\n\nB.2.1 Some properties of triangular matrices\nA triangular matrix with non-zero diagonal elements is non-singular. One way to show this is because its determinant, written \\(\\left|\\mathbf{L}\\right|\\), which is the product of its diagonal elements, is non-zero. In the case of a Cholesky factor the determinant will be positive because all the diagonal elements are positive.\nA more straightforward way of showing that such a matrix is non-singular is to show how a triangular system of equations, like \\[\n\\mathbf{Lx}=\\mathbf{b}\n\\tag{B.9}\\] can be solved. In the case of a lower-triangular system the method is called forward solution, with the sequence of scalar equations \\[\n\\begin{aligned}\nx_1&=b_1/\\mathbf{L}_{1,1}\\\\\nx_2&=\\left(b_2-x_1\\mathbf{L}_{2,1}\\right)/\\mathbf{L}_{2,2}\\\\\nx_3&=\\left(b_3-x_1\\mathbf{L}_{3,1}-x_2\\mathbf{L}_{3,2}\\right)/\\mathbf{L}_{3,3}\n\\end{aligned}\n\\tag{B.10}\\] and so on.\nOne point to note here is that \\(b_1\\) is not needed after \\(x_1\\) is evaluated, \\(b_2\\) is not needed after \\(x_2\\) is evaluated, and so on. That is, the forward solution can be carried out in place with each element of \\(\\mathbf{b}\\) overwriting the corresponding element of \\({\\mathbf{x}}\\). This property is useful for avoiding allocation of storage in, for example, evaluation of an objective function during optimization.\nThe corresponding method of solving an upper-triangular system of equations is called backward solution, where \\(b_n\\) is evaluated first, then \\(b_{n-1}\\), and so on.\nRepeated forward solution (or backward solution for upper triangular) can be used to evaluate the inverse, \\(\\mathbf{L}^{-1}\\), of a lower triangular matrix, \\(\\mathbf{L}\\). However, a general rule in numerical linear algebra is that you rarely need to evaluate the full inverse of a matrix. Solving a triangular system like Equation B.9 by evaluating \\(\\mathbf{L}^{-1}\\) and forming the product \\[\n{\\mathbf{x}}= \\mathbf{L}^{-1}\\mathbf{b}\n\\tag{B.11}\\] involves doing roughly \\(n\\) times as much work as solving the system directly, as in Equation B.10. Requiring that the inverse of a matrix must be evaluated to solve a linear system is like saying that a quotient, \\(a/b\\), must be evalated by calculating \\(b^{-1}\\), the reciprocal of \\(b\\), then evaluating the product \\(b^{-1}a\\), instead of evaluating the quotient directly.\nIn a derivation we may write an expression like \\(\\mathbf{L}^{-1}\\mathbf{b}\\) but the evaluation is performed by solving a system like Equation B.10.\n\n\nB.2.2 Positive definiteness and the Cholesky factor\nIt turns out that the ability to form the Cholesky factor, which means that all the quantities like \\({\\boldsymbol{\\Sigma}}_{2,2}-\\mathbf{L}_{2,1}^2\\), whose square roots form the diagonal of \\(\\mathbf{L}\\), evaluate to positive numbers, is equivalent to \\({\\boldsymbol{\\Sigma}}\\) being positive definite. It is straightforward to show that having a Cholesky factor implies that \\({\\boldsymbol{\\Sigma}}\\) is positive definite, because \\[\n{\\mathbf{x}}'{\\boldsymbol{\\Sigma}}{\\mathbf{x}}= {\\mathbf{x}}'{\\mathbf{R}}'{\\mathbf{R}}{\\mathbf{x}}=\\left(\\mathbf{Rx}\\right)'\\mathbf{Rx}=\\left\\|\\mathbf{Rx}\\right\\|^2\n\\tag{B.12}\\] where \\(\\left\\|\\mathbf{v}\\right\\|^2\\) is the squared length of the vector \\(\\mathbf{v}\\). Because \\({\\mathbf{R}}\\) is non-singular, \\({\\mathbf{x}}\\ne\\mathbf{0}\\implies\\mathbf{Rx}\\ne\\mathbf{0}\\) and the squared length in Equation B.12 is greater than zero.\nThe other direction is a bit more complicated to prove but essentially it amounts to showing that if the process of generating the Cholesky factor requires the square root of a non-positive number to obtain a diagonal element then there is a direction in which the quadratic form gives a non-positive result.\nIn practice, the easiest way to check a symmetric matrix to see if it is positive definite is to attempt to evaluate the Cholesky factor and check whether that succeeds. This is exactly what the isposdef methods in the LinearAlgebra package do.\n\n\nB.2.3 Density of the multivariate Gaussian\nFor the general multivariate normal distribution, \\({\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})\\), where \\({\\boldsymbol{\\Sigma}}\\) is positive definite with lower Cholesky factor \\(\\mathbf{L}\\), the probability density function is \\[\n\\begin{aligned}\nf({\\mathbf{x}};{\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})&=\n\\frac{1}{\\sqrt{(2\\pi)^n\\left|{\\boldsymbol{\\Sigma}}\\right|}}\n\\exp\\left(\\frac{-[{\\mathbf{x}}-{\\boldsymbol{\\mu}}]'{\\boldsymbol{\\Sigma}}^{-1}[{\\mathbf{x}}-{\\boldsymbol{\\mu}}]}{2}\\right)\\\\\n&=\\frac{1}{\\sqrt{(2\\pi)^n}\\left|\\mathbf{L}\\right|}\n\\exp\\left(\\frac{-[{\\mathbf{x}}-{\\boldsymbol{\\mu}}]'{\\mathbf{L}'}^{-1}\\mathbf{L}^{-1}[{\\mathbf{x}}-{\\boldsymbol{\\mu}}]}{2}\\right)\\\\\n&=\\frac{1}{\\sqrt{(2\\pi)^n}\\left|\\mathbf{L}\\right|}\n\\exp\\left(\\frac{-\\left\\|\\mathbf{L}^{-1}[{\\mathbf{x}}-{\\boldsymbol{\\mu}}]\\right\\|^2}{2}\\right)\\\\\n\\end{aligned}\n\\tag{B.13}\\] and the standardizing transformation becomes \\[\n\\mathbf{z}=\\mathbf{L}^{-1}[{\\mathbf{x}}-{\\boldsymbol{\\mu}}] ,\n\\tag{B.14}\\] which, in practice, means using forward solution on the lower-triangular system of equations \\[\n\\mathbf{Lz}={\\mathbf{x}}-{\\boldsymbol{\\mu}}.\n\\tag{B.15}\\]\nNote that the standardizing transformation gives us a way to simulate values from a general \\(n\\)-dimensional multivariate Gaussian, \\({\\mathcal{X}}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})\\) as \\[\n{\\mathbf{x}}={\\boldsymbol{\\mu}}+\\mathbf{L}\\mathbf{z}\n\\tag{B.16}\\] where \\(\\mathbf{z}\\) is simulated from the \\(n\\)-dimensional standard multivariate Gaussian, \\({\\mathcal{Z}}\\sim{\\mathcal{N}}(\\mathbf{0},{\\mathbf{I}})\\), which is \\(n\\) independent univariate standard normal distributions.\n\n\nB.2.4 Linear functions of a multivariate Gaussian\nIn general, if \\({\\mathcal{X}}\\) is an \\(n\\)-dimensional random variable with mean \\({\\boldsymbol{\\mu}}\\) and covariance matrix \\({\\boldsymbol{\\Sigma}}\\), and \\(\\mathbf{A}\\) is a matrix with \\(n\\) columns then the mean and variance of \\({\\mathcal{U}}=\\mathbf{A}{\\mathcal{X}}\\) are given by \\[\n\\require{unicode}\n𝔼\\left[{\\mathcal{U}}\\right] =\n𝔼\\left[\\mathbf{A}{\\mathcal{X}}\\right] =\n\\mathbf{A}𝔼\\left[{\\mathcal{X}}\\right] =\n\\mathbf{A}{\\boldsymbol{\\mu}}\n\\tag{B.17}\\] and \\[\n\\begin{aligned}\n\\text{Var}\\left({\\mathcal{U}}\\right)\n&=𝔼\\left[\\left({\\mathcal{U}}-𝔼\\left[{\\mathcal{U}}\\right]\\right)\\left({\\mathcal{U}}-𝔼\\left[{\\mathcal{U}}\\right]\\right)'\\right]\\\\\n&=𝔼\\left[\\left(\\mathbf{A}{\\mathcal{X}}-\\mathbf{A}{\\boldsymbol{\\mu}}\\right)\\left(\\mathbf{A}{\\mathcal{X}}-\\mathbf{A}{\\boldsymbol{\\mu}}\\right)'\\right]\\\\\n&=𝔼\\left[\\mathbf{A}\\left({\\mathcal{X}}-{\\boldsymbol{\\mu}}\\right)\\left({\\mathcal{X}}-{\\boldsymbol{\\mu}}\\right)'\\mathbf{A}'\\right]\\\\\n&=\\mathbf{A}\\,𝔼\\left[\\left({\\mathcal{X}}-{\\boldsymbol{\\mu}}\\right)\\left({\\mathcal{X}}-{\\boldsymbol{\\mu}}\\right)'\\right]\\mathbf{A}'\\\\\n&=\\mathbf{A}\\text{Var}({\\mathcal{X}})\\mathbf{A}'\\\\\n&=\\mathbf{A}{\\boldsymbol{\\Sigma}}\\mathbf{A}'\n\\end{aligned}\n\\tag{B.18}\\]\nA linear function, \\({\\mathcal{U}}=\\mathbf{A}{\\mathcal{X}}\\), of a multivariate Gaussian distribution, \\({\\mathcal{X}}\\sim{\\mathcal{N}}({\\boldsymbol{\\mu}},{\\boldsymbol{\\Sigma}})\\), is also Gaussian and these relationships imply that \\[\n{\\mathcal{U}}\\sim{\\mathcal{N}}(\\mathbf{A}{\\boldsymbol{\\mu}}, \\mathbf{A}{\\boldsymbol{\\Sigma}}\\mathbf{A}')\n\\tag{B.19}\\]\nFor the special case of \\(\\mathbf{A}\\) being of dimension \\(1\\times n\\) (i.e. a row vector), the expression for the \\(1\\times 1\\) covariance matrix is the quadratic form defined by \\({\\boldsymbol{\\Sigma}}\\), which is why \\({\\boldsymbol{\\Sigma}}\\) must be positive definite for the conditional distributions to be non-degenerate."
  },
  {
    "objectID": "linalg.html#back-at-the-linear-model",
    "href": "linalg.html#back-at-the-linear-model",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.3 Back at the linear model",
    "text": "B.3 Back at the linear model\nThe probability density function for the linear model, Equation B.2, is \\[\n\\begin{aligned}\nf({\\mathbf{y}}; {\\boldsymbol{\\beta}}, \\sigma^2)&=\n\\frac{1}{\\sqrt{2\\pi\\left|\\sigma^2{\\mathbf{I}}\\right|}}\n\\exp\\left(\\frac{-[{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}]'\n\\left(\\sigma^2{\\mathbf{I}}\\right)^{-1}[{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}]}{2}\\right)\\\\\n&=\\left(2\\pi\\sigma^2\\right)^{-n/2}\\exp\\left(-\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\right\\|^2/\\left(2\\sigma^2\\right)\\right)\n\\end{aligned}\n\\tag{B.20}\\]\nEquation B.20 describes the density of the random variable, \\({\\mathcal{Y}}\\), representing the observations, given the values of the parameters, \\({\\boldsymbol{\\beta}}\\) and \\(\\sigma^2\\). For parameter estimation we use the likelihood function, which is the same expression as Equation B.20 but regarded as a function of the parameters, \\({\\boldsymbol{\\beta}}\\) and \\(\\sigma^2\\), with the observed response, \\({\\mathbf{y}}\\), fixed. \\[\nL({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})=\n\\left(2\\pi\\sigma^2\\right)^{-n/2}\\exp\\left(-\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\right\\|^2/\\left(2\\sigma^2\\right)\\right)\n\\tag{B.21}\\] The maximum likelihood estimates of the parameters are, as the name implies, the values of \\({\\boldsymbol{\\beta}}\\) and \\(\\sigma^2\\) that maximize the expression on the right of Equation B.21 .\nBecause the logarithm is a monotone increasing function, the maximum likelihood estimates will also maximize the log-likelihood \\[\n\\begin{aligned}\n\\ell({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})\n&=\\log L({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})\\\\\n&=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\right\\|^2}{2\\sigma^2}\n\\end{aligned}\n\\tag{B.22}\\] Usually the log-likelihood is easier to optimize, either algebraically or numerically, than the likelihood itself.\nTo avoid the negative signs and the factors of 2 in the denominator, we often convert the log-likelihood to the deviance scale, which is negative twice the log-likelihood, \\[\n\\begin{aligned}\nd({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})\n&=-2\\ell({\\boldsymbol{\\beta}},\\sigma^2; {\\mathbf{y}})\\\\\n&=n\\log(2\\pi\\sigma^2)+\\frac{\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\right\\|^2}{\\sigma^2} .\n\\end{aligned}\n\\tag{B.23}\\] Because of the negative sign, the maximum likelihood estimates are those that minimize \\(d({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})\\).\n(The term deviance scale is used for \\(d({\\boldsymbol{\\beta}},\\sigma^2;{\\mathbf{y}})\\) rather than deviance because the deviance involves an additive shift, which is a correction for the saturated model - see the link. It is obvious what the saturated model should be for the linear model but not for the linear mixed model so, to avoid confusion, we refer to the log-likelihood on the deviance scale as the objective.)\nThe form of Equation B.23 makes it easy to determine the maximum likelihood estimates. Because \\({\\boldsymbol{\\beta}}\\) appears only in the sum of squared residuals expression, \\(\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\|^2\\), we minimize that with respect to \\({\\boldsymbol{\\beta}}\\) \\[\n\\widehat{{\\boldsymbol{\\beta}}}=\n\\arg\\min_{{\\boldsymbol{\\beta}}}\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\|^2 ,\n\\tag{B.24}\\] where \\(\\arg\\min_{{\\boldsymbol{\\beta}}}\\) means the value of \\({\\boldsymbol{\\beta}}\\) that minimizes the expression that follows.\nLet \\(r^2(\\widehat{{\\boldsymbol{\\beta}}}) = \\left\\|{\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\right\\|^2\\) be the minimum sum of squared residuals. Substituting this value into Equation B.23, differentiating with respect to \\(\\sigma^2\\), and setting this derivative to zero gives \\[\n\\widehat{\\sigma^2}=\\frac{r^2(\\widehat{{\\boldsymbol{\\beta}}})}{n} .\n\\]"
  },
  {
    "objectID": "linalg.html#minimizing-the-sum-of-squared-residuals",
    "href": "linalg.html#minimizing-the-sum-of-squared-residuals",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.4 Minimizing the sum of squared residuals",
    "text": "B.4 Minimizing the sum of squared residuals\nA condition for \\(\\widehat{{\\boldsymbol{\\beta}}}\\) to minimize the sum of squared residuals is that the gradient \\[\n\\nabla r^2({\\boldsymbol{\\beta}})=-2{\\mathbf{X}}'({\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}})\n\\tag{B.25}\\] be zero at \\(\\widehat{{\\boldsymbol{\\beta}}}\\). This condition can be rewritten as \\[\n{\\mathbf{X}}'{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}={\\mathbf{X}}'{\\mathbf{y}},\n\\tag{B.26}\\] which are called the normal equations.\nThe term normal in this expression comes from the fact that requiring the gradient, Equation B.25, to be zero is equivalent to requiring that the residual vector, \\({\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\), be perpendicular, or normal, to the columns of \\({\\mathbf{X}}\\).\nWhen the model matrix, \\({\\mathbf{X}}\\), is of full column rank, which means \\[\n{\\mathbf{X}}\\mathbf{{\\boldsymbol{\\beta}}}\\ne\\mathbf{0}\\quad\\forall{\\boldsymbol{\\beta}}\\ne\\mathbf{0} ,\n\\tag{B.27}\\] then the quadratic form defined by \\({\\mathbf{X}}'{\\mathbf{X}}\\) is positive definite and has a Cholesky factor, say \\({\\mathbf{R}}_{XX}\\), and the normal equations can be solved in two stages. First, solve \\[\n{\\mathbf{R}}_{XX}'\\mathbf{r}_{Xy}={\\mathbf{X}}'{\\mathbf{y}}\n\\tag{B.28}\\] for \\(\\mathbf{r}_{Xy}\\) using forward solution, then solve \\[\n{\\mathbf{R}}_{XX}\\widehat{{\\boldsymbol{\\beta}}}=\\mathbf{r}_{Xy}\n\\tag{B.29}\\] for \\(\\widehat{{\\boldsymbol{\\beta}}}\\) using backward solution.\nAn alternative approach is to write the residual sum of squares as a quadratic form \\[\n\\begin{aligned}\nr^2({\\boldsymbol{\\beta}})&=\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\|^2\\\\\n&=({\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}})'({\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}})\\\\\n&=({\\mathbf{X}}{\\boldsymbol{\\beta}}-{\\mathbf{y}})'({\\mathbf{X}}{\\boldsymbol{\\beta}}-{\\mathbf{y}})\\\\\n&=\\begin{bmatrix}{\\boldsymbol{\\beta}}&-1\\end{bmatrix}\n\\begin{bmatrix}\n{\\mathbf{X}}'{\\mathbf{X}}& {\\mathbf{X}}'{\\mathbf{y}}\\\\\n{\\mathbf{y}}'{\\mathbf{X}}& {\\mathbf{y}}'{\\mathbf{y}}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\boldsymbol{\\beta}}\\\\\n-1\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}{\\boldsymbol{\\beta}}&-1\\end{bmatrix}\n\\begin{bmatrix}\n{\\mathbf{R}}_{XX}' & \\mathbf{0}\\\\\n\\mathbf{r}_{Xy}' & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\mathbf{R}}_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\boldsymbol{\\beta}}\\\\\n-1\n\\end{bmatrix}\\\\\n&=\\left\\|\n\\begin{bmatrix}\n{\\mathbf{R}}_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\boldsymbol{\\beta}}\\\\\n-1\n\\end{bmatrix}\\right\\|^2\\\\\n&=\\left\\|{\\mathbf{R}}_{XX}{\\boldsymbol{\\beta}}-\\mathbf{r}_{Xy}\\right\\|^2+r_{yy}^2\n\\end{aligned}\n\\tag{B.30}\\]\nThe first term, \\(\\left\\|{\\mathbf{R}}_{XX}{\\boldsymbol{\\beta}}-\\mathbf{r}_{Xy}\\right\\|^2\\), is non-negative and can be made zero by solving Equation B.29 for \\(\\widehat{{\\boldsymbol{\\beta}}}\\). Thus, the minimum sum of squared residuals is \\(r_{yy}^2\\).\nOne consequence of this derivation is that the minimum sum of squared residuals can be evaluated directly from the extended Cholesky factor \\[\n\\begin{bmatrix}\n{\\mathbf{R}}_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\tag{B.31}\\] without needing to solve for \\(\\widehat{{\\boldsymbol{\\beta}}}\\) first. This is not terribly important for a linear model where the evaluation of \\(\\widehat{{\\boldsymbol{\\beta}}}\\) and the residual is typically done only once. However, for the linear mixed model, a similar calculation must be done for every evaluation of the objective in the iterative optimization, and being able to evaluate the minimum penalized sum of squared residuals without solving for parameter values and without needing to evaluate the residual saves a non-negligible amount of time and effort."
  },
  {
    "objectID": "linalg.html#numerical-example",
    "href": "linalg.html#numerical-example",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.5 Numerical example",
    "text": "B.5 Numerical example\nSuppose we wish to fit a simple linear regression model to the reaction time as a function of days of sleep deprivation to the data from subject S372 in the sleepstudy dataset.\n\nsleepstudy = Table(dataset(:sleepstudy))\nS372 = sleepstudy[sleepstudy.subj .== \"S372\"]\n\nTable with 3 columns and 10 rows:\n      subj  days  reaction\n    ┌─────────────────────\n 1  │ S372  0     269.412\n 2  │ S372  1     273.474\n 3  │ S372  2     297.597\n 4  │ S372  3     310.632\n 5  │ S372  4     287.173\n 6  │ S372  5     329.608\n 7  │ S372  6     334.482\n 8  │ S372  7     343.22\n 9  │ S372  8     369.142\n 10 │ S372  9     364.124\n\n\nThe model matrix and the response vector can be constructed as\n\nX = hcat(ones(length(S372)), S372.days)\n\n10×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nand\n\ny = S372.reaction\nshow(y)\n\n[269.4117, 273.474, 297.5968, 310.6316, 287.1726, 329.6076, 334.4818, 343.2199, 369.1417, 364.1236]\n\n\nfrom which we obtain the Cholesky factor\n\nchfac = cholesky!(X'X)\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n2×2 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  14.2302\n  ⋅        9.08295\n\n\n(Recall that the upper triangular Cholesky factor is the U property of the Cholesky type.)\nThe \\ operator with a Cholesky factor on the left performs both the forward and backward solutions to obtain the least squares estimates\n\nβ̂ = chfac \\ (X'y)\n\n2-element Vector{Float64}:\n 267.04479999999984\n  11.298073333333367\n\n\nAlternatively, we could carry out the two solutions of the triangular systems explicitly by first solving for \\(\\mathbf{r}_{Xy}\\)\n\nrXy = ldiv!(chfac.L, X'y)\n\n2-element Vector{Float64}:\n 1005.244207376381\n  102.61984718485874\n\n\nthen solving in-place to obtain \\(\\widehat{{\\boldsymbol{\\beta}}}\\)\n\nldiv!(chfac.U, rXy)\n\n2-element Vector{Float64}:\n 267.04479999999967\n  11.298073333333384\n\n\nThe residual vector, \\({\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\), is\n\nr = y - X * β̂\n\n10-element Vector{Float64}:\n   2.3669000000001574\n  -4.868873333333227\n   7.95585333333338\n   9.692580000000078\n -25.064493333333303\n   6.072433333333322\n  -0.35144000000002507\n  -2.911413333333428\n  11.712313333333213\n  -4.603860000000111\n\n\nwith geometric length or “norm”,\n\nnorm(r)\n\n31.915932906580256\n\n\nFor the extended Cholesky factor, create the extended matrix of sums of squares and cross products\n\ncrprod = let x = S372.days\n  Symmetric(\n    [\n      length(x) sum(x) sum(y)\n      0.0 sum(abs2, x) dot(x, y)\n      0.0 0.0 sum(abs2, y)\n    ],\n    :U,\n  )\nend\n\n3×3 Symmetric{Float64, Matrix{Float64}}:\n   10.0      45.0   3178.86\n   45.0     285.0  15237.0\n 3178.86  15237.0      1.02207e6\n\n\nThe call to Symmetric with the second argument the symbol :U indicates that the matrix should be treated as symmetric but only the upper triangle is given.\nThe Cholesky factor of the crprod reproduces \\({\\mathbf{R}}_{XX}\\), \\(\\mathbf{r}_{Xy}\\), and the norm of the residual, \\(r_{yy}\\).\n\nextchfac = cholesky(crprod)\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  14.2302   1005.24\n  ⋅        9.08295   102.62\n  ⋅         ⋅         31.9159\n\n\nand information from which the parameter estimates can be evaluated.\n\nβ̂ ≈ ldiv!(\n  UpperTriangular(view(extchfac.U, 1:2, 1:2)),\n  copy(view(extchfac.U, 1:2, 3)),\n)\n\ntrue\n\n\nThe operator ≈ is a check of approximate equality of floating point numbers or arrays. Exact equality of floating point results from “equivalent” calculations cannot be relied upon.\nSimilarly we check that the value of \\(r_{y,y}\\) is approximately equal to the norm of the residual vector.\n\nnorm(r) ≈ extchfac.U[3, 3]\n\ntrue"
  },
  {
    "objectID": "linalg.html#sec-matrixdecomp",
    "href": "linalg.html#sec-matrixdecomp",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.6 Alternative decompositions of X",
    "text": "B.6 Alternative decompositions of X\nThere are two other decompositions of the model matrix \\({\\mathbf{X}}\\) or the augmented model matrix \\([\\mathbf{X,y}]\\) that can be used to evaluate the least squares estimates; the QR decomposition and the singular value decomposition (SVD).\nThe QR decomposition expresses \\({\\mathbf{X}}\\) as the product of an orthogonal matrix, \\(\\mathbf{Q}\\), and an upper triangular matrix \\({\\mathbf{R}}\\). The upper triangular \\({\\mathbf{R}}\\) is related to the upper triangular Cholesky factor in that the numerical values are the same but the signs can be different. In particular, the usual way of creating \\(\\mathbf{Q}\\) and \\({\\mathbf{R}}\\) using Householder transformations typically results in the first row of \\({\\mathbf{R}}\\) from the qr function being the negative of the first row of the upper Cholesky factor.\n\nqrfac = qr(X)\nqrfac.R\n\n2×2 Matrix{Float64}:\n -3.16228  -14.2302\n  0.0        9.08295\n\n\nJust as the Cholesky factor can be used on the left of the \\ operator, so can the qr factor but with y on the right.\n\nb3 = qrfac \\ y\n\n2-element Vector{Float64}:\n 267.0447999999998\n  11.298073333333361\n\n\nThe matrix \\({\\mathbf{R}}\\) is returned as a square matrix with the same number of columns as \\({\\mathbf{X}}\\). That is, if \\({\\mathbf{X}}\\) is of size \\(n\\times p\\) where \\(n&gt;p\\), as in the example, then \\({\\mathbf{R}}\\) is \\(p\\times p\\), as shown above.\nThe matrix \\(\\mathbf{Q}\\) is usually considered to be an \\(n\\times n\\) orthogonal matrix, which means that its transpose is its inverse \\[\n\\mathbf{Q'Q}=\\mathbf{QQ'}={\\mathbf{I}}\n\\tag{B.32}\\] To form the product \\(\\mathbf{QR}\\) the matrix \\({\\mathbf{R}}\\) is treated as if it were \\(n\\times p\\) with zeros below the main diagonal.\nThe \\(n\\times n\\) matrix \\(\\mathbf{Q}\\) can be very large if \\(n\\), the number of observations, is large but it does not need to be explicitly evaluated. In practice \\(\\mathbf{Q}\\) is a “virtual” matrix represented as a product of Householder reflections that only require storage of the size of \\({\\mathbf{X}}\\). The effect of multiplying a vector or matrix by \\(\\mathbf{Q}\\) or by \\(\\mathbf{Q}'\\) is achieved by applying the Householder reflections in a particular order.\n\nrXy2 = qrfac.Q'y\n\n10-element Vector{Float64}:\n -1005.244207376381\n   102.61984718485857\n     8.057970700644944\n     9.32194331578711\n   -25.90788406907069\n     4.75628854607146\n    -2.140338838786306\n    -5.1730662236441844\n     8.977906391498038\n    -7.8110209933598185\n\n\n\nb4 = ldiv!(UpperTriangular(qrfac.R), rXy2[1:2])\n\n2-element Vector{Float64}:\n 267.0447999999998\n  11.298073333333361\n\n\nForming the QR decomposition is a direct, non-iterative, calculation, like forming the Cholesky factor. Forming the SVD, by contrast, is usually an iterative calculation. (It should be noted that modern methods for evaluating the SVD are very fast for an iterative calculation.) The SVD consists of two orthogonal matrices, the \\(n\\times n\\) \\(\\mathbf{U}\\) and the \\(p\\times p\\) \\(\\mathbf{V}\\) and an \\(n\\times p\\) matrix \\(\\mathbf{S}\\) that is zero off the main diagonal, where \\[\n{\\mathbf{X}}=\\mathbf{USV'} .\n\\]\nUnlike the \\(\\mathbf{Q}\\) in the QR decomposition, the orthogonal matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are explicitly evaluated. Because of this, the default for the svd function is to produce a compact form where \\(\\mathbf{U}\\) is \\(n\\times p\\) and only the diagonal of \\(\\mathbf{S}\\) is returned.\n\nXsvd = svd(X)\n\nSVD{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nU factor:\n10×2 Matrix{Float64}:\n 0.00921331   0.587682\n 0.0669862    0.493961\n 0.124759     0.400241\n 0.182532     0.306521\n 0.240305     0.2128\n 0.298078     0.11908\n 0.355851     0.0253594\n 0.413623    -0.068361\n 0.471396    -0.162081\n 0.529169    -0.255802\nsingular values:\n2-element Vector{Float64}:\n 17.093167142525193\n  1.680368125649001\nVt factor:\n2×2 Matrix{Float64}:\n 0.157485   0.987521\n 0.987521  -0.157485\n\n\n(In general, the Vt factor from this decomposition is \\(\\mathbf{V}'\\), the transpose of \\(\\mathbf{V}\\). The distinction is not important in this case because \\(\\mathbf{V}\\) is symmetric. )\nIf all the singular values are non-zero, as is the case here, the least squares solution \\(\\widehat{{\\boldsymbol{\\beta}}}\\) can be obtained as\n\\[\n\\mathbf{V}\\mathbf{S}^{-1}\\mathbf{U}'{\\mathbf{y}}\n\\tag{B.33}\\]\nfor the diagonal \\(\\mathbf{S}\\).\n\nb5 = Xsvd.V * (Xsvd.U'y ./ Xsvd.S)\n\n2-element Vector{Float64}:\n 267.04479999999995\n  11.298073333333347\n\n\nIn the extensions to linear mixed-effects models we will emphasize the Cholesky factorization over the QR decomposition or the SVD."
  },
  {
    "objectID": "linalg.html#sec-lmmtheory",
    "href": "linalg.html#sec-lmmtheory",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.7 Linear mixed-effects models",
    "text": "B.7 Linear mixed-effects models\nAs described in Bates et al. (2015) , a linear mixed-effects model is based on two vector-valued random variables: the \\(q\\)-dimensional vector of random effects, \\({\\mathcal{B}}\\), and the \\(n\\)-dimensional response vector, \\({\\mathcal{Y}}\\). Equation 1.2 defines the unconditional distribution of \\({\\mathcal{B}}\\) and the conditional distribution of \\({\\mathcal{Y}}\\), given \\({\\mathcal{B}}=\\mathbf{b}\\), as multivariate Gaussian distributions of the form \\[\n\\begin{aligned}\n({\\mathcal{Y}}|{\\mathcal{B}}=\\mathbf{b})&\\sim{\\mathcal{N}}({\\mathbf{X}}{\\boldsymbol{\\beta}}+\\mathbf{Z}\\mathbf{b},\\sigma^2{\\mathbf{I}})\\\\\n{\\mathcal{B}}&\\sim{\\mathcal{N}}(\\mathbf{0},{\\boldsymbol{\\Sigma}}_\\theta) .\n\\end{aligned}\n\\]\nThe \\(q\\times q\\), symmetric, variance-covariance matrix, \\(\\mathrm{Var}({\\mathcal{B}})={\\boldsymbol{\\Sigma}}_\\theta\\), depends on the variance-component parameter vector, \\({\\boldsymbol{\\theta}}\\), through a lower triangular relative covariance factor, \\({\\boldsymbol{\\Lambda}}_\\theta\\), as\n\\[\n{\\boldsymbol{\\Sigma}}_\\theta=\\sigma^2{\\boldsymbol{\\Lambda}}_\\theta{\\boldsymbol{\\Lambda}}_\\theta' .\n\\tag{B.34}\\]\n(Recall that the lower Cholesky factor is generally written \\(\\mathbf{L}\\). In this case the lower Cholesky factor contains parameters and is named with the corresponding Greek letter, \\({\\boldsymbol{\\Lambda}}\\).)\nIn many descriptions of linear mixed models, computational formulas are written in terms of the precision matrix, \\({\\boldsymbol{\\Sigma}}_\\theta^{-1}\\). Such formulas will become unstable as \\({\\boldsymbol{\\Sigma}}_\\theta\\) approaches singularity. And it can do so. It is a fact that singular (i.e. non-invertible) \\({\\boldsymbol{\\Sigma}}_\\theta\\) can and do occur in practice, as we have seen in some of the examples in earlier chapters. Moreover, during the course of the numerical optimization by which the parameter estimates are determined, it is frequently the case that the deviance or the REML criterion will need to be evaluated at values of \\({\\boldsymbol{\\theta}}\\) that produce a singular \\({\\boldsymbol{\\Sigma}}_\\theta\\). Because of this we will take care to use computational methods that can be applied even when \\({\\boldsymbol{\\Sigma}}_\\theta\\) is singular and are stable as \\({\\boldsymbol{\\Sigma}}_\\theta\\) approaches singularity.\nAccording to Equation B.34, \\({\\boldsymbol{\\Sigma}}\\) depends on both \\(\\sigma\\) and \\(\\theta\\), and we should write it as \\({\\boldsymbol{\\Sigma}}_{\\sigma,\\theta}\\). However, we will blur that distinction and continue to write \\(\\text{Var}({\\mathcal{B}})={\\boldsymbol{\\Sigma}}_\\theta\\).\nAnother technicality is that the common scale parameter, \\(\\sigma\\), could, in theory, be zero. However, the only way for its estimate, \\(\\widehat{\\sigma}\\), to be zero is for the fitted values from the fixed-effects only, \\({\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\), to be exactly equal to the observed data. This occurs only with data that have been (incorrectly) simulated without error. In practice we can safely assume that \\(\\sigma&gt;0\\). However, \\({\\boldsymbol{\\Lambda}}_\\theta\\), like \\({\\boldsymbol{\\Sigma}}_\\theta\\), can be singular.\nThe computational methods in the MixedModels package are based on \\({\\boldsymbol{\\Lambda}}_\\theta\\) and do not require evaluation of \\({\\boldsymbol{\\Sigma}}_\\theta\\). In fact, \\({\\boldsymbol{\\Sigma}}_\\theta\\) is explicitly evaluated only at the converged parameter estimates.\nThe spherical random effects, \\({\\mathcal{U}}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma^2{\\mathbf{I}}_q)\\), determine \\({\\mathcal{B}}\\) as\n\\[\n{\\mathcal{B}}={\\boldsymbol{\\Lambda}}_\\theta{\\mathcal{U}}.\n\\tag{B.35}\\]\nAlthough it may seem more intuitive to write \\({\\mathcal{U}}\\) as a linear transformation of \\({\\mathcal{B}}\\), we cannot do that when \\({\\boldsymbol{\\Lambda}}_\\theta\\) is singular, which is why Equation B.35 is in the form shown.\nWe can easily verify that Equation B.35 provides the desired distribution for \\({\\mathcal{B}}\\). As a linear transformation of a multivariate Gaussian random variable, \\({\\mathcal{B}}\\) will also be multivariate Gaussian with mean\n\\[\n𝔼\\left[{\\mathcal{B}}\\right]=\n𝔼\\left[{\\boldsymbol{\\Lambda}}_\\theta{\\mathcal{U}}\\right]=\n{\\boldsymbol{\\Lambda}}_\\theta\\,𝔼\\left[{\\mathcal{U}}\\right]=\n{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{0}=\\mathbf{0}\n\\]\nand covariance matrix\n\\[\n\\text{Var}({\\mathcal{B}})=\n{\\boldsymbol{\\Lambda}}_\\theta\\text{Var}({\\mathcal{U}}){\\boldsymbol{\\Lambda}}_\\theta'=\n\\sigma^2{\\boldsymbol{\\Lambda}}_\\theta{\\boldsymbol{\\Lambda}}_\\theta'={\\boldsymbol{\\Sigma}}_\\theta\n\\]\nJust as we concentrate on how \\({\\boldsymbol{\\theta}}\\) determines \\({\\boldsymbol{\\Lambda}}_\\theta\\), not \\({\\boldsymbol{\\Sigma}}_\\theta\\), we will concentrate on properties of \\({\\mathcal{U}}\\) rather than \\({\\mathcal{B}}\\). In particular, we now define the model according to the distributions\n\\[\n\\begin{aligned}\n({\\mathcal{Y}}|{\\mathcal{U}}=\\mathbf{u})&\\sim{\\mathcal{N}}(\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{u}+{\\mathbf{X}}\\beta,\\sigma^2{\\mathbf{I}}_n)\\\\\n{\\mathcal{U}}&\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma^2{\\mathbf{I}}_q) .\n\\end{aligned}\n\\tag{B.36}\\]\nThe joint density for \\({\\mathcal{Y}}\\) and \\({\\mathcal{U}}\\) is the product of densities of the two distributions in Equation B.36. That is\n\\[\nf_{{\\mathcal{Y}},{\\mathcal{U}}}({\\mathbf{y}},\\mathbf{u})=\n\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{-(n+q)/2}}\\exp\n\\left(\\frac{\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\n-\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{u}\\right\\|^2+\n\\left\\|\\mathbf{u}\\right\\|^2}{-2\\sigma^2}\\right) .\n\\tag{B.37}\\]\nTo evaluate the likelihood for the parameters, \\({\\boldsymbol{\\theta}}\\), \\({\\boldsymbol{\\beta}}\\), and \\(\\sigma^2\\), given the observed response, \\({\\mathbf{y}}\\), we must evaluate the marginal distribution of \\({\\mathcal{Y}}\\), which is the integral of \\(f_{{\\mathcal{Y}},{\\mathcal{U}}}({\\mathbf{y}},\\mathbf{u})\\) with respect to \\(\\mathbf{u}\\).\nThis is much simpler if we rewrite the penalized sum of squared residuals, \\(\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}-\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{u}\\right\\|^2+ \\left\\|\\mathbf{u}\\right\\|^2\\), from Equation B.37, as a quadratic form in \\(\\mathbf{u}\\), to isolate the dependence on \\(\\mathbf{u}\\)\n\\[\n\\begin{aligned}\nr^2_\\theta(\\mathbf{u},{\\boldsymbol{\\beta}})\n&=\n\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}-\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{u}\\|^2+\\|\\mathbf{u}\\|^2 \\\\\n&=\n\\left\\|\n\\begin{bmatrix}\n\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta & {\\mathbf{X}}& {\\mathbf{y}}\\\\\n-{\\mathbf{I}}_q & \\mathbf{0} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-{\\boldsymbol{\\beta}}\\\\\n1\n\\end{bmatrix}\n\\right\\|^2 \\\\\n&=\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-{\\boldsymbol{\\beta}}\\\\\n1\n\\end{bmatrix} '\n\\begin{bmatrix}\n{\\boldsymbol{\\Lambda}}'\\mathbf{Z}'\\mathbf{Z}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}}& {\\boldsymbol{\\Lambda}}'\\mathbf{Z}'{\\mathbf{X}}& {\\boldsymbol{\\Lambda}}'\\mathbf{Z}'{\\mathbf{y}}\\\\\n{\\mathbf{X}}'\\mathbf{Z}{\\boldsymbol{\\Lambda}}& {\\mathbf{X}}'{\\mathbf{X}}& {\\mathbf{X}}'{\\mathbf{y}}\\\\\n{\\mathbf{y}}'\\mathbf{Z}{\\boldsymbol{\\Lambda}}& {\\mathbf{y}}'{\\mathbf{X}}& {\\mathbf{y}}'{\\mathbf{y}}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-{\\boldsymbol{\\beta}}\\\\\n1\n\\end{bmatrix} \\\\\n&= \\left\\|\n\\begin{bmatrix}\n{\\mathbf{R}}_{ZZ} & {\\mathbf{R}}_{ZX} & \\mathbf{r}_{Zy}\\\\\n\\mathbf{0} & {\\mathbf{R}}_{XX}' & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-{\\boldsymbol{\\beta}}\\\\\n1\n\\end{bmatrix}\n\\right\\|^2\\\\\n&= \\| \\mathbf{r}_{Zy}-{\\mathbf{R}}_{ZX}{\\boldsymbol{\\beta}}-{\\mathbf{R}}_{ZZ}\\mathbf{u} \\|^2 +\n\\| \\mathbf{r}_{Xy}-{\\mathbf{R}}_{XX}{\\boldsymbol{\\beta}}\\|^2 + r_{yy}^2 ,\n\\end{aligned}\n\\tag{B.38}\\]\nusing the Cholesky factor of the blocked matrix,\n\\[\n\\begin{aligned}\n{\\boldsymbol{\\Omega}}_\\theta&=\n\\begin{bmatrix}\n{\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'Z}{\\boldsymbol{\\Lambda}}_\\theta+{\\mathbf{I}}&\n{\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'X} & {\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'y} \\\\\n\\mathbf{X'Z}{\\boldsymbol{\\Lambda}}_\\theta & \\mathbf{X'X} & \\mathbf{X'y} \\\\\n\\mathbf{y'Z}{\\boldsymbol{\\Lambda}}_\\theta & \\mathbf{y'X} & \\mathbf{y'y}\n\\end{bmatrix}\\\\\n& =\n\\begin{bmatrix}\n{\\mathbf{R}}_{ZZ}' & \\mathbf{0} & \\mathbf{0} \\\\\n{\\mathbf{R}}_{ZX}' & {\\mathbf{R}}'_{XX} & \\mathbf{0} \\\\\n\\mathbf{r}_{Zy}' & \\mathbf{r}'_{Xy} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n{\\mathbf{R}}_{ZZ} & {\\mathbf{R}}_{ZX} & \\mathbf{r}_{Zy} \\\\\n\\mathbf{0} & {\\mathbf{R}}_{XX} & \\mathbf{r}_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix} .\n\\end{aligned}\n\\tag{B.39}\\]\nNote that the block in the upper left, \\({\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'Z}{\\boldsymbol{\\Lambda}}_\\theta+{\\mathbf{I}}\\), is positive definite even when \\({\\boldsymbol{\\Lambda}}_\\theta\\) is singular, because\n\\[\n\\mathbf{u}'\\left({\\boldsymbol{\\Lambda}}_\\theta'\\mathbf{Z'Z}{\\boldsymbol{\\Lambda}}_\\theta+{\\mathbf{I}}\\right)\\mathbf{u} = \\left\\|\\mathbf{Z}{\\boldsymbol{\\Lambda}}_\\theta\\mathbf{u}\\right\\|^2\n+\\left\\|\\mathbf{u}\\right\\|^2\n\\tag{B.40}\\]\nand the first term is non-negative while the second is positive if \\(\\mathbf{u}\\ne\\mathbf{0}\\).\nThus \\({\\mathbf{R}}_{ZZ}\\), with positive diagonal elements, can be evaluated and its determinant, \\(\\left|{\\mathbf{R}}_{ZZ}\\right|\\), is positive. This determinant appears in the marginal density of \\({\\mathcal{Y}}\\), from which the likelihood of the parameters is evaluated.\nTo evaluate the likelihood,\n\\[\nL({\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}},\\sigma|{\\mathbf{y}}) = \\int_\\mathbf{u} f_{{\\mathcal{Y}},{\\mathcal{U}}}({\\mathbf{y}},\\mathbf{u})\\, d\\mathbf{u}\n\\tag{B.41}\\]\nwe isolate the part of the joint density that depends on \\(\\mathbf{u}\\) and perform a change of variable\n\\[\n\\mathbf{v}={\\mathbf{R}}_{ZZ}\\mathbf{u}+{\\mathbf{R}}_{ZX}{\\boldsymbol{\\beta}}-\\mathbf{r}_{Zy} .\n\\tag{B.42}\\]\nFrom the properties of the multivariate Gaussian distribution\n\\[\n\\begin{multline*}\n\\int_{\\mathbf{u}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\n\\exp\\left(-\\frac{\\|{\\mathbf{R}}_{ZZ}\\mathbf{u}+{\\mathbf{R}}_{ZX}{\\boldsymbol{\\beta}}-\\mathbf{r}_{Zy}\\|^2}{2\\sigma^2}\\right)\n\\,d\\mathbf{u}\\\\\n\\begin{aligned}\n&= \\int_{\\mathbf{v}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\n\\exp\\left(-\\frac{\\|\\mathbf{v}\\|^2}{2\\sigma^2}\\right)|{\\mathbf{R}}_{ZZ}|^{-1}\\,d\\mathbf{v}\\\\\n&=|{\\mathbf{R}}_{ZZ}|^{-1}\n\\end{aligned}\n\\end{multline*}\n\\tag{B.43}\\]\nfrom which we obtain the likelihood as\n\\[\nL({\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}},\\sigma;{\\mathbf{y}})=\n\\frac{|{\\mathbf{R}}_{ZZ}|^{-1}}{(2\\pi\\sigma^2)^{n/2}}\n\\exp\\left(-\\frac{r_{yy}^2 + \\|{\\mathbf{R}}_{XX}({\\boldsymbol{\\beta}}-\\widehat{{\\boldsymbol{\\beta}}})\\|^2}{2\\sigma^2}\\right) ,\n\\tag{B.44}\\]\nwhere the conditional estimate, \\(\\widehat{{\\boldsymbol{\\beta}}}\\), given \\({\\boldsymbol{\\theta}}\\), satisfies \\[\n{\\mathbf{R}}_{XX}\\widehat{{\\boldsymbol{\\beta}}} = \\mathbf{r}_{Xy} .\n\\]\nSetting \\({\\boldsymbol{\\beta}}=\\widehat{{\\boldsymbol{\\beta}}}\\) and taking the logarithm provides the estimate of \\(\\sigma^2\\), given \\({\\boldsymbol{\\theta}}\\), as\n\\[\n\\widehat{\\sigma^2}=\\frac{r_\\mathbf{yy}^2}{n}\n\\tag{B.45}\\]\nwhich gives the profiled log-likelihood, \\(\\ell({\\boldsymbol{\\theta}}|{\\mathbf{y}})=\\log L({\\boldsymbol{\\theta}},\\widehat{{\\boldsymbol{\\beta}}},\\widehat{\\sigma})\\), on the deviance scale, as\n\\[\n-2\\ell({\\boldsymbol{\\theta}}|{\\mathbf{y}})=2\\log(|{\\mathbf{R}}_{ZZ}|) +\nn\\left(1+\\log\\left(\\frac{2\\pi r_{yy}^2({\\boldsymbol{\\theta}})}{n}\\right)\\right)\n\\tag{B.46}\\]\nOne of the interesting aspects of this formulation is that it is not necessary to solve for the conditional estimate of \\({\\boldsymbol{\\beta}}\\) or the conditional modes of the random effects when evaluating the log-likelihood. The two values needed for the log-likelihood evaluation, \\(2\\log(|{\\mathbf{R}}_{ZZ}|)\\) and \\(r_\\mathbf{yy}^2\\), are obtained directly from the diagonal elements of the Cholesky factor.\nFurthermore, \\({\\boldsymbol{\\Omega}}_{\\theta}\\) and, from that, the Cholesky factor, \\({\\mathbf{R}}_{\\theta}\\), and the objective to be optimized can be evaluated for a given value of \\({\\boldsymbol{\\theta}}\\) from\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\mathbf{Z}^\\prime\\mathbf{Z} & \\mathbf{Z}^\\prime{\\mathbf{X}}& \\mathbf{Z}^\\prime{\\mathbf{y}}\\\\\n{\\mathbf{X}}^\\prime\\mathbf{Z} & {\\mathbf{X}}^\\prime{\\mathbf{X}}& {\\mathbf{X}}^\\prime{\\mathbf{y}}\\\\\n{\\mathbf{y}}^\\prime\\mathbf{Z} & {\\mathbf{y}}^\\prime{\\mathbf{X}}& {\\mathbf{y}}^\\prime{\\mathbf{y}}\n\\end{bmatrix}\n\\tag{B.47}\\]\nand \\({\\boldsymbol{\\Lambda}}_{\\theta}\\).\nIn the MixedModels package the LinearMixedModel struct contains a symmetric blocked array in the A field and a similarly structured lower-triangular blocked array in the L field. Evaluation of the objective simply involves updating the template matrices, \\({\\boldsymbol{\\Lambda}}_i, i=1,\\dots,k\\) in the ReMat structures then updating L from A and the \\(\\lambda_i\\)."
  },
  {
    "objectID": "linalg.html#sec-REML",
    "href": "linalg.html#sec-REML",
    "title": "Appendix B — Linear Algebra for Linear Models",
    "section": "B.8 The REML criterion",
    "text": "B.8 The REML criterion\nThe so-called REML estimates of variance components are often preferred to the maximum likelihood estimates. (“REML” can be considered to be an acronym for “restricted” or “residual” maximum likelihood, although neither term is completely accurate because these estimates do not maximize a likelihood.) We can motivate the use of the REML criterion by considering a linear regression model,\n\\[\n{\\mathcal{Y}}\\sim{\\mathcal{N}}({\\mathbf{X}}{\\boldsymbol{\\beta}},\\sigma^2{\\mathbf{I}}_n),\n\\tag{B.48}\\]\nin which we typically estimate \\(\\sigma^2\\) as\n\\[\n\\widehat{\\sigma^2_R}=\\frac{\\|{\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\|^2}{n-p}\n\\tag{B.49}\\]\neven though the maximum likelihood estimate of \\(\\sigma^2\\) is\n\\[\n\\widehat{\\sigma^2_{L}}=\\frac{\\|{\\mathbf{y}}-\\vec\nX\\widehat{{\\boldsymbol{\\beta}}}\\|^2}{n} .\n\\tag{B.50}\\]\nThe argument for preferring \\(\\widehat{\\sigma^2_R}\\) to \\(\\widehat{\\sigma^2_{L}}\\) as an estimate of \\(\\sigma^2\\) is that the numerator in both estimates is the sum of squared residuals at \\(\\widehat{{\\boldsymbol{\\beta}}}\\) and, although the residual vector, \\({\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}}\\), is an \\(n\\)-dimensional vector, it satisfies \\(p\\) linearly independent constraints, \\({\\mathbf{X}}'({\\mathbf{y}}-{\\mathbf{X}}\\widehat{{\\boldsymbol{\\beta}}})=\\mathbf{0}\\). That is, the residual at \\(\\widehat{{\\boldsymbol{\\beta}}}\\) is the projection of the observed response vector, \\({\\mathbf{y}}\\), into an \\((n-p)\\)-dimensional linear subspace of the \\(n\\)-dimensional response space. The estimate \\(\\widehat{\\sigma^2_R}\\) takes into account the fact that \\(\\sigma^2\\) is estimated from residuals that have only \\(n-p\\) degrees of freedom.\nAnother argument often put forward for REML estimation is that \\(\\widehat{\\sigma^2_R}\\) is an unbiased estimate of \\(\\sigma^2\\), in the sense that the expected value of the estimator is equal to the value of the parameter. However, determining the expected value of an estimator involves integrating with respect to the density of the estimator and we have seen that densities of estimators of variances will be skewed, often highly skewed. It is not clear why we should be interested in the expected value of a highly skewed estimator. If we were to transform to a more symmetric scale, such as the estimator of the standard deviation or the estimator of the logarithm of the standard deviation, the REML estimator would no longer be unbiased. Furthermore, this property of unbiasedness of variance estimators does not generalize from the linear regression model to linear mixed models. This is all to say that the distinction between REML and ML estimates of variances and variance components is probably less important than many people believe.\nNevertheless it is worthwhile seeing how the computational techniques described in this chapter apply to the REML criterion because the REML parameter estimates \\(\\widehat{{\\boldsymbol{\\theta}}}_R\\) and \\(\\widehat{\\sigma_R^2}\\) for a linear mixed model have the property that they would specialize to \\(\\widehat{\\sigma^2_R}\\) from Equation B.49 for a linear regression model, as seen in Section 1.3.3.\nAlthough not usually derived in this way, the REML criterion (on the deviance scale) can be expressed as\n\\[\nd_R({\\boldsymbol{\\theta}},\\sigma|{\\mathbf{y}})=-2\\log\n\\int_{\\mathbb{R}^p}L({\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}},\\sigma|{\\mathbf{y}})\\,d{\\boldsymbol{\\beta}}.\n\\tag{B.51}\\]\nThe REML estimates \\(\\widehat{{\\boldsymbol{\\theta}}}_R\\) and \\(\\widehat{\\sigma_R^2}\\) minimize \\(d_R({\\boldsymbol{\\theta}},\\sigma|{\\mathbf{y}})\\).\nTo evaluate this integral we form an expansion, similar to Equation B.44, of \\(r^2_{\\theta,\\beta}\\) about \\(\\widehat{{\\boldsymbol{\\beta}}}_\\theta\\)\n\\[\nr^2_{\\theta,\\beta}=r^2_\\theta+\\|{\\mathbf{R}}_{XX}({\\boldsymbol{\\beta}}-\\widehat{{\\boldsymbol{\\beta}}}_\\theta)\\|^2 .\n\\tag{B.52}\\]\nfrom which we can derive\n\\[\n\\int_{\\mathbb{R}^p}\\frac{\\exp\\left(-\\frac{r^2_{\\theta,\\beta}}{2\\sigma^2}\\right)}\n{(2\\pi\\sigma^2)^{n/2}|{\\mathbf{R}}_{ZZ}|} \\,d{\\boldsymbol{\\beta}}=\n\\frac{\\exp\\left(-\\frac{r^2_\\theta}{2\\sigma^2}\\right)}\n{(2\\pi\\sigma^2)^{(n-p)/2}|{\\mathbf{R}}_{ZZ}||{\\mathbf{R}}_X|}\n\\tag{B.53}\\]\ncorresponding to a REML criterion on the deviance scale of\n\\[\nd_R({\\boldsymbol{\\theta}},\\sigma|{\\mathbf{y}})=(n-p)\\log(2\\pi\\sigma^2)+\n2\\log\\left(|{\\mathbf{R}}_{ZZ}||{\\mathbf{R}}_X|\\right)+\\frac{r^2_\\theta}{\\sigma^2} .\n\\tag{B.54}\\]\nPlugging in the conditional REML estimate, \\(\\widehat{\\sigma^2}_R=r^2_\\theta/(n-p)\\), provides the profiled REML criterion\n\\[\n\\tilde{d}_R({\\boldsymbol{\\theta}}|{\\mathbf{y}})=\n2\\log\\left(|{\\mathbf{R}}_{ZZ}||{\\mathbf{R}}_X|\\right)+(n-p)\n\\left[1+\\log\\left(\\frac{2\\pi r^2_\\theta}{n-p}\\right)\\right].\n\\tag{B.55}\\]\nThe REML estimate of \\({\\boldsymbol{\\theta}}\\) is\n\\[\n\\widehat{{\\boldsymbol{\\theta}}}_R=\\arg\\min_{{\\boldsymbol{\\theta}}}\\tilde{d}_R({\\boldsymbol{\\theta}}|{\\mathbf{y}}) ,\n\\tag{B.56}\\]\nand the REML estimate of \\(\\sigma^2\\) is the conditional REML estimate of \\(\\sigma^2\\) at \\(\\widehat{{\\boldsymbol{\\theta}}}_R\\),\n\\[\n\\widehat{\\sigma^2_R}=r^2_{\\widehat\\theta_R}/(n-p) .\n\\tag{B.57}\\]\nIt is not entirely clear how one would define a “REML estimate” of \\({\\boldsymbol{\\beta}}\\) because the REML criterion, \\(d_R({\\boldsymbol{\\theta}},\\sigma|{\\mathbf{y}})\\), defined in Equation B.54, does not depend on \\({\\boldsymbol{\\beta}}\\). However, it is customary (and not unreasonable) to use \\(\\widehat{{\\boldsymbol{\\beta}}}_R=\\widehat{{\\boldsymbol{\\beta}}}_{\\widehat{{\\boldsymbol{\\theta}}}_R}\\) as the REML estimate of \\({\\boldsymbol{\\beta}}\\).\n\n\n\n\nBates, D., Maechler, M., Bolker, B. M., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01"
  },
  {
    "objectID": "aGHQ.html#the-bernoulli-glmm",
    "href": "aGHQ.html#the-bernoulli-glmm",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.1 The Bernoulli GLMM",
    "text": "C.1 The Bernoulli GLMM\nThe Bernoulli GLMM model defines the conditional distribution \\(({{\\mathcal{Y}}}|{{\\mathcal{B}}}={{\\mathbf{b}}})\\) as independent Bernoulli random variables with expected values \\({{\\boldsymbol{\\mu}}}={{\\mathbf{g}}}^{-1}({{\\boldsymbol{\\eta}}})\\), where \\({{\\boldsymbol{\\eta}}}={{\\mathbf{X}}}{{\\boldsymbol{\\beta}}}+{{\\mathbf{Z}}}{{\\mathbf{b}}}\\) is the linear predictor and \\({\\mathbf{g}}^{-1}\\) is an inverse link function.\nWe will use the logit link function, \\({\\boldsymbol{\\eta}}={\\mathbf{g}}({\\boldsymbol{\\mu}})\\), defined component-wise from the scalar logit link, \\(g\\), as \\[\n\\eta_i=g(\\mu_i)=\\mathrm{logit}(\\mu_i)=\\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\\quad i=1,\\dots,n .\n\\] The inverse link, \\({\\boldsymbol{\\mu}}={\\mathbf{g}}^{-1}({\\boldsymbol{\\eta}})\\), is similarly defined component-wise from the inverse of the scalar logit, which is the scalar logistic function, \\[\n\\mu_i=g^{-1}(\\eta_i)=\\mathrm{logistic}(\\eta_i)=\\frac{1}{1+e^{-\\eta_i}}\\quad i=1,\\dots,n .\n\\tag{C.1}\\] The logit is the canonical link function (Section 6.2.2) for the Bernoulli distribution.\nAs in the linear mixed model discussed in Section B.7, the \\(q\\)-dimensional random effects, \\({\\mathcal{B}}\\), are expressed as \\({\\mathcal{B}}={\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\,{\\mathcal{U}}\\) where \\({\\mathcal{U}}\\) has a standard, multivariate Gaussian distribution (Section B.2) \\[\n{\\mathcal{U}}\\sim{\\mathcal{N}}({\\mathbf{0}},{\\mathbf{I}}_q) ,\n\\tag{C.2}\\] with probability density function \\[\nf_{{\\mathcal{U}}}({\\mathbf{u}})=\\frac{1}{\\sqrt{2\\pi}^q}e^{-\\|{\\mathbf{u}}\\|^2/2} .\n\\tag{C.3}\\]\nFor a linear mixed model the distribution of these spherical random effects was given as \\({\\mathcal{U}}\\sim({\\mathbf{0}},\\sigma^2{\\mathbf{I}}_q)\\) (Equation B.35). A dispersion parameter like \\(\\sigma^2\\) is not present in Equation C.2 because the Bernoulli distribution does not have a separate dispersion parameter — it is entirely determined by its mean.\nAs is the case for the linear mixed model, the covariance factor, \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\), is sparse and patterned. It is not uncommon in practical examples, such as the one in Section C.4, for \\({\\boldsymbol{\\theta}}\\) to be one-dimensional and \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}=\\theta\\,{\\mathbf{I}}_q\\), to be a scalar multiple of the \\(q\\times q\\) identity matrix.\n\nC.1.1 Log-likelihood for a Bernoulli GLMM\nThe likelihood for the parameters, \\({\\boldsymbol{\\theta}}\\) and \\({\\boldsymbol{\\beta}}\\), given the observed data, \\({\\mathbf{y}}\\), is the value of the marginal probability mass function for the response, \\({\\mathcal{Y}}\\), evaluated at \\({\\mathbf{y}}\\), the observed vector of {0,1} responses. We obtain this value by integrating the product of the probability mass function for the conditional distribution, \\(({\\mathcal{Y}}|{\\mathcal{U}}={\\mathbf{u}})\\), and unconditional density of \\({\\mathcal{U}}\\) (Equation C.3), with respect to \\({\\mathbf{u}}\\).\nRecall that the probability mass for a single Bernoulli response can be written as \\((1-\\mu)^{1-y}\\mu^y\\), which is the specialization to \\(n=1\\) of the probability mass function for the binomial distribution \\[\n\\binom{n}{y}(1-\\mu)^{n-y}\\mu^y ,\\quad 0\\le\\mu\\le 1, \\quad y\\in\\{0,\\dots,n\\} .\n\\] Because the components of the vector-valued conditional distribution, \\(({\\mathcal{Y}}|{\\mathcal{U}}={\\mathbf{u}})\\), are assumed to be independent, its probability mass function can be written as the product of the probability masses for each component \\[\nf_{{\\mathcal{Y}}|{\\mathcal{U}}={\\mathbf{u}}}({\\mathbf{y}}|{\\mathbf{u}})=\\prod_{i=1}^n \\left[(1-\\mu_i)^{1-y_i}\\mu_i^{y_i}\\right]\n\\quad\\mathrm{where}\\quad\n{\\boldsymbol{\\mu}}={\\mathbf{g}}^{-1}({\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}{\\mathbf{u}}) ,\n\\] providing the likelihood as \\[\n\\begin{aligned}\nL({\\boldsymbol{\\eta}},{\\boldsymbol{\\theta}}|{\\mathbf{y}})&=\n\\int_{{\\mathbf{u}}}f_{{\\mathcal{Y}},{\\mathcal{U}}={\\mathbf{u}}}({\\mathbf{y}},{\\mathbf{u}})f_{{\\mathcal{U}}}({\\mathbf{u}})\\,d{\\mathbf{u}}\\\\\n&=\\int_{{\\mathbf{u}}}\\frac{1}{\\sqrt{2\\pi}^q}e^{\\sum_{i=1}^n(1-y_i)\\log(1-\\mu_i)+y_i\\,\\log(\\mu_i)}\ne^{-\\left\\|{\\mathbf{u}}\\right\\|^2/2}\\,d{\\mathbf{u}}\\\\\n&=\\int_{{\\mathbf{u}}}\\frac{1}{\\sqrt{2\\pi}^q}\\exp\\left(\\frac{\\left\\|{\\mathbf{u}}\\right\\|^2+\\sum_{i=1}^n d(y_i,\\mu_i)}{-2}\\right)\\,d{\\mathbf{u}}\n\\end{aligned}\n\\tag{C.4}\\] where the unit deviances, \\(d(y_i,\\mu_i)\\), are \\[\nd(y_i,\\mu_i)=-2\\left[(1-y_i)\\log(1-\\mu_i)+y_i\\log(\\mu_i)\\right]\\quad i=1,\\dots,n .\n\\tag{C.5}\\]\nBy converting from the logarithm of the probability mass function to the deviance scale, which is negative twice the log-probability, we get a quantity, \\(\\sum_{i=1}^n d(y_i,\\mu_i)\\), which is on the same scale as the squared length, \\(\\|{\\mathbf{u}}\\|^2\\), of a standard multivariate Gaussian. The sum of the unit deviances is analogous to the sum of squared residuals, \\(\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}\\|^2\\), in a linear model.\nIn Section B.7 we showed that the integral defining the likelihood for a linear mixed model, Equation B.43, has an analytic solution. In general, the integral in Equation C.4 does not. We will approximate the value of this integral using a quadratic approximation to the argument of the exponential function in Equation C.4 at the value of \\({\\mathbf{u}}\\) that maximizes the integrand, which is the density of the conditional distribution, \\(({\\mathcal{U}}|{\\mathcal{Y}}={\\mathbf{y}})\\), up to a scale factor. Because the scale factor does not affect the location of the maximum, the value of \\({\\mathbf{u}}\\) that maximizes the integrand, \\[\n\\begin{aligned}\n\\tilde{{\\mathbf{u}}}({\\mathbf{y}}|{\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}})\n&=\\arg\\max_{{\\mathbf{u}}}\\exp\\left(\\frac{\\left\\|{\\mathbf{u}}\\right\\|^2 + \\sum_{i=1}^n d(y_i,\\mu_i)}{-2}\\right)\\\\\n&=\\arg\\min_{{\\mathbf{u}}}\\left(\\left\\|{\\mathbf{u}}\\right\\|^2 + \\sum_{i=1}^n d(y_i,\\mu_i)\\right)\n\\end{aligned} ,\n\\tag{C.6}\\] is also the conditional mode — the value of \\({\\mathbf{u}}\\) that maximizes the conditional density. The expression being minimized in Equation C.6, \\(\\left\\|{\\mathbf{u}}\\right\\|^2 + \\sum_{i=1}^n d(y_i,\\mu_i)\\), is called the penalized deviance.\nUsing a quadratic approximation to the penalized deviance at this conditional mode (i.e. the mode of the conditional distribution of \\({\\mathcal{U}}\\) given \\({\\mathcal{Y}}={\\mathbf{y}}\\)) is equivalent to using a multivariate Gaussian approximation to this conditional distribution. Approximating an integral like Equation C.4 by approximating the integrand as a scaled multivariate Gaussian distribution at its mode is called Laplace’s approximation (Tierney & Kadane (1986)).\nThe penalized iteratively re-weighted least squares (PIRLS) algorithm (Section C.4) provides a fast and stable method of determining the conditional mode, \\(\\tilde{{\\mathbf{u}}}({\\mathbf{y}}|{\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}})\\) (Equation C.6), thereby making it feasible to use Laplace’s approximation at scale.\nBefore discussing PIRLS, however, we will describe generalized linear models (GLMs) without random effects (Section C.2), for which the deviance is defined as the sum of the unit deviances and the maximum likelihood estimate of the coefficient vector, \\(\\widehat{{\\boldsymbol{\\beta}}}\\), is the value that minimizes the deviance. In Section C.3 we describe the iteratively re-weighted least squares (IRLS) algorithm, which is a stable, fast algorithm to minimize the deviance.\nWe will illustrate the IRLS algorithm with the contra data discussed in Chapter 6 and a model like com05, which was fit in that chapter, but without the random effects. Later we will use the full com05 model to illustrate some of the computations for GLMMs.\nAlthough 0/1 responses and the Bernoulli distribution are easy to describe, the theory of the generalized linear mixed model (GLMM) and the details of the implementation are not. Readers who wish to focus on practical applications more than on the theory should feel free to skim this appendix.\nLoad the packages to be used\n\n\nCode\nusing AlgebraOfGraphics\nusing BenchmarkTools\nusing CairoMakie\nusing EmbraceUncertainty: dataset\nusing FreqTables\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie\nusing NLopt\nusing PooledArrays\nusing ProgressMeter\nusing StatsAPI\n\n# showcompact prints a vector compactly\nshowcompact(x) = show(IOContext(stdout, :compact =&gt; true), x)\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")"
  },
  {
    "objectID": "aGHQ.html#sec-BernoulliGLM",
    "href": "aGHQ.html#sec-BernoulliGLM",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.2 Generalized linear models for binary data",
    "text": "C.2 Generalized linear models for binary data\nTo introduce some terms and workflows we first consider the generalized linear model (GLM) for the Bernoulli distribution and the logit link. The linear predictor for a GLM - a model without random effects - is simply \\[\n{{\\boldsymbol{\\eta}}}= {{\\mathbf{X}}}{{\\boldsymbol{\\beta}}} ,\n\\] and the mean response vector, \\({{\\boldsymbol{\\mu}}}={\\mathbf{g}}^{-1}({\\boldsymbol{\\eta}})\\), is obtained by component-wise application of the scalar logistic function (Equation C.1).\nThe probability mass function for the Bernoulli distribution is \\[\nf_{{\\mathcal{Y}}}(y|\\mu) = \\mu^y\\,(1-\\mu)^{(1-y)}\\quad\\mathrm{for}\\quad y\\in\\{0,1\\} .\n\\]\nBecause the elements of \\({{\\mathcal{Y}}}|{{\\boldsymbol{\\mu}}}\\) are assumed to be independent, the log-likelihood is simply the sum of contributions from each element, which, on the deviance scale, can be written in terms of the unit deviances \\[\n\\begin{aligned}\n-2\\,\\ell({{\\boldsymbol{\\mu}}}|{\\mathbf{y}})&= -2\\,\\log(L({{\\boldsymbol{\\mu}}}|{\\mathbf{y}}))\\\\\n&=-2\\,\\sum_{i=1}^n y_i\\log(\\mu_i)+(1-y_i)\\log(1-\\mu_i) .\n\\end{aligned}\n\\tag{C.7}\\]\nAs described above, it is customary when working with GLMs to convert the log-likelihood to a deviance, which, for the Bernoulli distribution, is negative twice the log-likelihood. (For other distributions, the deviance may incorporate an additional term that depends only on \\({\\mathbf{y}}\\).)\nOne reason for preferring the deviance scale is that the change in deviance for nested models has approximately a \\(\\chi^2\\) distribution with degrees of freedom determined by the number of independent constraints on the parameters in the simpler model. Especially for GLMs, the deviance plays a role similar to the sum of squared residuals in linear models.\nFor greater numerical precision we avoid calculating \\(1-\\mu\\) directly when evaluating expressions like Equation C.7 and instead use \\[\n1 - \\mu = 1 - \\frac{1}{1+e^{-\\eta}}=\\frac{e^{-\\eta}}{1+e^{-\\eta}} .\n\\] Evaluation of the last expression provides greater precision for large negative values of \\(\\eta\\) (corresponding to small values of \\(\\mu\\)) than does first evaluating \\(\\mu\\) followed by \\(1 - \\mu\\).\nAfter some algebra, we write the unit deviance, \\(d(y_i,\\eta_i)\\), which is the contribution to the deviance from the \\(i\\)th observation, as \\[\n\\begin{aligned}\nd(y_i, \\eta_i)&=-2\\left[y_i\\log(\\mu_i)+(1-y_i)\\log(1-\\mu_i)\\right]\\\\\n&=2\\left[(1-y_i)\\eta_i-\\log(1+e^{-\\eta_i})\\right]\n\\end{aligned}\n\\quad i=1,\\dots,n\n\\]\nA Julia function to evaluate both the mean and the unit deviance can be written as\n\nfunction meanunitdev(y::T, η::T) where {T&lt;:AbstractFloat}\n  expmη = exp(-η)\n  return (; μ=inv(1 + expmη), dev=2 * ((1 - y) * η + log1p(expmη)))\nend\n\n\n\n\n\n\n\nlog1p\n\n\n\n\n\nMathematically log1p, read log of 1 plus, is defined as \\(\\mathrm{log1p}(x)=\\log(1+x)\\) but it is implemented in such a way as to provide greater accuracy when \\(x\\) is small. For example,\n\nlet small = eps() / 10\n  @show small\n  @show 1 + small\n  @show log(1 + small)\n  @show log1p(small)\nend;\n\nsmall = 2.2204460492503132e-17\n1 + small = 1.0\nlog(1 + small) = 0.0\nlog1p(small) = 2.2204460492503132e-17\n\n\n1 + small evaluates to 1.0 in floating point arithmetic because of round-off, producing 0 for the expression log(1 + small), whereas log1p(small) ≈ small, as it should be.\n\n\n\nThis function returns a NamedTuple of values from scalar arguments. For example,\n\nmeanunitdev(0.0, 0.21)\n\n(μ = 0.5523079095743253, dev = 1.6072991620435673)\n\n\nA Vector of such NamedTuples is a row-table (Section A.2), which can be updated in place by dot-vectorization of the scalar meanunitdev function, as shown below.\n\nC.2.1 An example: fixed-effects only from com05\nWe illustrate some of these computations using only the fixed-effects specification for com05, a GLMM fit to the contra data set in Chapter 6. Because we will use the full GLMM later we reproduce com05 by loading the data, creating the binary ch variable indicating children/no-children, defining the contrasts and formula to be used, and fitting the model as in Chapter 6.\n\n\nCode\ncontra = let tbl = dataset(:contra)\n  Table(tbl; ch=tbl.livch .≠ \"0\")\nend\ncontrasts = Dict{Symbol,Any}(\n  :urban =&gt; HelmertCoding(),\n  :ch =&gt; HelmertCoding(),\n  :dist =&gt; Grouping(),\n)\ncom05 =\n  let f = @formula use ~\n      1 + urban + ch * age + age & age + (1 | dist & urban)\n    fit(MixedModel, f, contra, Bernoulli(); contrasts, nAGQ=9)\n  end\n\n\nExtract the fixed-effects model matrix, \\({\\mathbf{X}}\\), and initialize the coefficient vector, \\({\\boldsymbol{\\beta}}\\), to a copy (in case we modify it) of the estimated fixed-effects.\n\nβm05 = copy(com05.β)\nshowcompact(βm05)   # showcompact is defined in the first code block\n\n[-0.341475, 0.393592, 0.606449, -0.0129098, 0.0332106, -0.00562463]\n\n\nAs stated above, the meanunitdev function can be applied to the vectors, \\({\\mathbf{y}}\\) and \\({{\\boldsymbol{\\eta}}}\\), via dot-vectorization to produce a Vector{NamedTuple}, which is the typical form of a row-table.\n\nrowtbl = meanunitdev.(com05.y, com05.X * βm05)\ntypeof(rowtbl)\n\n\nVector{NamedTuple{(:μ, :dev), Tuple{Float64, Float64}}} (alias for Array{NamedTuple{(:μ, :dev), Tuple{Float64, Float64}}, 1})\n\n\n\nFor display we convert the row-table to a column-table and prepend another column-table consisting of \\({\\mathbf{y}}\\) and \\({\\boldsymbol{\\eta}}\\).\n\nTable((; y=com05.y, η=com05.X * βm05), rowtbl)  # display as a Table\n\nTable with 4 columns and 1934 rows:\n      y    η          μ         dev\n    ┌───────────────────────────────────\n 1  │ 0.0  -0.87965   0.29325   0.694158\n 2  │ 0.0  -0.471779  0.384195  0.96965\n 3  │ 0.0  0.676137   0.662876  2.17461\n 4  │ 0.0  0.429243   0.605693  1.86125\n 5  │ 0.0  -0.96316   0.276246  0.646607\n 6  │ 0.0  -0.772819  0.31587   0.759213\n 7  │ 0.0  -0.87965   0.29325   0.694158\n 8  │ 0.0  0.515012   0.625981  1.9669\n 9  │ 0.0  0.371817   0.591898  1.79248\n 10 │ 0.0  0.676137   0.662876  2.17461\n 11 │ 1.0  -0.772819  0.31587   2.30485\n 12 │ 0.0  -0.473125  0.383877  0.968617\n 13 │ 0.0  0.449039   0.610411  1.88532\n 14 │ 0.0  0.60255    0.64624   2.07827\n 15 │ 0.0  0.645435   0.655981  2.13412\n 16 │ 1.0  0.637821   0.654261  0.848499\n 17 │ 0.0  -0.471779  0.384195  0.96965\n 18 │ 1.0  0.645435   0.655981  0.843247\n 19 │ 1.0  0.283345   0.570366  1.12295\n 20 │ 0.0  0.515012   0.625981  1.9669\n 21 │ 0.0  -0.460979  0.386754  0.977977\n 22 │ 0.0  -0.627476  0.348083  0.855677\n 23 │ 0.0  0.674614   0.662536  2.17259\n ⋮  │  ⋮       ⋮         ⋮         ⋮\n\n\nThe deviance for this value of \\({{\\boldsymbol{\\beta}}}\\) in this model is the sum of the unit deviances, which we write as sum applied to a generator expression. (In general we extract columns of a row-table with generator expressions that produce iterators.)\n\nsum(r.dev for r in rowtbl)\n\n2411.1935798730165\n\n\n\n\nC.2.2 Encapsulating the model in a struct\nWhen minimizing the deviance it is convenient to have the different components of the model encapsulated in a user-created struct type so we can update the parameter values and evaluate the deviance without needing to keep track of all the pieces of the model.\n\nstruct BernoulliGLM{T&lt;:AbstractFloat}\n  X::Matrix{T}\n  β::Vector{T}\n  ytbl::NamedTuple{(:y, :η),NTuple{2,Vector{T}}}\n  rtbl::Vector{NamedTuple{(:μ, :dev),NTuple{2,T}}}\nend\n\nWe also create an external constructor, which is a function defined outside the struct and of the same name as the struct, that constructs and returns an object of that type. In this case the external constructor creates a BernoulliGLM from the model matrix and the response vector, after some consistency checks on the arguments passed to it.\n\nfunction BernoulliGLM(\n  X::Matrix{T},\n  y::Vector{T},\n) where {T&lt;:AbstractFloat}\n\n  # check consistency of arguments\n  n = size(X, 1)                  # number of rows in X\n  if length(y) ≠ n || any(!in([0, 1]), y)\n    throw(ArgumentError(\"y is not an $n-vector of 0's and 1's\"))\n  end\n\n  # initial β from linear regression of y in {-1,1} coding\n  β = X \\ replace(y, 0 =&gt; -1)\n  η = X * β\n\n  return BernoulliGLM(X, β, (; y, η), meanunitdev.(y, η))\nend\n\nTo optimize the deviance we define an extractor method that returns the deviance\n\nStatsAPI.deviance(m::BernoulliGLM) = sum(r.dev for r in m.rtbl)\n\n\n\n\n\n\n\nWhy StatsAPI.deviance and not just deviance?\n\n\n\n\n\nThis extractor is written as a method for the generic deviance function defined in the StatsAPI package. Doing so allows us to use the deviance name for the extractor without interfering with deviance methods defined for other model types.\n\n\n\nWe also define a mutating function, setβ!, that installs a new value of β then updates η and rtbl in place.\n\nfunction setβ!(m::BernoulliGLM, newβ)\n  (; y, η) = m.ytbl                # destructure ytbl\n  mul!(η, m.X, copyto!(m.β, newβ)) # η = X * newβ in place\n  m.rtbl .= meanunitdev.(y, η)     # update rtbl in place\n  return m\nend\n\nCreate such a struct from X and y for model com05.\n\ncom05fe = BernoulliGLM(com05.X, com05.y)\nβ₀ = copy(com05fe.β)          # keep a copy of the initial values\nshowcompact(β₀)\n\n[-0.107533, 0.175966, 0.219448, -0.00285474, 0.0101399, -0.00216183]\n\n\nThese initial values of \\({\\boldsymbol{\\beta}}\\) are from a least squares fit of \\({\\mathbf{y}}\\), converted from {0,1} coding to {-1,1} coding, on the model matrix, \\({\\mathbf{X}}\\).\nAs a simple test of the setβ! and deviance methods we can check that com05fe produces the same deviance value for βm05 as was evaluated above.\n\ndeviance(setβ!(com05fe, βm05))\n\n2411.1935798730165\n\n\nFor fairness in later comparisons we restore the initial values β₀ to the model. These are rough starting estimates with a deviance that is considerably greater than that at βm05.\n\ndeviance(setβ!(com05fe, β₀))\n\n2491.151439053724\n\n\n\n\nC.2.3 Fit the GLM using a general optimizer\nWe can use a general optimizer like those available in NLopt.jl to minimize the deviance. Following the instructions given at that package’s repository, we create an Opt object specifying the algorithm to be used, BOBYQA (Powell (2009)), and the dimension of the problem, then define and assign the objective function in the required form, and call optimize\n\nfunction StatsAPI.fit!(m::BernoulliGLM{T}) where {T}\n  opt = Opt(:LN_BOBYQA, length(m.β))\n  function objective(x::Vector{T}, g::Vector{T}) where {T}\n    isempty(g) || throw(\n      ArgumentError(\"Gradient not available, g must be empty\"),\n    )\n    return deviance(setβ!(m, x))\n  end\n  opt.min_objective = objective\n  minf, minx, ret = optimize(opt, copy(m.β))\n  @info (; code=ret, nevals=opt.numevals, minf)\n  return m\nend\n\n\nfit!(com05fe);\n\n[ Info: (code = :ROUNDOFF_LIMITED, nevals = 545, minf = 2409.377428160019)\n\n\nThe optimizer has determined a coefficient vector that reduces the deviance to 2409.38, at which point convergence was declared because changes in the objective are limited by round-off. This required about 500 evaluations of the deviance at candidate values of \\({\\boldsymbol{\\beta}}\\).\nEach evaluation of the deviance is fast, requiring only a fraction of a millisecond on a laptop computer,\n\nβopt = copy(com05fe.β)\n@benchmark deviance(setβ!(m, β)) seconds = 1 setup =\n  (m = com05fe; β = βopt)\n\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  27.333 μs … 617.625 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     32.125 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   32.164 μs ±  10.004 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n      █▁        ▅▆▃                                             \n  ▃▃▄███▅▃▂▃▃▃▅████▇▅▅▄▃▃▃▄▄▄▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂ ▃\n  27.3 μs         Histogram: frequency by time           47 μs &lt;\n Memory estimate: 16 bytes, allocs estimate: 1.\n\n\n\nbut the already large number of evaluations for these six coefficients would not scale well as this dimension increases.\nFortunately there is an algorithm, called iteratively reweighted least squares (IRLS), that uses the special structure of the GLM to provide fast and stable convergence to estimates of the coefficients, even for models with a large number of coefficients. This will be important to us in fitting GLMMs where we must optimize with respect to the random effects, whose dimension can be large."
  },
  {
    "objectID": "aGHQ.html#sec-IRLS",
    "href": "aGHQ.html#sec-IRLS",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.3 The IRLS algorithm",
    "text": "C.3 The IRLS algorithm\nAs we have seen, in a GLM we are modeling the responses and the predicted values on two scales — the linear predictor scale, for \\({\\boldsymbol{\\eta}}\\), and the response scale, for \\({\\mathbf{y}}\\) and \\({\\boldsymbol{\\mu}}\\). The scalar link function, \\(\\eta=g(\\mu)\\), and the inverse link, \\(\\mu=g^{-1}(\\eta)\\), map vectors component-wise between these two scales.\nFor operations like determining a new candidate value of \\({\\boldsymbol{\\beta}}\\), the linear predictor scale is preferred, because, on that scale, \\({\\boldsymbol{\\eta}}={\\mathbf{X}}{\\boldsymbol{\\beta}}\\) is a linear function of \\({\\boldsymbol{\\beta}}\\). Thus it would be convenient if we could transform the response, \\({\\mathbf{y}}\\), to the linear predictor scale where we could define a residual and use some form of minimizing a sum of squared residuals to evaluate a new coefficient vector (or, alternatively, evaluate an increment that will be added to the current coefficient vector). Unfortunately, a naive approach of transforming \\({\\mathbf{y}}\\) to the linear predictor scale won’t work because the elements of \\({\\mathbf{y}}\\) are all \\(0\\) or \\(1\\) and the logit link function maps these values to \\(-\\infty\\) and \\(\\infty\\), respectively.\nFor an iterative algorithm, however, we can use a local linear approximation to the link function to define a working residual, from which to evaluate an increment to the coefficient vector, or a working response, from which we evaluate the new coefficient vector directly. Because the link and inverse link functions are defined component-wise we will define the approximation for scalars \\(y_i\\), \\(\\mu_i\\), and \\(\\eta_i\\) and for the scalar link function, \\(g\\), with the understanding that these definitions apply component-wise to the vectors.\nThe working residual is evaluated by mapping the residual on the response scale, \\(y_i-\\mu_i\\), through the linear approximation to the link, \\(g(\\mu)\\), at \\(\\mu_i\\). That is, \\[\n\\tilde{r_i}=(y_i-\\mu_i)g'(\\mu_i)\\quad i=1,\\dots,n .\n\\] Because the derivative, \\(g'(\\mu_i)\\), for the logit link function is \\(1/[\\mu_i(1-\\mu_i)]\\), these working residuals are \\[\n\\tilde{r}_i = (y_i-\\mu_i)g'(\\mu_i) = \\frac{y_i - \\mu_i}{\\mu_i(1-\\mu_i)}\\quad i=1,\\dots,n .\n\\] Similarly, the working response on the linear predictor scale, is defined by adding the working residual to the current linear predictor value, \\[\n\\tilde{y_i}=\\eta_i + \\tilde{r_i}=\\eta_i +(y_i-\\mu_i)g'(\\mu_i)=\n\\eta_i + \\frac{y_i - \\mu_i}{\\mu_i(1-\\mu_i)}\\quad i=1,\\dots,n .\n\\]\nOn the linear predictor scale we can fit a linear model to the working response to obtain a new parameter vector, but we must take into account that the variances of the noise terms in this linear model, which are the working residuals, are not constant. We use weighted least squares where the weights are inversely proportional to the variance of the working residual. The variance of the random variable \\({\\mathcal{Y}}_i\\) is \\(\\mu_i(1-\\mu_i)\\), hence the variance of the working residual is \\[\n\\mathrm{Var}(\\tilde{r_i})=g'(\\mu_i)^2 \\mathrm{Var}({\\mathcal{Y}}_i)=\\frac{\\mu_i(1-\\mu_i)}{\\left[\\mu_i(1-\\mu_i)\\right]^2}=\\frac{1}{\\mu_i(1-\\mu_i)}\n\\quad i=1,\\dots,n .\n\\]\nThus the working weights are \\[\n\\begin{aligned}\nw_i&=\\mu_i(1-\\mu_i)\\\\\n&=\\frac{1}{1+e^{-\\eta_i}}\\frac{e^{-\\eta_i}}{1+e^{-\\eta_i}}\n\\end{aligned}\n,\\quad i=1,\\dots,n.\n\\]\nIn practice we will use the square roots of the working weights, evaluated as \\[\n\\sqrt{w_i}=\\frac{\\sqrt{e^{-\\eta_i}}}{1+e^{-\\eta_i}}=\\frac{e^{-\\eta_i/2}}{1+e^{-\\eta_i}}\\quad i=1,\\dots,n .\n\\]\nNote that \\(\\mathrm{Var}({\\mathcal{Y}}_i)\\) happens to be the inverse of \\(g'(\\mu_i)\\) for a Bernoulli response and the logit link function. This will always be true for distributions in the exponential family and their canonial links.\nAt the \\(k\\)th iteration the IRLS algorithm updates the coefficient vector to \\({\\boldsymbol{\\beta}}^{(k)}\\), which is a weighted least squares solution that could be written as \\[\n{\\boldsymbol{\\beta}}^{(k)}= \\left({\\mathbf{X}}'{\\mathbf{W}}{\\mathbf{X}}\\right)^{-1}\\left({\\mathbf{X}}'{\\mathbf{W}}\\tilde{{\\mathbf{y}}}\\right) ,\n\\] where \\({\\mathbf{W}}\\) is an \\(n\\times n\\) diagonal matrix of the working weights and \\(\\tilde{{\\mathbf{y}}}\\) is the working response, both evaluated at \\({\\boldsymbol{\\beta}}^{(k-1)}\\), the coefficient vector from the previous iteration.\nIn practice we use the square roots of the working weights, which we write as a diagonal matrix, \\({\\mathbf{W}}^{1/2}\\), and a QR decomposition (Section B.6) of a weighted model matrix, \\({\\mathbf{W}}^{1/2}{\\mathbf{X}}\\), to solve for the updated coefficient vector from the weighted working response, \\({\\mathbf{W}}^{1/2}\\tilde{{\\mathbf{y}}}\\), with elements \\[\n\\begin{aligned}\n\\sqrt{w_i}(\\eta_i+\\tilde{r}_i)&=\\sqrt{\\mu_i(1-\\mu_i)}(\\eta_i+\\tilde{r}_i)\\\\\n&=\\sqrt{w_i}\\eta_i +\\frac{(y_i-\\mu_i)\\sqrt{\\mu_i(1-\\mu_i)}}{\\mu_i(1-\\mu_i)}\\\\\n&=\\sqrt{w_i}\\eta_i +\\frac{y_i-\\mu_i}{\\sqrt{w_i}}\n\\end{aligned},\\quad i=1,\\dots,n\n\\]\nIt is possible to write the IRLS algorithm using a weighted least squares fit of the working residuals on the model matrix to determine a parameter increment. However, in the PIRLS algorithm it is necessary to use the working response, not the working residual, so we define the IRLS algorithm in those terms too.\nFurthermore, in the PIRLS algorithm we will need to allow for an offset when calculating the working response. In the presence of an offset, \\({\\mathbf{o}}\\), a constant vector of length \\(n\\), the linear predictor is defined as \\[\n{\\boldsymbol{\\eta}}= {\\mathbf{o}}+ {\\mathbf{X}}{\\boldsymbol{\\beta}}.\n\\] The mean, \\({\\boldsymbol{\\mu}}\\), the working weights and the working residuals are defined as before but the working response becomes \\[\n\\tilde{{\\mathbf{y}}}=\\tilde{{\\mathbf{r}}} + {\\boldsymbol{\\eta}}- {\\mathbf{o}}.\n\\]\nFor a linear model there is rarely a reason for using an offset. Instead we can simply subtract the constant vector, \\({\\mathbf{o}}\\), from the response, \\({\\mathbf{y}}\\), because the response and the linear predictor are on the same scale. However, this is not the case for a GLM where we must deal with the effects of the constant offset on the linear predictor scale, not on the response scale.\n\nC.3.1 Implementation of IRLS for Bernoulli-Logit\nWe define a BernoulliIRLS struct with three additional elements in the rowtable: the square roots of the working weights, rtwwt, the weighted working residuals, wwres, and the weighted working response, wwresp. In the discussion above, rtwwt is the diagonal of \\({\\mathbf{W}}^{1/2}\\), wwres is \\({\\mathbf{W}}^{1/2}\\tilde{{\\mathbf{r}}}\\) and wwresp is \\({\\mathbf{W}}^{1/2}\\tilde{{\\mathbf{y}}}\\).\nWe also add fields Xqr, in which the weighted model matrix, \\({\\mathbf{W}}^{1/2}{\\mathbf{X}}\\), is formed followed by its QR decomposition, and βcp, which holds a copy of the previous coefficient vector.\n\nstruct BernoulliIRLS{T&lt;:AbstractFloat}\n  X::Matrix{T}\n  Xqr::Matrix{T}                # copy of X used in the QR decomp\n  β::Vector{T}\n  βcp::Vector{T}                # copy of previous β\n  Whalf::Diagonal{T,Vector{T}}  # rtwwt as a Diagonal matrix\n  ytbl::NamedTuple{(:y, :η),NTuple{2,Vector{T}}}\n  rtbl::Vector{\n    NamedTuple{(:μ, :dev, :rtwwt, :wwres, :wwresp),NTuple{5,T}},\n  }\nend\n\nwith constructor\n\nfunction BernoulliIRLS(\n  X::Matrix{T},\n  y::Vector{T},\n) where {T&lt;:AbstractFloat}\n  n = size(X, 1)          # number of rows of X\n  if length(y) ≠ n || !all(v -&gt; (iszero(v) || isone(v)), y)\n    throw(ArgumentError(\"y is not an $n-vector of 0's and 1's\"))\n  end\n  # initial β from linear least squares fit of y in {-1,1} coding\n  Xqr = copy(X)\n  β = qr!(Xqr) \\ replace(y, 0 =&gt; -1)\n  βcp = copy(β)\n  η = X * β\n  rtbl = tblrow.(y, η)\n  Whalf = Diagonal([r.rtwwt for r in rtbl])\n  return BernoulliIRLS(X, Xqr, β, βcp, Whalf, (; y, η), rtbl)\nend\n\nThe tblrow function evaluates the mean, unit deviance, square root of the weight, and the weighted, working residual and weighted, working response for scalar \\(y\\) and \\(\\eta\\). The offset argument, which defaults to zero, is not used in calls for BernoulliIRLS models, but will be used in Section C.4 when we discuss the PIRLS algorithm.\n\nfunction tblrow(\n  y::T,\n  η::T,\n  offset::T=zero(T),\n) where {T&lt;:AbstractFloat}\n  rtexpmη = exp(-η / 2)      # square root of exp(-η)\n  expmη = abs2(rtexpmη)      # exp(-η)\n  denom = 1 + expmη\n  μ = inv(denom)\n  dev = 2 * ((1 - y) * η + log1p(expmη))\n  rtwwt = rtexpmη / denom    # sqrt of working wt\n  wwres = (y - μ) / rtwwt    # weighted working resid\n  wwresp = wwres + rtwwt * (η - offset)\n  return (; μ, dev, rtwwt, wwres, wwresp)\nend\n\n\nStatsAPI.deviance(m::BernoulliIRLS) = sum(r.dev for r in m.rtbl)\n\nNext we define a mutating function, updateβ!, that evaluates \\({\\boldsymbol{\\beta}}^{(k)}\\), the updated coefficient vector at iteration \\(k\\), in place by weighted least squares then updates the response table.\n\nfunction updateβ!(m::BernoulliIRLS)\n  (; X, Xqr, β, βcp, Whalf, ytbl, rtbl) = m  # destructure m & ytbl\n  (; y, η) = ytbl\n  copyto!(βcp, β)                            # keep a copy of β\n  copyto!(Whalf.diag, r.rtwwt for r in rtbl) # rtwwt -&gt; Whalf\n  mul!(Xqr, Whalf, X)                        # weighted model matrix\n  copyto!(η, r.wwresp for r in rtbl)         # use η as temp storage\n  ldiv!(β, qr!(Xqr), η)                      # weighted least squares\n  rtbl .= tblrow.(y, mul!(η, X, β))          # update η and rtbl\n  return m\nend\n\nFor our example, we start at the same coefficient vector as we did with the general optimizer.\n\ncom05fe = BernoulliIRLS(com05.X, com05.y)\ndeviance(com05fe)\n\n2491.151439053725\n\n\nThe first IRLS iteration\n\ndeviance(updateβ!(com05fe))\n\n2411.165742459925\n\n\nreduces the deviance substantially.\nWe create a fit! method to iterate to convergence.\n\nfunction StatsAPI.fit!(m::BernoulliIRLS, β₀=m.β; verbose::Bool=true)\n  (; X, β, βcp, ytbl, rtbl) = m\n  (; y, η) = ytbl\n  rtbl .= tblrow.(y, mul!(η, X, copyto!(β, β₀)))\n  olddev = deviance(m)\n  verbose && @info 0, olddev   # record the deviance at initial β\n  for i in 1:100               # perform at most 100 iterations\n    newdev = deviance(updateβ!(m))\n    verbose && @info i, newdev # iteration number and deviance\n    if newdev &gt; olddev\n      @warn \"failure to decrease deviance\"\n      copyto!(β, βcp)          # roll back changes to β, η, and rtbl\n      rtbl = tblrow.(y, mul!(η, X, β))\n      break\n    elseif (olddev - newdev) &lt; (1.0e-10 * olddev)\n      break                    # exit loop if deviance is stable\n    else\n      olddev = newdev\n    end\n  end\n  return m\nend\n\n\nfit!(com05fe, β₀);\n\n[ Info: (0, 2491.151439053724)\n[ Info: (1, 2411.1657424599252)\n[ Info: (2, 2409.3824513371324)\n[ Info: (3, 2409.3774282835834)\n[ Info: (4, 2409.3774281600413)\n\n\nThe IRLS algorithm has converged in 4 iterations to essentially the same deviance as the general optimizer achieved after around 500 function evaluations. Each iteration of the IRLS algorithm takes more time than a deviance evaluation, but still only a fraction of a millisecond on a laptop computer.\n\n@benchmark deviance(updateβ!(m)) seconds = 1 setup = (m = com05fe)\n\n\nBenchmarkTools.Trial: 8007 samples with 1 evaluation.\n Range (min … max):  114.875 μs …  2.926 ms  ┊ GC (min … max): 0.00% … 94.82%\n Time  (median):     122.833 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   123.615 μs ± 42.209 μs  ┊ GC (mean ± σ):  0.53% ±  1.50%\n                         ▁▂▂▂▆██▃                               \n  ▁▁▁▂▂▃▃▄▅▅▅▄▄▃▃▃▃▄▄▅▄▅▆█████████▆▃▃▂▂▂▂▂▂▂▂▁▂▂▂▃▂▂▃▃▃▃▃▃▂▂▂▁ ▃\n  115 μs          Histogram: frequency by time          132 μs &lt;\n Memory estimate: 16.09 KiB, allocs estimate: 5."
  },
  {
    "objectID": "aGHQ.html#sec-PIRLS",
    "href": "aGHQ.html#sec-PIRLS",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.4 GLMMs and the PIRLS algorithm",
    "text": "C.4 GLMMs and the PIRLS algorithm\nIn Section B.7 we showed that, given a value of \\({\\boldsymbol{\\theta}}\\), which determines the relative covariance factor, \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\), of the random effects, \\({\\mathcal{B}}\\), the conditional mode, \\(\\tilde{{\\mathbf{b}}}\\), of the random effects can be evaluated as the solution to a penalized least squares (PLS) problem. It is convenient to write the PLS problem in terms of the spherical random effects, \\({\\mathcal{U}}\\sim{\\mathcal{N}}({\\mathbf{0}},\\sigma^2{\\mathbf{I}})\\), with the defining relationship \\({\\mathcal{B}}={\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}{\\mathcal{U}}\\), as in Equation B.38 \\[\n\\tilde{{\\mathbf{u}}}=\\arg\\min_{{\\mathbf{u}}}\\left(\n\\left\\|{\\mathbf{y}}-{\\mathbf{X}}{\\boldsymbol{\\beta}}-{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}{\\mathbf{u}}\\right\\|^2 +\n\\left\\|{\\mathbf{u}}\\right\\|^2\n\\right) .\n\\]\nWe wrote Equation B.38 for the LMM case as minimizing the penalized sum of squared residuals with respect to both \\({\\boldsymbol{\\beta}}\\) and \\({\\mathbf{u}}\\). Here, and in Equation C.8 below, we minimize with respect to \\({\\mathbf{u}}\\) only while holding \\({\\boldsymbol{\\beta}}\\) fixed.\nThe solution of this PLS problem, \\(\\tilde{\\mathbf{u}}\\), is the conditional mode of \\({\\mathcal{U}}\\), in that it maximizes the density of the conditional distribution, \\(({\\mathcal{U}}|{\\mathcal{Y}}={\\mathbf{y}})\\), at the observed \\({\\mathbf{y}}\\). (In the case of a LMM, where the conditional distributions, \\(({\\mathcal{B}}|{\\mathcal{Y}}={\\mathbf{y}})\\) and \\(({\\mathcal{U}}|{\\mathcal{Y}}={\\mathbf{y}})\\), are multivariate Gaussian, the solution of the PLS problem is also the mean of the conditional distribution, but this property doesn’t carry over to GLMMs.)\nIn a Bernoulli generalized linear mixed model (GLMM) the mode of the conditional distribution, \\(({\\mathcal{U}}|{\\mathcal{Y}}={\\mathbf{y}})\\), minimizes the penalized GLM deviance, \\[\n\\tilde{{\\mathbf{u}}}=\\arg\\min_{{\\mathbf{u}}}\\left(\n\\left\\|{\\mathbf{u}}\\right\\|^2+\\sum_{i-1}^n d(y_i,\\eta_i({\\mathbf{u}}))\n\\right) ,\n\\tag{C.8}\\] where \\(d(y_i,\\eta_i),\\,i=1,\\dots,n\\) are the unit deviances defined in Section C.2. We modify the IRLS algorithm as penalized iteratively re-weighted least squares (PIRLS) to determine these values.\nAs with IRLS, each iteration of the PIRLS algorithm involves using the current linear predictor, \\({\\boldsymbol{\\eta}}({\\mathbf{u}})={\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}{\\mathbf{u}}\\), (\\({\\boldsymbol{\\beta}}\\) and \\({\\boldsymbol{\\theta}}\\) are assumed known and fixed, and \\({\\mathbf{X}}{\\boldsymbol{\\beta}}\\) is an offset) to evaluate the mean, \\({\\boldsymbol{\\mu}}={\\mathbf{g}}^{-1}({\\boldsymbol{\\eta}})\\), of the conditional distribution, \\(({\\mathcal{Y}}|{\\mathcal{U}}={\\mathbf{u}})\\), as well as the unit deviances, \\(d(y_i,\\eta_i)\\), the square roots of the working weights, which are on the diagonal of \\({\\mathbf{W}}^{1/2}\\), and the weighted, working response, \\({\\mathbf{W}}^{1/2}\\tilde{{\\mathbf{y}}}\\). The updated spherical random effects vector, \\({\\mathbf{u}}\\), is the solution to \\[\n({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}}){\\mathbf{u}}={\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}\\tilde{{\\mathbf{y}}}\n\\] and is evaluated using the Cholesky factor, \\({\\mathbf{L}}\\), of \\({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}}\\).\nAs in the solution of the PLS problem in Section B.7, the fact that \\({\\mathbf{Z}}\\) is sparse and that the sparsity is also present in \\({\\mathbf{L}}\\), makes it feasible to solve for \\({\\mathbf{u}}\\) even when its dimension is large.\n\nC.4.1 PIRLS for com05\nTo illustrate the calculations we again use the com05 model, which has a single, scalar random-effects term, (1 | dist & urban), in its formula. The matrix \\({\\mathbf{Z}}\\) is displayed as\n\ncom05re = only(com05.reterms)\nInt.(collect(com05re))  # Int values for compact printing\n\n1934×102 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱  ⋮              ⋮              ⋮  \n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  1\n\n\nbut internally it is stored much more compactly because it is an indicator matrix (also called one-hot encoding), which means that all \\(Z_{i,j}\\in\\{0,1\\}\\) and in each row there is exactly one value that is one (and all the others are zero). The column in which the non-zero element of each row occurs is given as an integer vector in the refs property of com05re.\n\ncom05re.refs'      # transpose for compact printing\n\n1×1934 adjoint(::Vector{Int32}) with eltype Int32:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  102  102  102  102  102  102  102\n\n\nWe define a struct\n\nstruct BernoulliPIRLS{T&lt;:AbstractFloat,S&lt;:Integer}\n  X::Matrix{T}\n  θβ::Vector{T}\n  ytbl::NamedTuple{                     # column-table\n    (:refs, :y, :η, :offset),\n    Tuple{Vector{S},Vector{T},Vector{T},Vector{T}},\n  }\n  utbl::NamedTuple{                     # column-table\n    (:u, :u0, :Ldiag, :pdev, :pdev0, :aGHQ),\n    NTuple{6,Vector{T}},\n  }\n  rtbl::Vector{                         # row-table\n    NamedTuple{(:μ, :dev, :rtwwt, :wwres, :wwresp),NTuple{5,T}},\n  }\nend\n\nwith an external constructor\n\nfunction BernoulliPIRLS(\n  X::Matrix{T},\n  y::Vector{T},\n  refs::Vector{S},\n) where {T&lt;:AbstractFloat,S&lt;:Integer}\n  # use IRLS to check X and y, obtain initial β, and establish rtbl\n  irls = fit!(BernoulliIRLS(X, y); verbose=false)\n  β = irls.β\n  θβ = append!(ones(T, 1), β)       # initial θ = 1\n  η = X * β\n\n  # refs should contain all values from 1 to maximum(refs)\n  refvals = sort!(unique(refs))\n  q = length(refvals)\n  if refvals ≠ 1:q\n    throw(ArgumentError(\"sort!(unique(refs)) must be 1:$q\"))\n  end\n  length(refs) == length(y) ||\n    throw(ArgumentError(\"lengths of y and refs aren't equal\"))\n\n  ytbl = (; refs, y, η, offset=copy(η))\n\n  utbl = NamedTuple(\n    nm =&gt; zeros(T, q) for\n    nm in (:u, :u0, :Ldiag, :pdev, :pdev0, :aGHQ)\n  )\n  return updatetbl!(BernoulliPIRLS(X, θβ, ytbl, utbl, irls.rtbl))\nend\n\n\n\n\n\n\n\nWhy are θ and β stored in a single vector?\n\n\n\n\n\nThe reason for storing both \\({\\boldsymbol{\\theta}}\\) and \\({\\boldsymbol{\\beta}}\\) in a single vector is to provide for their simultaneous optimization with an optimizer such as those in NLopt.jl.\n\n\n\nThe utbl field in a BernoulliPIRLS struct contains vectors named u0, pdev, pdev0, and aGHQ, in addition to u and Ldiag. These are not used in the PIRLS algorithm (other than for keeping a copy of the previous \\({\\mathbf{u}}\\)) or for optimizing Laplace’s approximation to the objective. However, they will be used in adaptive Gauss-Hermite quadrature evaluation of the objective (Section C.6), so we keep them in the struct throughout.\nThe updatetbl! method for this type first evaluates \\({\\boldsymbol{\\eta}}\\) via a “virtual” multiplication that forms \\({\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}{\\mathbf{u}}\\) plus the stored offset, which is \\({\\mathbf{X}}{\\boldsymbol{\\beta}}\\), then updates the rowtable from \\({\\mathbf{y}}\\), \\({\\boldsymbol{\\eta}}\\), and the offset. For this model \\({\\boldsymbol{\\theta}}\\) is one-dimensional and \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\) is a scalar multiple of \\({\\mathbf{I}}_q\\), the identity matrix of size \\(q\\), and thus the matrix multiplication by \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}\\) can be expressed as scalar products.\n\nfunction updatetbl!(m::BernoulliPIRLS)\n  (; refs, y, η, offset) = m.ytbl\n  u = m.utbl.u\n  θ = first(m.θβ)\n  # evaluate η = offset + ZΛu where Λ is θ * I and Z is one-hot\n  fill!(η, 0)\n  @inbounds for i in eachindex(η, refs, offset)\n    η[i] += offset[i] + u[refs[i]] * θ\n  end\n  m.rtbl .= tblrow.(y, η, offset)\n  return m\nend\n\nThe pdeviance method returns the deviance for the GLM model plus the penalty on the squared length of u.\n\nfunction pdeviance(m::BernoulliPIRLS)\n  return sum(r.dev for r in m.rtbl) + sum(abs2, m.utbl.u)\nend\n\nThe updateu! method is similar to updateβ! for the BernoulliIRLS type except that it is based on the diagonal matrix \\({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+ {\\mathbf{I}}\\). Only the diagonal elements of this matrix are constructed and used to solve for the updated \\({\\mathbf{u}}\\) vector. At convergence of the PIRLS algorithm the elements of Ldiag are replaced by their square roots.\n\nfunction updateu!(m::BernoulliPIRLS)\n  (; u, u0, Ldiag) = m.utbl\n  copyto!(u0, u)              # keep a copy of u\n  θ = first(m.θβ)             # extract the scalar θ\n  fill!(u, 0)\n  if iszero(θ)                # skip the update if θ == 0\n    fill!(Ldiag, 1)           # L is the identity if θ == 0\n    return updatetbl!(m)\n  end\n  fill!(Ldiag, 0)\n  @inbounds for (ri, ti) in zip(m.ytbl.refs, m.rtbl)\n    rtWΛ = θ * ti.rtwwt       # non-zero in i'th row of √WZΛ\n    Ldiag[ri] += abs2(rtWΛ)   # accumulate Λ'Z'WZΛ\n    u[ri] += rtWΛ * ti.wwresp # accumulate Λ'Z'Wỹ\n  end\n  Ldiag .+= 1                 # form diagonal of Λ'Z'WZΛ + I = LL'\n  u ./= Ldiag                 # solve for u with diagonal LL'\n  return updatetbl!(m)        # and update η and rtbl\nend\n\nCreate a BernoulliPIRLS struct for the com05 model and check the penalized deviance at the initial values\n\nm = BernoulliPIRLS(com05.X, com05.y, only(com05.reterms).refs)\npdeviance(m)\n\n2409.3774281600413\n\n\nAs with IRLS, the first iteration of PIRLS reduces the objective, which is the penalized deviance in this case, substantially.\n\npdeviance(updateu!(m))\n\n2233.120947635784\n\n\nCreate a pirls! method for this struct.\n\nfunction pirls!(m::BernoulliPIRLS; verbose::Bool=false)\n  (; u, u0, Ldiag) = m.utbl\n  fill!(u, 0)                  # start from u == 0\n  copyto!(u0, u)               # keep a copy of u\n  oldpdev = pdeviance(updatetbl!(m))\n  verbose && @info 0, oldpdev\n  for i in 1:10                # maximum of 10 PIRLS iterations\n    newpdev = pdeviance(updateu!(m))\n    verbose && @info i, newpdev\n    if newpdev &gt; oldpdev       # PIRLS iteration failed\n      @warn \"PIRLS iteration did not reduce penalized deviance\"\n      copyto!(u, u0)           # restore previous u\n      updatetbl!(m)            # restore η and rtbl\n      break\n    elseif (oldpdev - newpdev) &lt; (1.0e-8 * oldpdev)\n      copyto!(u0, u)           # keep a copy of u\n      break\n    else\n      copyto!(u0, u)           # keep a copy of u\n      oldpdev = newpdev\n    end\n  end\n  map!(sqrt, Ldiag, Ldiag)     # replace diag(LL') by diag(L)\n  return m\nend\n\nThe PIRLS iterations always start from \\({\\mathbf{u}}=\\mathbf{0}\\) so that the converged value of the penalized deviance is reproducible for given values of \\(\\theta\\) and \\({\\boldsymbol{\\beta}}\\). If we allowed the algorithm to start at whatever values are currently stored in \\({\\mathbf{u}}\\) then there could be slight differences in the value of the penalized deviance at convergence of PIRLS, which can cause problems when trying to optimize with respect to \\(\\theta\\) and \\({\\boldsymbol{\\beta}}\\).\n\npirls!(m; verbose=true);\n\n[ Info: (0, 2409.3774281600413)\n[ Info: (1, 2233.120947635784)\n[ Info: (2, 2231.6059352797056)\n[ Info: (3, 2231.600219832158)\n[ Info: (4, 2231.6002194065604)\n\n\nAs with IRLS, PIRLS is a fast and stable algorithm for determining the mode of the conditional distribution \\(({\\mathcal{U}}|{\\mathcal{Y}}={\\mathbf{y}})\\) with \\({\\boldsymbol{\\theta}}\\) and \\({\\boldsymbol{\\beta}}\\) held fixed.\n\n@benchmark pirls!(mm) seconds = 1 setup = (mm = m)\n\n\nBenchmarkTools.Trial: 5271 samples with 1 evaluation.\n Range (min … max):  183.292 μs … 236.625 μs  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     187.958 μs               ┊ GC (median):    0.00%\n Time  (mean ± σ):   188.373 μs ±   3.113 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\n               ▄█▅                                               \n  ▂▃▅▅▄▃▃▃▃▃▃▃▅████▇▆▆▆▆▅▄▄▃▂▃▂▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▂ ▃\n  183 μs           Histogram: frequency by time          202 μs &lt;\n Memory estimate: 112 bytes, allocs estimate: 1.\n\n\n\nThe time taken for the four iterations to determine the conditional mode of \\({\\mathbf{u}}\\) is comparable to the time taken for a single call to updateβ!. Most of the time in updateβ! is spent in the QR factorization to solve the weighted least squares problem, whereas in updateu!and thus in pirls!, we take advantage of the fact that the solution of the penalized, weighted least squares problem is based on a diagonal matrix."
  },
  {
    "objectID": "aGHQ.html#sec-GLMMLaplace",
    "href": "aGHQ.html#sec-GLMMLaplace",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.5 Laplace’s approximation to the log-likelihood",
    "text": "C.5 Laplace’s approximation to the log-likelihood\nThe PIRLS algorithm determines the value of \\({\\mathbf{u}}\\) that minimizes the penalized deviance \\[\n\\tilde{{\\mathbf{u}}}=\\arg\\min_{{\\mathbf{u}}}\\left(\\left\\|{\\mathbf{u}}\\right\\|^2+\\sum_{i=1}^n d(y_i,\\eta_i)\\right) ,\n\\] where \\(\\eta_i, i=1,\\dots,n\\) is the \\(i\\)th component of \\({\\boldsymbol{\\eta}}={\\mathbf{X}}{\\boldsymbol{\\beta}}+{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}{\\mathbf{u}}\\). A quadratic approximation to the penalized deviance at \\(\\tilde{{\\mathbf{u}}}\\) is \\[\n\\begin{aligned}\n\\tilde{d}({\\mathbf{y}},{\\mathbf{u}})&=\\|{\\mathbf{u}}\\|^2+\\sum_{i=1}^n d(y_i,\\eta_i)\\\\\n&\\approx\\|\\tilde{{\\mathbf{u}}}\\|^2+\\sum_{i=1}^n d(y_i,\\tilde{\\eta}_i)+\n({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})'({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}})({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})\\\\\n&=\\|\\tilde{{\\mathbf{u}}}\\|^2+\\sum_{i=1}^n d(y_i,\\tilde{\\eta}_i)+\n({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})'{\\mathbf{L}}{\\mathbf{L}}'({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})\\\\\n&=\\tilde{d}({\\mathbf{y}},\\tilde{{\\mathbf{u}}})+\n({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})'{\\mathbf{L}}{\\mathbf{L}}'({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})\n\\end{aligned}\n\\tag{C.9}\\] where \\({\\mathbf{L}}\\) is the lower Cholesky factor of \\({\\boldsymbol{\\Lambda}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}+{\\mathbf{I}}\\). (In Equation C.9 the linear term in \\(({\\mathbf{u}}-\\tilde{{\\mathbf{u}}})\\) that would normally occur in such an expression is omitted because the gradient of \\(\\tilde{d}({\\mathbf{y}},{\\mathbf{u}})\\) is zero at \\(\\tilde{{\\mathbf{u}}}\\).)\nLaplace’s approximation to the log-likelihood uses this quadratic approximation to the penalized deviance, which is negative one-half the logarithm of the integrand, to approximate the value of the integral.\nOn the deviance scale, which is negative twice the log-likelihood, the approximation is \\[\n\\begin{aligned}\n-2\\,\\ell({\\mathbf{u}}|{\\mathbf{y}},{\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}})&=-2\\,\\log\\left(L({\\mathbf{u}}|{\\mathbf{y}},{\\boldsymbol{\\theta}},{\\boldsymbol{\\beta}})\\right)\\\\\n&=-2\\,\\log\\left(\\int_{{\\mathbf{u}}}\\frac{1}{\\sqrt{2\\pi}^q}\\exp\\left(\\frac{\\left\\|{\\mathbf{u}}\\right\\|^2+\\sum_{i=1}^n d(y_i,\\mu_i)}{-2}\\right)\\,d{\\mathbf{u}}\\right)\\\\\n&\\approx\\tilde{d}({\\mathbf{y}},\\tilde{{\\mathbf{u}}})-2\\,\\log\\left(\n\\int_{\\mathbf{u}}\\frac{1}{\\sqrt{2\\pi}^q}\\exp\\left(\\frac{[{\\mathbf{u}}-\\tilde{{\\mathbf{u}}}]'{\\mathbf{L}}{\\mathbf{L}}'[{\\mathbf{u}}-\\tilde{{\\mathbf{u}}}]}{-2}\\right)\\,d{\\mathbf{u}}\n\\right)\\\\\n&=\\tilde{d}({\\mathbf{y}},\\tilde{{\\mathbf{u}}})-2\\,\\log\\left(\n\\int_{\\mathbf{u}}\\frac{1}{\\sqrt{2\\pi}^q}\\exp\\left(\\frac{\\left\\|{\\mathbf{L}}'[{\\mathbf{u}}-\\tilde{{\\mathbf{u}}}\\right\\|^2}{-2}\\right)\\,d{\\mathbf{u}}\n\\right)\\\\\n&=\\tilde{d}({\\mathbf{y}},\\tilde{{\\mathbf{u}}})-2\\,\\log\\left(|{\\mathbf{L}}|^{-1}\\right)\\\\\n&=\\tilde{d}({\\mathbf{y}},\\tilde{{\\mathbf{u}}})+\\log\\left(|{\\mathbf{L}}|^2\\right)\n\\end{aligned}\n\\]\n\nfunction laplaceapprox(m::BernoulliPIRLS)\n  return pdeviance(m) + 2 * sum(log, m.utbl.Ldiag)\nend\n\n\nlaplaceapprox(pirls!(m))\n\n2373.5180527521634\n\n\nThe remaining step is to optimize Laplace’s approximation to the GLMM deviance with respect to \\(\\theta\\) and \\({\\boldsymbol{\\beta}}\\), which we do using the BOBYQA optimizer from NLopt.jl\n\nfunction StatsAPI.fit!(m::BernoulliPIRLS)\n  θβ = m.θβ\n  pp1 = length(θβ)                # length(β) = p and length(θ) = 1\n  opt = Opt(:LN_BOBYQA, pp1)\n  mβ = view(θβ, 2:pp1)\n  function obj(x, g)\n    if !isempty(g)\n      throw(ArgumentError(\"gradient not provided, g must be empty\"))\n    end\n    copyto!(θβ, x)\n    mul!(m.ytbl.offset, m.X, mβ)\n    return laplaceapprox(pirls!(m))\n  end\n  opt.min_objective = obj\n  lb = fill!(similar(θβ), -Inf)   # vector of lower bounds\n  lb[1] = 0                       # scalar θ must be non-negative\n  NLopt.lower_bounds!(opt, lb)\n  minf, minx, ret = optimize(opt, copy(θβ))\n  @info (; ret, fevals=opt.numevals, minf)\n  return m\nend\n\n\nfit!(m);\n\n[ Info: (ret = :ROUNDOFF_LIMITED, fevals = 567, minf = 2354.474481568814)\n\n\n\n\nCode\nprintln(\"Converged to θ = \", first(m.θβ), \" and β =\")\nshowcompact(view(m.θβ, 2:lastindex(m.θβ)))\n\n\nConverged to θ = 0.5683043987329829 and β =\n[-0.340978, 0.39338, 0.606486, -0.0129262, 0.0332348, -0.00562619]\n\n\nThese estimates differ somewhat from those for model com05.\n\n\nCode\nprintln(\n  \"Estimates for com05: θ = \",\n  only(com05.θ),\n  \", fmin = \",\n  deviance(com05),\n  \", and β =\",\n)\nshowcompact(com05.β)\n\n\nEstimates for com05: θ = 0.576131809145932, fmin = 2353.8241975678648, and β =\n[-0.341475, 0.393592, 0.606449, -0.0129098, 0.0332106, -0.00562463]\n\n\nThe discrepancy in the results is because the com05 results are based on a more accurate approximation to the integral called adaptive Gauss-Hermite Quadrature, which is discussed in Section C.6.\n\nC.5.1 Generalizations to more complex structure\nThere is an implicit assumption in the BernoulliPIRLS structure that random effects in the model are simple, scalar random effects associated with a single grouping factor, which is represented by m.ytbl.refs. For such models the random effects model matrix, \\({\\mathbf{Z}}\\), is an \\(n\\times q\\) indicator matrix, the covariance parameter, \\({\\boldsymbol{\\theta}}\\), is one-dimensional and the covariance factor, \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}=\\theta\\,{\\mathbf{I}}_q\\) is a scalar multiple of the \\(q\\times q\\) identity matrix, \\({\\mathbf{I}}_q\\). Furthermore, \\({\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}'{\\mathbf{Z}}'{\\mathbf{W}}{\\mathbf{Z}}{\\boldsymbol{\\Lambda}}_{{\\boldsymbol{\\theta}}}+{\\mathbf{I}}_q\\) is also diagonal, as is its Cholesky factor, \\({\\mathbf{L}}\\).\nWe have taken advantage of the special structure of these matrices both in representations — storing \\({\\mathbf{L}}\\) by storing only the diagonal values in the vector m.utbl.Ldiag — and in some algorithms for the PIRLS iterative step.\nThe PIRLS algorithm to determine the conditional mode, \\(\\tilde{{\\mathbf{u}}}\\), of the random effects, \\({\\mathcal{U}}\\), and Laplace’s approximation to the log-likelihood for GLMMs can be generalized to models with vector-valued random effects, or with random effects associated with more than one grouping factor, or with both. The more general representation of \\({\\mathbf{Z}}\\) and \\({\\mathbf{L}}\\) used with linear mixed models can be adapted for GLMMs as well.\nWe chose to specialize the representation of GLMMs in this appendix to this specific type of random effects to be able to demonstrate adaptive Gauss-Hermite quadrature in Section C.6, which, at present, is restricted to models with a single, simple, scalar, random effects term."
  },
  {
    "objectID": "aGHQ.html#sec-aGHQ",
    "href": "aGHQ.html#sec-aGHQ",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.6 Adaptive Gauss-Hermite quadrature",
    "text": "C.6 Adaptive Gauss-Hermite quadrature\nRecall from Equation C.9 that Laplace’s approximation to the likelihood is based on a quadratic approximation to the penalized (GLM) deviance, \\(\\tilde{d}({\\mathbf{y}},{\\mathbf{u}})\\), at the conditional mode \\(\\tilde{{\\mathbf{u}}}\\). In the case of a model with a single, scalar, random effects term, like the model com05, each linear predictor value, \\(\\eta_i,\\,i=1,\\dots,n\\) depends on only one element of \\({\\mathbf{u}}\\). Writing the set of indices \\(i\\) for which refs[i] == j as \\({\\mathcal{I}}(j)\\), we can express the penalized deviance, and its quadratic approximation, as sums of scalar contributions, \\[\n\\begin{aligned}\n\\tilde{d}({\\mathbf{y}},{\\mathbf{u}})&=\\|{\\mathbf{u}}\\|^2+\\sum_{i=1}^n d(y_i,\\mu_i)\\\\\n&=\\sum_{j=1}^q\\left(u_j^2+\\sum_{i\\in{\\mathcal{I}}(j)}d(y_i,\\mu_i)\\right)\\\\\n&\\approx\\sum_{j=1}^q \\left(\\tilde{u}_j^2+\\sum_{i\\in{\\mathcal{I}}(j)}\\left(d(y_i,\\tilde{\\mu}_i)+\\ell_j^2(u_j-\\tilde{u}_j)^2\\right)\\right)\n\\end{aligned}\n\\] where \\(\\ell_j\\) is the \\(j\\)th diagonal element of \\({\\mathbf{L}}\\) and \\(\\tilde{\\mu}_i\\) is the value of \\(\\mu_i\\) when \\(u_j=\\tilde{u}_j\\).\nExtending the notation of Equation C.9 we write the contribution from \\(u_j\\) to the penalized (GLM) deviance, and to its quadratic approximation, as \\[\n\\begin{aligned}\n\\tilde{d}_j(u_j)&= u_j+\\sum_{i\\in{\\mathcal{I}}(j)}d(y_i,\\mu_i)\\\\\n&\\approx \\tilde{u_j} + \\sum_{i\\in{\\mathcal{I}}(j)}\\left(d(y_i,\\tilde{\\mu}_i)+\\ell_j^2(u_j-\\tilde{u}_j)\\right)\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)+\\ell_j^2(u_j-\\tilde{u}_j)\\quad j=1,\\dots,q ,\n\\end{aligned}\n\\] giving Laplace’s approximation to the scalar integral defining the contribution of \\(u_j\\) to negative twice the log-likelihood as \\[\n\\begin{aligned}\n-2\\log\\int_{u_j}\\frac{e^{-\\tilde{d}_j(u_j)/2}}{\\sqrt{2\\pi}}\\,du_j&\\approx\n-2\\log\\int_{u_j}\\frac{e^{\\left((-\\tilde{d}_j(\\tilde{u_j})-\\ell_j^2(u_j-\\tilde{u}_j)^2)/2\\right)}}{\\sqrt{2\\pi}}\\,du_j\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)-2\\log\\int_{u_j}\\frac{e^{-\\ell_j^2(u_j-\\tilde{u}_j)/2}}{\\sqrt{2\\pi}}\\,du_j\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)-2\\log\\int_{z_j}\\frac{e^{-z_j^2/2}}{\\sqrt{2\\pi}}\\,\\frac{dz_j}{\\ell_j}\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)+2\\log(\\ell_j)-2\\log\\int_{z_j}\\frac{e^{-z_j^2/2}}{\\sqrt{2\\pi}}\\,dz_j\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)+2\\log(\\ell_j)-2\\log(1)\\\\\n&=\\tilde{d}_j(\\tilde{u}_j)+2\\log(\\ell_j)\n\\end{aligned}\n\\tag{C.10}\\] using the change of variable \\[\nz_j=\\ell_j(u_j-\\tilde{u}_j)\\quad j=1,\\dots,q\n\\tag{C.11}\\] with inverse \\[\nu_j=\\frac{z_j}{\\ell_j}+\\tilde{u}_j\\quad j=1,\\dots,q\n\\tag{C.12}\\] and derivative \\[\n\\frac{du_j}{dz_j}=\\frac{1}{\\ell_j} .\n\\]\nThe change of variable Equation C.11 allows us to express the contribution from \\(u_j\\) to Laplace’s approximation as a constant, \\(\\tilde{d}_j(\\tilde{u}_j)\\), plus the integral of a multiple, \\(1/\\ell_j\\), of the density of a standard normal distribution \\[\n\\phi(z)=\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}} .\n\\]\nThe \\(K\\)th-order normalized Gauss-Hermite quadrature rule allows us to extend this approach to evaluate integrals of the form \\[\n\\int_z f(z)\\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}\\, dz \\approx \\sum_{k=1}^K w_k f(z_k)\n\\tag{C.13}\\] where the weights, \\(w_k,\\,k=1,\\dots,K\\), and the absiccae, \\(z_k,\\,k=1,\\dots,K\\) are evaluated as described in Section C.6.1. The approximation Equation C.13 is exact when \\(f(z)\\) is a polynomial of order \\(2K-1\\) or less.\nWe will apply a rule like Equation C.13 where \\(f\\) is the exponential of negative half the difference between the penalized deviance, \\(\\tilde{d}_j(z_j/\\ell_j+\\tilde{u}_j)\\), and its quadratic approximation at the conditional mode, \\(\\tilde{u}_j\\). That is, we will center the standard normal density at the conditional mode, \\(\\tilde{u}_j,\\,j=1,\\dots,q\\), and scale it by the inverse of the quadratic term, \\(\\ell_j,\\,j=1,\\dots,q\\), in the quadratic approximation at that value of \\(u\\). This is said to be an adaptive quadrature rule because we are shifting and scaling the evaluation points according to the current conditions in the iterative algorithm.\nIn other words we will first apply PIRLS to determine the conditional modes and the quadratic terms, then use a normalized Gauss-Hermite quadrature rule.\n\nC.6.1 Normalized Gauss-Hermite quadrature rules\nGaussian Quadrature rules provide sets of x values, called abscissae, and corresponding weights, w, to approximate an integral with respect to a weight function, \\(g(x)\\). For a \\(K\\)th order rule the approximation is \\[\n\\int f(x)g(x)\\,dx \\approx \\sum_{k=1}^K w_k f(x_k)\n\\]\nFor the Gauss-Hermite rule the weight function is \\[\ng(x) = e^{-x^2}\n\\] and the domain of integration is \\((-\\infty, \\infty)\\). A slight variation of this is the normalized Gauss-Hermite rule for which the weight function is the standard normal density \\[\ng(z) = \\phi(z) = \\frac{e^{-z^2/2}}{\\sqrt{2\\pi}} .\n\\]\nThus, the expected value of \\(f(z)\\), where \\(\\mathcal{Z}\\sim\\mathscr{N}(0,1)\\), is approximated as \\[\n\\mathbb{E}[f]=\\int_{-\\infty}^{\\infty} f(z) \\phi(z)\\,dz\\approx\\sum_{k=1}^K w_k\\,f(z_k) .\n\\]\nNaturally, there is a caveat. For the approximation to be accurate the function \\(f(z)\\) must behave like a low-order polynomial over the range of interest. More formally, a \\(K\\)th order rule is exact when \\(f\\) is a polynomial of order \\(2K-1\\) or less.\n\nC.6.1.1 Evaluating the weights and abscissae\nIn the Golub-Welsch algorithm the abscissae for a particular Gaussian quadrature rule are determined as the eigenvalues of a symmetric tri-diagonal matrix and the weights are derived from the squares of the first row of the matrix of eigenvectors. For a \\(K\\)th order normalized Gauss-Hermite rule the tridiagonal matrix has zeros on the diagonal and the square roots of 1:k-1 on the super- and sub-diagonal, e.g.\n\nsym5 = SymTridiagonal(zeros(5), sqrt.(1:4))\n\n5×5 SymTridiagonal{Float64, Vector{Float64}}:\n 0.0  1.0       ⋅        ⋅        ⋅ \n 1.0  0.0      1.41421   ⋅        ⋅ \n  ⋅   1.41421  0.0      1.73205   ⋅ \n  ⋅    ⋅       1.73205  0.0      2.0\n  ⋅    ⋅        ⋅       2.0      0.0\n\n\n\nev = eigen(sym5);\nshowcompact(ev.values)\n\n[-2.85697, -1.35563, 8.88178e-16, 1.35563, 2.85697]\n\n\n\nshowcompact(abs2.(ev.vectors[1, :]))\n\n[0.0112574, 0.222076, 0.533333, 0.222076, 0.0112574]\n\n\nA function of k to evaluate the abscissae and weights is\n\nfunction gausshermitenorm(k)\n  ev = eigen(SymTridiagonal(zeros(k), sqrt.(1:(k - 1))))\n  return Table((;\n    abscissae=ev.values,\n    weights=abs2.(ev.vectors[1, :]),\n  ))\nend\n\nproviding\n\ngausshermitenorm(5)\n\nTable with 2 columns and 5 rows:\n     abscissae    weights\n   ┌───────────────────────\n 1 │ -2.85697     0.0112574\n 2 │ -1.35563     0.222076\n 3 │ 8.88178e-16  0.533333\n 4 │ 1.35563      0.222076\n 5 │ 2.85697      0.0112574\n\n\nThe weights and positions for the 9th order rule are shown in Figure C.1.\n\n\nCode\ndraw(\n  data(gausshermitenorm(9)) *\n  mapping(:abscissae =&gt; \"Positions\", :weights),\n)\n\n\n\n\n\nFigure C.1: Weights and positions for the 9th order normalized Gauss-Hermite quadrature rule\n\n\n\n\nNotice that the magnitudes of the weights drop quite dramatically away from zero, even on a logarithmic scale (Figure C.2)\n\n\nCode\ndraw(\n  data(gausshermitenorm(9)) * mapping(\n    :abscissae =&gt; \"Positions\",\n    :weights =&gt; log2 =&gt; \"log₂(weight)\",\n  ),\n)\n\n\n\n\n\nFigure C.2: Weights (logarithm base 2) and positions for the 9th order normalized Gauss-Hermite quadrature rule\n\n\n\n\nThe definition of MixedModels.GHnorm is similar to the gausshermitenorm function with some extra provisions for ensuring symmetry of the abscissae and of the weights and for caching values once they have been calculated.\n\nlet tbl = GHnorm(9)\n  Table(abscissae=tbl.z, weights=tbl.w)\nend\n\nTable with 2 columns and 9 rows:\n     abscissae  weights\n   ┌──────────────────────\n 1 │ -4.51275   2.23458e-5\n 2 │ -3.20543   0.00278914\n 3 │ -2.07685   0.0499164\n 4 │ -1.02326   0.244098\n 5 │ 0.0        0.406349\n 6 │ 1.02326    0.244098\n 7 │ 2.07685    0.0499164\n 8 │ 3.20543    0.00278914\n 9 │ 4.51275    2.23458e-5\n\n\nIn particular, when \\(K\\) is odd the middle abscissa, at index \\((K+1)/2\\), is exactly zero.\nAs an example of evaluation using these weights and abscissae, we consider \\[\n\\mathbb{E}[g(x)] \\approx \\sum_{i=1}^k g(\\mu + \\sigma z_i)\\,w_i\n\\] where \\(\\mathcal{X}\\sim\\mathscr{N}(\\mu, \\sigma^2)\\).\nFor example, \\(\\mathbb{E}[\\mathcal{X}^2]\\) where \\(\\mathcal{X}\\sim\\mathcal{N}(2, 3^2)\\) is\n\nlet μ = 2, σ = 3, ghn3 = GHnorm(3)\n  sum(@. ghn3.w * abs2(μ + σ * ghn3.z))  # should be μ² + σ² = 13\nend\n\n13.0\n\n\n(In general a dot, ‘.’, after the function name in a function call, as in abs2.(...), or before an operator creates a fused vectorized evaluation in Julia. The macro @. has the effect of vectorizing all operations in the subsequent expression.)\n\n\n\nC.6.2 Illustration of contributions to the objective\nWe have provided a pdev vector in the utbl field of a BernoulliPIRLS object to allow for accumulation of the penalized deviance contributions for each component of \\({\\mathbf{u}}\\) in the model.\n\nfunction pdevcomps!(m::BernoulliPIRLS)\n  (; u, pdev) = m.utbl\n  pdev .= abs2.(u)        # initialize pdevj to square of uj\n  @inbounds for (ri, ti) in zip(m.ytbl.refs, m.rtbl)\n    pdev[ri] += ti.dev\n  end\n  return m\nend\n\nAfter PIRLS has converged, we evaluate pdevcomps! and copy the pdev column of the utbl field to its pdev0 column, which will be the baseline evaluation of the penalized deviance at the conditional modes. Other evaluations of the penalized deviance components are plotted as differences from pdev0.\n\npdevcomps!(pirls!(m))\ncopyto!(m.utbl.pdev0, m.utbl.pdev)\nTable(m.utbl)\n\nTable with 6 columns and 102 rows:\n      u           u0          Ldiag    pdev     pdev0    aGHQ\n    ┌────────────────────────────────────────────────────────\n 1  │ -1.02425    -1.02425    2.3596   78.2322  78.2322  0.0\n 2  │ -1.6554     -1.6554     1.87894  41.917   41.917   0.0\n 3  │ -0.0432084  -0.0432084  1.54253  21.8681  21.8681  0.0\n 4  │ 0.500796    0.500796    1.07154  2.68642  2.68642  0.0\n 5  │ 1.10928     1.10928     1.29471  8.58305  8.58305  0.0\n 6  │ -0.415844   -0.415844   1.49343  18.6901  18.6901  0.0\n 7  │ 0.0305151   0.0305151   1.06624  1.46897  1.46897  0.0\n 8  │ 0.216409    0.216409    1.87125  46.1746  46.1746  0.0\n 9  │ 0.433361    0.433361    1.2297   9.7717   9.7717   0.0\n 10 │ -0.648279   -0.648279   2.10967  58.417   58.417   0.0\n 11 │ -0.328134   -0.328134   1.47521  23.5301  23.5301  0.0\n 12 │ 0.499976    0.499976    1.06882  2.74129  2.74129  0.0\n 13 │ 0.139717    0.139717    1.82727  43.6776  43.6776  0.0\n 14 │ 0.176548    0.176548    1.10663  2.77502  2.77502  0.0\n 15 │ -0.471723   -0.471723   1.51184  21.2797  21.2797  0.0\n 16 │ -0.757145   -0.757145   1.25035  7.09335  7.09335  0.0\n 17 │ -1.59799    -1.59799    1.32299  8.74912  8.74912  0.0\n 18 │ -0.2873     -0.2873     1.19599  8.65057  8.65057  0.0\n 19 │ -0.095961   -0.095961   1.61545  34.1614  34.1614  0.0\n 20 │ -0.227454   -0.227454   1.26232  8.98806  8.98806  0.0\n 21 │ 0.447541    0.447541    1.44906  17.716   17.716   0.0\n 22 │ 1.22259     1.22259     2.80768  128.155  128.155  0.0\n 23 │ 0.123348    0.123348    1.46632  23.0924  23.0924  0.0\n ⋮  │     ⋮           ⋮          ⋮        ⋮        ⋮      ⋮\n\n\nConsider the change in the penalized deviance from that at the conditional mode for a selection of groups, which, by default, we choose to be the first 5 groups.\n\n\nCode\nfunction pdevdiff(\n  m::BernoulliPIRLS{T};\n  zvals=collect(-3.5:inv(32):3.5),\n  inds=1:5,\n) where {T}\n  (; u, u0, Ldiag, pdev, pdev0) = m.utbl\n  pdevcomps!(pirls!(m))             # assign u0\n  copyto!(pdev0, pdev)              # and pdev0\n  ni = length(inds)\n  nz = length(zvals)\n  uvals = Array{T}(undef, ni, nz)\n  exact = Array{T}(undef, ni, nz)\n  for (j, z) in enumerate(zvals)\n    u .= u0 .+ z ./ Ldiag           # evaluate u from z\n    pdevcomps!(updatetbl!(m))       # evaluate pdev\n    for (i, ind) in enumerate(inds) # store selected u and pdev\n      uvals[i, j] = u[ind]\n      exact[i, j] = pdev[ind] - pdev0[ind]\n    end\n  end\n  uvals = collect(uvals')           # transpose uvals\n  exact = collect(exact')           # and exact\n  return (; zvals, inds, uvals, exact)\nend\nm05pdevdiff = pdevdiff(m);\n\n\nWe begin with plots of the difference in the penalized deviance from its value at the conditional mode, \\(\\tilde{d}_j(u_j)-\\tilde{d_j}(\\tilde{u}_j)\\), for \\(j=1,\\dots,5\\) in Figure C.3\n\n\nCode\nlet (; zvals, inds, uvals, exact) = m05pdevdiff,\n  fig = Figure(; resolution=(800, 450)),\n  ax = Axis(\n    fig[1, 1];\n    xlabel=\"u\",\n    ylabel=\"Change in penalized deviance\",\n  )\n\n  lins = [\n    lines!(ax, view(uvals, :, j), view(exact, :, j)) for\n    j in axes(uvals, 2)\n  ]\n  Legend(fig[1, 2], lins, string.(inds))\n  fig\nend\n\n\n\n\n\nFigure C.3: Change in the penalized deviance contribution from that at the conditional mode, for each of the first 5 groups, in model com05, as a function of u.\n\n\n\n\nthen shift and scale the horizontal axis to the \\(z\\) scale for this difference in the penalized deviance (from that at the conditional mode) in Figure C.4.\n\n\nCode\nlet (; zvals, inds, uvals, exact) = m05pdevdiff,\n  fig = Figure(; resolution=(800, 450)),\n  ax = Axis(\n    fig[1, 1];\n    xlabel=\"z\",\n    ylabel=\"Change in penalized deviance\",\n  )\n\n  lins =\n    [lines!(ax, zvals, view(exact, :, j)) for j in axes(uvals, 2)]\n  Legend(fig[1, 2], lins, string.(inds))\n  fig\nend\n\n\n\n\n\nFigure C.4: Change in the penalized deviance contribution from that at the conditional mode, for each of the first 5 groups, in model com05, as a function of z.\n\n\n\n\nThe next stage is to plot, on the \\(z\\) scale, the difference between the penalized deviance and its quadratic approximation at \\(z=0\\) in Figure C.5\n\n\nCode\nlet (; zvals, inds, uvals, exact) = m05pdevdiff,\n  fig = Figure(; resolution=(800, 450)),\n  ax = Axis(\n    fig[1, 1];\n    xlabel=\"z\",\n    ylabel=\"Penalized deviance minus quadratic approx\",\n  )\n\n  lins = [\n    lines!(ax, zvals, view(exact, :, j) .- abs2.(zvals)) for\n    j in axes(uvals, 2)\n  ]\n  Legend(fig[1, 2], lins, string.(inds))\n  fig\nend\n\n\n\n\n\nFigure C.5: The difference between the contribution to the penalized deviance and its quadratic approximation for each of first 5 groups in model com05 as a function of z.\n\n\n\n\nand, finally, the exponential of negative half of this difference in Figure C.6\n\n\nCode\nlet (; zvals, inds, uvals, exact) = m05pdevdiff,\n  fig = Figure(; resolution=(800, 450)),\n  ax = Axis(\n    fig[1, 1];\n    xlabel=\"z\",\n    ylabel=\"Exp of half the quadratic minus penalized deviance\",\n  )\n\n  lins = [\n    lines!(\n      ax,\n      zvals,\n      exp.(0.5 .* (abs2.(zvals) .- view(exact, :, j))),\n    ) for j in axes(uvals, 2)\n  ]\n  Legend(fig[1, 2], lins, string.(inds))\n  fig\nend\n\n\n\n\n\nFigure C.6: Exponential of half the difference between the quadratic approximation and the contribution to the penalized deviance, for each of first 5 groups in model com05 as a function of z.\n\n\n\n\nWriting the function shown in Figure C.6 — the exponential of negative half the difference between the penalized deviance and its quadratic approximation — as \\[\nf_j(z_j)=\\exp{\\left(\\frac{\\tilde{d}_j(z_j/\\ell_j+\\tilde{u}_j)-\\tilde{d}_j(\\tilde{u}_j)-z_j^2}{-2}\\right)}\n\\quad j=1,\\dots,q\n\\] we can modify the scalar version of Laplace’s approximation, Equation C.10, as \\[\n\\begin{multline*}\n-2\\log\\int_{u_j}\\frac{e^{-\\tilde{d}_j(u_j)/2}}{\\sqrt{2\\pi}}\\,du_j\\\\\n=-2\\log\\int_{z_j}\\frac{f_j(z_j)\\,e^{-(\\tilde{d}_j(\\tilde{u}_j)+z_j^2)/2}}{\\sqrt{2\\pi}}\\,\\frac{dz_j}{\\ell_j}\\\\\n=\\tilde{d}_j(\\tilde{u}_j)+2\\log(\\ell_j)-2\\log\\int_{z_j}f_j(z_j)\\frac{e^{-z_j^2/2}}{\\sqrt{2\\pi}}\\,dz_j\n\\end{multline*}\n\\tag{C.14}\\]\nA \\(K\\)th-order adaptive Gauss-Hermite quadrature approximation to the objective, negative twice the log-likelihood, for the GLMM model is Laplace’s approximation minus twice the logarithm of the \\(K\\)th order normalized Gauss-Hermite quadrature rule applied to \\(f_j(z_j)\\). We use the aGHQ column of m.utbl to accumulate these contributions and to take the logarithm then multiply the result by -2.\n\n\nCode\nfunction evalGHQ!(m::BernoulliPIRLS; nGHQ::Integer=9)\n  (; ytbl, utbl, rtbl) = m\n  (; u, u0, Ldiag, pdev, pdev0, aGHQ) = utbl\n  ghqtbl = GHnorm(nGHQ)\n  pdevcomps!(pirls!(m))  # ensure that u0 and pdev0 are current\n  copyto!(pdev0, pdev)\n  fill!(aGHQ, 0)\n  for (z, w) in zip(ghqtbl.z, ghqtbl.w)\n    if iszero(z)         # exp term is one when z == 0\n      aGHQ .+= w\n    else\n      u .= u0 .+ z ./ Ldiag\n      pdevcomps!(updatetbl!(m))\n      aGHQ .+= w .* exp.((abs2(z) .+ pdev0 .- pdev) ./ 2)\n    end\n  end\n  map!(log, aGHQ, aGHQ)  # log.(aGHQ) in place\n  aGHQ .*= -2\n  return m\nend\n\n\n\nevalGHQ!(m)\nTable(m.utbl)\n\nTable with 6 columns and 102 rows:\n      u         u0          Ldiag    pdev     pdev0    aGHQ\n    ┌─────────────────────────────────────────────────────────────\n 1  │ 0.888261  -1.02425    2.3596   98.9905  78.2322  -0.00503286\n 2  │ 0.746346  -1.6554     1.87894  65.9282  41.917   -0.00602822\n 3  │ 2.88235   -0.0432084  1.54253  42.7156  21.8681  -0.0077699\n 4  │ 4.71226   0.500796    1.07154  22.5154  2.68642  -0.00332321\n 5  │ 4.5948    1.10928     1.29471  26.5466  8.58305  -0.00560788\n 6  │ 2.60588   -0.415844   1.49343  40.3098  18.6901  -0.00726535\n 7  │ 4.26289   0.0305151   1.06624  21.5982  1.46897  -0.00232427\n 8  │ 2.62803   0.216409    1.87125  67.5008  46.1746  -0.00639679\n 9  │ 4.10316   0.433361    1.2297   28.6561  9.7717   -0.00680155\n 10 │ 1.4908    -0.648279   2.10967  80.9664  58.417   -0.00583025\n 11 │ 2.73092   -0.328134   1.47521  44.9692  23.5301  -0.00736271\n 12 │ 4.72216   0.499976    1.06882  22.6241  2.74129  -0.00283963\n 13 │ 2.60939   0.139717    1.82727  64.9152  43.6776  -0.00681333\n 14 │ 4.25447   0.176548    1.10663  22.371   2.77502  -0.00457801\n 15 │ 2.51322   -0.471723   1.51184  43.0725  21.2797  -0.00742932\n 16 │ 2.85204   -0.757145   1.25035  29.6883  7.09335  -0.00309483\n 17 │ 1.81302   -1.59799    1.32299  32.9677  8.74912  -0.00213836\n 18 │ 3.48593   -0.2873     1.19599  28.7773  8.65057  -0.00590273\n 19 │ 2.69754   -0.095961   1.61545  55.085   34.1614  -0.00779244\n 20 │ 3.34752   -0.227454   1.26232  29.0427  8.98806  -0.0072417\n 21 │ 3.56181   0.447541    1.44906  37.9855  17.716   -0.00763473\n 22 │ 2.82988   1.22259     2.80768  146.8    128.155  -0.00360118\n 23 │ 3.20095   0.123348    1.46632  44.1065  23.0924  -0.00745216\n ⋮  │    ⋮          ⋮          ⋮        ⋮        ⋮          ⋮\n\n\n\nextrema(m.utbl.aGHQ)\n\n(-0.008514109733813818, -0.000413268627218805)\n\n\nAs we see, these “correction terms” relative to Laplace’s approximation are relatively small, compared to the contributions to the objective from each component of \\({\\mathbf{u}}\\). Also, the corrections are all negative, in this case. Close examination of the individual curves in Figure C.5 shows that these curves, which are \\(-2\\log(f_j(z))\\), are more-or-less odd functions, in the sense that the value at \\(-z\\) is approximately the negative of the value at \\(z\\). If we were integrating \\(\\log(f_j(z_j))\\phi(z_j)\\) with a normalized Gauss-Hermite rule the negative and positive values would cancel out, for the most part, and some of the integrals would be positive while others would be negative.\nWhen we consider \\(f_j(z_j)\\), shown in Figure C.6, the exponential function converts from differences to ratios and stretches positive differences more than negative differences, resulting in values slightly greater than 1 for \\(\\int_z f_j(z)\\phi(z) dz\\) and, after taking negative twice the logarithm, correction terms that are slightly less than zero."
  },
  {
    "objectID": "aGHQ.html#optimization-of-the-aghq-objective",
    "href": "aGHQ.html#optimization-of-the-aghq-objective",
    "title": "Appendix C — GLMM log-likelihood",
    "section": "C.7 Optimization of the aGHQ objective",
    "text": "C.7 Optimization of the aGHQ objective\n\nfunction fitGHQ!(m::BernoulliPIRLS; nGHQ::Integer=9)\n  (; Ldiag, pdev0, aGHQ) = m.utbl\n  θβ = m.θβ\n  pp1 = length(θβ)                # length(β) = p and length(θ) = 1\n  opt = Opt(:LN_BOBYQA, pp1)\n  mβ = view(θβ, 2:pp1)\n  function obj(x, g)\n    if !isempty(g)\n      throw(ArgumentError(\"gradient not provided, g must be empty\"))\n    end\n    copyto!(θβ, x)\n    mul!(m.ytbl.offset, m.X, mβ)\n    evalGHQ!(m; nGHQ)\n    return sum(pdev0) + sum(aGHQ) + 2 * sum(log, Ldiag)\n  end\n  opt.min_objective = obj\n  lb = fill!(similar(θβ), -Inf)   # vector of lower bounds\n  lb[1] = 0                       # scalar θ must be non-negative\n  NLopt.lower_bounds!(opt, lb)\n  minf, minx, ret = optimize(opt, copy(θβ))\n  @info (; ret, fevals=opt.numevals, minf)\n  return m\nend\n\n\nfitGHQ!(m)\nshowcompact(m.θβ)\n\n┌ Warning: PIRLS iteration did not reduce penalized deviance\n└ @ Main In[40]:11\n┌ Warning: PIRLS iteration did not reduce penalized deviance\n└ @ Main In[40]:11\n\n\n[0.576132, -0.341466, 0.393599, 0.606445, -0.0129097, 0.0332099, -0.00562461]\n\n\n[ Info: (ret = :ROUNDOFF_LIMITED, fevals = 502, minf = 2353.8241975532205)\n\n\n\n\n\n\nPowell, M. J. (2009). The BOBYQA algorithm for bound constrained optimization without derivatives. Cambridge NA Report NA2009/06, University of Cambridge, Cambridge, 26.\n\n\nTierney, L., & Kadane, J. B. (1986). Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393), 82–86. https://doi.org/10.1080/01621459.1986.10478240"
  }
]