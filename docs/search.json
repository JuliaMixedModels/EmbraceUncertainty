[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Embrace Uncertainty",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "",
    "text": "\\[\n\\newcommand\\bbA{{\\mathbf{A}}}\n\\newcommand\\bbb{{\\mathbf{b}}}\n\\newcommand\\bbI{{\\mathbf{I}}}\n\\newcommand\\bbR{{\\mathbf{R}}}\n\\newcommand\\bbX{{\\mathbf{X}}}\n\\newcommand\\bbx{{\\mathbf{x}}}\n\\newcommand\\bby{{\\mathbf{y}}}\n\\newcommand\\bbZ{{\\mathbf{Z}}}\n\\newcommand\\bbbeta{{\\boldsymbol{\\beta}}}\n\\newcommand\\bbeta{{\\boldsymbol{\\eta}}}\n\\newcommand\\bbLambda{{\\boldsymbol{\\Lambda}}}\n\\newcommand\\bbOmega{{\\boldsymbol{\\Omega}}}\n\\newcommand\\bbmu{{\\boldsymbol{\\mu}}}\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\newcommand\\bbtheta{{\\boldsymbol{\\theta}}}\n\\newcommand\\mcN{{\\mathcal{N}}}\n\\newcommand\\mcB{{\\mathcal{B}}}\n\\newcommand\\mcY{{\\mathcal{Y}}}\n\\]\nIn this book we describe the theory behind a type of statistical model called mixed-effects models and the practice of fitting and analyzing such models using the MixedModels package for Julia. These models are used in many different disciplines. Because the descriptions of the models can vary markedly between disciplines, we begin by describing what mixed-effects models are and by exploring a very simple example of one type of mixed model, the linear mixed model.\nThis simple example allows us to illustrate the use of the LinearMixedModel type in the MixedModels package for fitting such models and for analyzing the fitted model. We also describe methods of assessing the precision of the parameter estimates and of visualizing the conditional distribution of the random effects, given the observed data."
  },
  {
    "objectID": "intro.html#sec-memod",
    "href": "intro.html#sec-memod",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.1 Mixed-effects models",
    "text": "1.1 Mixed-effects models\nMixed-effects models, like many other types of statistical models, describe a relationship between a response variable and some of the covariates that have been measured or observed along with the response. In mixed-effects models at least one of the covariates is a categorical covariate representing experimental or observational “units” in the data set. In the example from the chemical industry that is given in this chapter, the observational unit is the batch of an intermediate product used in production of a dye. In medical and social sciences the observational units are often the human or animal subjects in the study. In agriculture the experimental units may be the plots of land or the specific plants being studied.\nIn all of these cases the categorical covariate or covariates are observed at a set of discrete levels. We may use numbers, such as subject identifiers, to designate the particular levels that we observed but these numbers are simply labels. The important characteristic of a categorical covariate is that, at each observed value of the response, the covariate takes on the value of one of a set of distinct levels.\nParameters associated with the particular levels of a covariate are sometimes called the “effects” of the levels. If the set of possible levels of the covariate is fixed and reproducible we model the covariate using fixed-effects parameters. If the levels that we observed represent a random sample from the set of all possible levels we incorporate random effects in the model.\nThere are two things to notice about this distinction between fixed-effects parameters and random effects. First, the names are misleading because the distinction between fixed and random is more a property of the levels of the categorical covariate than a property of the effects associated with them. Second, we distinguish between “fixed-effects parameters”, which are indeed parameters in the statistical model, and “random effects”, which, strictly speaking, are not parameters. As we will see shortly, random effects are unobserved random variables.\nTo make the distinction more concrete, suppose that we wish to model the annual reading test scores for students in a school district and that the covariates recorded with the score include a student identifier and the student’s gender. Both of these are categorical covariates. The levels of the gender covariate, male and female, are fixed. If we consider data from another school district or we incorporate scores from earlier tests, we will not change those levels. On the other hand, the students whose scores we observed would generally be regarded as a sample from the set of all possible students whom we could have observed. Adding more data, either from more school districts or from results on previous or subsequent tests, will increase the number of distinct levels of the student identifier.\nMixed-effects models or, more simply, mixed models are statistical models that incorporate both fixed-effects parameters and random effects. Because of the way that we will define random effects, a model with random effects always includes at least one fixed-effects parameter. Thus, any model with random effects is a mixed model.\nWe characterize the statistical model in terms of two random variables: a \\(q\\)-dimensional vector of random effects represented by the random variable \\(\\mcB\\) and an \\(n\\)-dimensional response vector represented by the random variable \\(\\mcY\\). (We use upper-case “script” characters to denote random variables. The corresponding lower-case upright letter denotes a particular value of the random variable, with vector-valued random variable values in boldface.) We observe the value, \\(\\bby\\), of \\(\\mcY\\). We do not observe the value, \\(\\bbb\\), of \\(\\mcB\\).\nWhen formulating the model we describe the unconditional distribution of \\(\\mcB\\) and the conditional distribution, \\((\\mcY|\\mcB=\\bbb)\\). The descriptions of the distributions involve the form of the distribution and the values of certain parameters. We use the observed values of the response and the covariates to estimate these parameters and to make inferences about them.\nThat’s the big picture. Now let’s make this more concrete by describing a particular, versatile class of mixed models called linear mixed models and by studying a simple example of such a model. First we describe the data in the example."
  },
  {
    "objectID": "intro.html#sec-DyestuffData",
    "href": "intro.html#sec-DyestuffData",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.2 The dyestuff and dyestuff2 data",
    "text": "1.2 The dyestuff and dyestuff2 data\nModels with random effects have been in use for a long time. The first edition of the classic book, Statistical Methods in Research and Production, edited by O.L. Davies, was published in 1947 and contained examples of the use of random effects to characterize batch-to-batch variability in chemical processes. The data from one of these examples are available as the dyestuff data in the MixedModels package. In this section we describe and plot these data and introduce a second example, the dyestuff2 data, described in Box & Tiao (1973).\n\n1.2.1 The dyestuff data\nThe data are described in Davies & Goldsmith (1972, Table 6.3, p. 131), the fourth edition of the book mentioned above, as coming from\n\nan investigation to find out how much the variation from batch to batch in the quality of an intermediate product (H-acid) contributes to the variation in the yield of the dyestuff (Naphthalene Black 12B) made from it. In the experiment six samples of the intermediate, representing different batches of works manufacture, were obtained, and five preparations of the dyestuff were made in the laboratory from each sample. The equivalent yield of each preparation as grams of standard colour was determined by dye-trial.\n\nTo access these data within Julia we must first attach this package, and others that we will use, to our session.\n\nusing AlgebraOfGraphics # high-level graphics\nusing CairoMakie        # graphics back-end\nusing CategoricalArrays # factors and ordered factors\nusing DataFrameMacros   # elegant DataFrame manipulation\nusing DataFrames        # DataFrame implementation\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing Markdown          # utilities for generating markdown text\nusing MixedModels       # fit and examine mixed-effects models\nusing MixedModelsMakie  # graphics for mixed-effects models\nusing Printf            # formatted printing\nusing ProgressMeter     # progress of optimizer iterations\nusing Random            # random number generation\nusing StatsBase         # basic statistical summaries\n\nusing AlgebraOfGraphics: AlgebraOfGraphics as AoG # for easy access to some non exported methods\n\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\n\n┌ Warning: SIMD capacity not detected by ScanByte, using scalar fallback\n└ @ ScanByte /Users/reinholdkliegl/.julia/packages/ScanByte/cr4PT/src/ScanByte.jl:11\n\n\nA package must be attached before any of the data sets or functions in the package can be used. If entering this line results in an error report stating that there is no package by one of these names then you must first install the package(s). If you are using Julia version 1.8.0 or later the error report will include instructions on how to do this.\nIn what follows, we will assume that these packages have been installed and attached to the session before any of the code shown has been run.\nThe MixedModels package includes several datasets that are used in examples and in tests of the package. Individual datasets can be loaded with the dataset function. To allow for other packages to incorporate their own dataset function without causing name clashes, this function is not exported and must be called with the fully-qualified name MixedModels.dataset. (If this explanation sounds like gibberish to you, the bottom line is that you must use MixedModels.dataset(:dyestuff) and not just dataset(:dyestuff).)\n\n\n\n\n\n\nNote\n\n\n\nAdd metadata to Arrow files for datasets\n\n\n\ndyestuff = MixedModels.dataset(:dyestuff)\n\nArrow.Table with 30 rows, 2 columns, and schema:\n :batch  String\n :yield  Int16\n\n\nThe output indicates that this dataset is a Table read from a file in the Arrow data format.\nA Table is a general, but “bare bones”, tabular form defined in the Tables package. This particular table, read from an Arrow file, will be read-only. Often it is convenient to convert the read-only table form to a DataFrame to be able to use the full power of the DataFrames package.\n\ndyestuff = DataFrame(dyestuff)\n\n30 rows × 2 columnsbatchyieldStringInt161A15452A14403A14404A15205A15806B15407B15558B14909B156010B149511C159512C155013C160514C151015C156016D144517D144018D159519D146520D154521E159522E163023E151524E163525E162526F152027F145528F145029F148030F1445\n\n\nThe describe method for a DataFrame provides a concise description of the structure of the data,\n\ndescribe(dyestuff)\n\n2 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1batchAF0String2yield1527.514401530.016350Int16\n\n\nCombining this information with the initial description of the Table we see that the data frame consists of 30 observations of the yield, the response variable, and of the covariate, batch, which is a categorical variable whose levels are character strings.\n\ntypeof(dyestuff.batch)\n\nArrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}\n\n\n(DictEncoded is the Arrow term for a categorical structure where the levels form a dictionary or lookup table and the values are stored as indices into this lookup table.)\n\nshow(levels(dyestuff.batch))\n\n[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n\n\nIf the labels for the factor levels are arbitrary, as they are here, we will use letters instead of numbers for the labels. That is, we label the batches as \"A\" through \"F\" rather than 1 through 6. When the labels are letters it is clear that the variable is categorical. When the labels are numbers a categorical covariate can be mistaken for a numeric covariate, with unintended consequences.\nIt is a good practice to apply describe to any data frame the first time you work with it and to check carefully that any categorical variables are indeed represented as factors.\nThe data in a data frame are viewed as a table with columns corresponding to variables and rows to observations. The functions first and last select the first or last few rows of the table.\n\nfirst(dyestuff, 7)\n\n7 rows × 2 columnsbatchyieldStringInt161A15452A14403A14404A15205A15806B15407B1555\n\n\n\nlast(dyestuff, 7)\n\n7 rows × 2 columnsbatchyieldStringInt161E16352E16253F15204F14555F14506F14807F1445\n\n\nor we could tabulate the data using groupby and combine from DataFrames.\n\ncombine(groupby(dyestuff, :batch), :yield => mean, nrow => :n)\n\n6 rows × 3 columnsbatchyield_meannStringFloat64Int641A1505.052B1528.053C1564.054D1498.055E1600.056F1470.05\n\n\nAlthough this table does show us an important property of the data, namely that there are exactly 5 observations on each batch — a property that we will describe by saying that the data are balanced with respect to batch — we usually learn much more about the structure of such data from plots like Figure 1.1\n\n\nCode\n\"\"\"\n    _meanrespfrm(df, :resp::Symbol, :grps::Symbol; sumryf::Function=mean)\n\nReturns a `DataFrame` created from df with the levels of `grps` reordered according to\n`combine(groupby(df, grps), resp => sumryf)` and this summary DataFrame, also with the\nlevels of `grps` reordered.\n\"\"\"\nfunction _meanrespfrm(\n  df,\n  resp::Symbol,\n  grps::Symbol;\n  sumryf::Function=mean,\n)\n  # ensure the relevant columns are types that Makie can deal with\n  df = transform(\n    df,\n    resp => Array,\n    grps => CategoricalArray;\n    renamecols=false,\n  )\n  # create a summary table by mean resp\n  sumry =\n    sort!(combine(groupby(df, grps), resp => sumryf => resp), resp)\n  glevs = string.(sumry[!, grps])   # group levels in ascending order of mean resp\n  levels!(df[!, grps], glevs)\n  levels!(sumry[!, grps], glevs)\n  return df, sumry\nend\nlet\n  df, sumry = _meanrespfrm(dyestuff, :yield, :batch)\n  mp = mapping(\n    :yield => \"Yield of dyestuff [g]\",\n    :batch => \"Batch of intermediate product\",\n  )\n  draw(\n    (data(df) * mp * visual(Scatter; marker='○', markersize=12)) +\n    (data(sumry) * mp * visual(Lines));\n    figure=(; resolution=(800, 400)),\n  )\nend\n\n\n\n\n\nFigure 1.1: Yield of dyestuff by batch. The line joins the mean yields.\n\n\n\n\nthan we do from numerical summaries.\nIn Figure 1.1 we can see that there is considerable variability in yield, even for preparations from the same batch, but there is also noticeable batch-to-batch variability. For example, four of the five preparations from batch F provided lower yields than did any of the preparations from batches B, C, and E.\nThis plot, and essentially all the other plots in this book, were created using the Makie package (Danisch & Krumbiegel (2021)).\nIn yet-to-be-written-appendix we review some of the principles of data graphics, such as reordering the levels of the factor by increasing mean response, that enhance the informativeness of the plot. For example, in this plot the levels of batch are sorted by increasing mean yield, to make visual comparisons between batches easier, and the vertical positions are jittered to avoid overplotting of points. (Note that the two lowest yields of samples from batch A are identical.)\n\n\n\n\n\n\nNote\n\n\n\nJittering has not yet been added.\n\n\nAt this point we will concentrate on the information conveyed by the plot and not on how the plot is created.\nIn Section 1.3 we will use mixed models to quantify the variability in yield between batches. For the time being let us just note that the particular batches used in this experiment are a selection or sample from the set of all batches that we wish to consider. Furthermore, the extent to which one particular batch tends to increase or decrease the mean yield of the process — in other words, the “effect” of that particular batch on the yield — is not as interesting to us as is the extent of the variability between batches. For the purposes of designing, monitoring and controlling a process we want to predict the yield from future batches, taking into account the batch-to-batch variability and the within-batch variability. Being able to estimate the extent to which a particular batch in the past increased or decreased the yield is not usually an important goal for us. We will model the effects of the batches as random effects rather than as fixed-effects parameters.\n\n\n1.2.2 The dyestuff2 data\nThe data are simulated data presented in Box & Tiao (1973, Table 5.1.4, p. 247) where the authors state\n\nThese data had to be constructed for although examples of this sort undoubtedly occur in practice they seem to be rarely published.\n\nThe structure and summary\n\ndyestuff2 = MixedModels.dataset(:dyestuff2)\n\nArrow.Table with 30 rows, 2 columns, and schema:\n :batch  String\n :yield  Float64\n\n\n\ndyestuff2 = DataFrame(dyestuff2)\ndescribe(dyestuff2)\n\n2 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1batchAF0String2yield5.6656-0.8925.36513.4340Float64\n\n\nare intentionally similar to those of the dyestuff data.\nA data plot (Figure 1.2)\n\n\nCode\nlet\n  df, sumry = _meanrespfrm(dyestuff2, :yield, :batch)\n  mp = mapping(:yield => \"Simulated yield\", :batch => \"Batch\")\n  draw(\n    (data(df) * mp * visual(Scatter; marker='○', markersize=12)) +\n    (data(sumry) * mp * visual(Lines));\n    figure=(; resolution=(800, 400)),\n  )\nend\n\n\n\n\n\nFigure 1.2: Artificial data of yield by batch. The line joins the mean yields.\n\n\n\n\nshows that the batch-to-batch variability in these data is small compared to the within-batch variability.\nIn some approaches to mixed models it can be difficult to fit models to such data. Paradoxically, small “variance components” can be more difficult to estimate than large variance components.\nThe methods we will present are not compromised when estimating small variance components."
  },
  {
    "objectID": "intro.html#sec-fittinglmms",
    "href": "intro.html#sec-fittinglmms",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.3 Fitting linear mixed models",
    "text": "1.3 Fitting linear mixed models\nBefore we formally define a linear mixed model, let’s go ahead and fit models to these data sets using MixedModels. The simplest way to do this is to use the generic fit function with arguments describing the type of model to be fit (i.e. MixedModel), a formula specifying the model and the data on which to evaluate the formula.\nWe will explain the structure of the formula after we have considered an example.\n\n1.3.1 A model for the dyestuff data\nWe fit a model to the data allowing for an overall level of the yield and for an additive random effect for each level of batch.\n\ndsm01 = let\n  form = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, form, dyestuff)\nend\n\n\u001b[32mMinimizing 19   Time: 0:00:00 (11.43 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_batch\n\n\n\n\n(Intercept)\n1527.5000\n17.6946\n86.33\n<1e-99\n37.2603\n\n\nResidual\n49.5101\n\n\n\n\n\n\n\n\n\nBy default, mixed-effects models in Jupyter notebooks, which is the evaluation engine behind quarto.org books, are shown compactly like this. To obtain more information we can print the model, as in\n\nprintln(dsm01)\n\nLinear mixed model fit by maximum likelihood\n yield ~ 1 + (1 | batch)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -163.6635   327.3271   333.3271   334.2501   337.5307\n\nVariance components:\n            Column    Variance Std.Dev.\nbatch    (Intercept)  1388.3332 37.2603\nResidual              2451.2501 49.5101\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────\n\n\n\n              Coef.  Std. Error      z  Pr(>|z|)\n────────────────────────────────────────────────\n(Intercept)  1527.5     17.6946  86.33    <1e-99\n────────────────────────────────────────────────\n\n\nThe call to fit constructs a LinearMixedModel object, evaluates the maximum likelihood parameter estimates, assigns the results to the name dsm01, and displays a summary of the fitted model.\n\n\n1.3.2 Details of the printed display\nThe display of the fitted model has four major sections:\n\na description of the model that was fit\nsome statistics characterizing the model fit\na summary of properties of the random effects and\na summary of the fixed-effects parameter estimates.\n\nWe consider each of these sections in turn.\nThe description section states that this is a linear mixed model in which the parameters have been estimate by maximum likelihood (ML). The model formula is displayed for later reference.\nThe display of a model fit by maximum likelihood provides several model-fit statistics such as Akaike’s Information Criterion (Sakamoto et al., 1986), Schwarz’s Bayesian Information Criterion (Schwarz, 1978), the log-likelihood at the parameter estimates, and negative twice the log-likelihood, which is the estimation criterion transformed to the scale of the deviance. For linear mixed models we refer to -2 loglik as the value of the objective because this is the value that is minimized during the optimization phase of fitting the model. To evaluate the deviance we should subtract the value of this criterion at a saturated or baseline model but it is not clear how to define such a baseline model in these cases.\nHowever, it is still possible to perform likelihood ratio tests of different models fit to the same data using the difference in the minimized objectives, because this difference is the same as the difference in the deviances. (Recall that a ratio of likelihoods corresponds to a difference in log-likelihoods.)\nThe third section is the table of estimates of parameters associated with the random effects. There are two sources of variability in the model we have fit, a batch-to-batch variability in the level of the response and the residual or per-observation variability — also called the within-batch variability. The name “residual” is used in statistical modeling to denote the part of the variability that cannot be explained or modeled with the other terms. It is the variation in the observed data that is “left over” after we have determined the estimates of the parameters in the other parts of the model.\nSome of the variability in the response is associated with the fixed-effects terms. In this model there is only one such term, labeled as the (Intercept). The name “intercept”, which is better suited to models based on straight lines written in a slope/intercept form, should be understood to represent an overall “typical” or mean level of the response in this case. (In case you are wondering about the parentheses around the name (Intercept), they are included so that you can’t accidentally create a variable with a name that conflicts with this name.) The line labeled batch in the random effects table shows that the random effects added to the term, one for each level of the factor batch, are modeled as random variables whose unconditional variance is estimated as 1388.33 g\\(^2\\) in the ML fit. The corresponding standard deviation is 37.26 g.\nNote that the last column in the random effects summary table is the estimate of the variability expressed as a standard deviation rather than as a variance. These values are provided because usually it is easier to visualize standard deviations, which are on the scale of the response, than it is to visualize the magnitude of a variance. The values in this column are a simple re-expression (the square root) of the estimated variances. Do not confuse them with standard errors of the variance estimators, which are not given here. In add-section-reference-here we explain why we do not provide standard errors of variance estimates.\nThe line labeled Residual in this table gives the estimate of the variance of the residuals (also in g\\(^2\\)) and its corresponding standard deviation. The estimated standard deviation of the residuals is 49.5 g.\nThe last line in the random effects table states the number of observations to which the model was fit and the number of levels of any “grouping factors” for the random effects. In this case we have a single random effects term, (1|batch), in the model formula and the grouping factor for that term is batch. There will be a total of six random effects, one for each level of batch.\nThe final part of the printed display gives the estimates and standard errors of any fixed-effects parameters in the model. The only fixed-effects term in the model formula is the 1, denoting a constant which, as explained above, is labeled as (Intercept). The estimate of this parameter is 1527.5 g, which happens to be the mean yield across all the data - a consequence of the experiment being balanced, in the sense that there was the same number of observations for each batch.\nThis coefficient table also includes the standard error of the estimate, which is the estimated standard deviation of the estimator, a z ratio, which is the ratio of the estimate to its standard error, and the probability the absolute value of a standard normal distribution exceeding the absolute value of the z ratio.\nIf the hypothesis that the coefficient is zero is of interest, which is not the case here, then the value in the fourth column is an approximate p-value for the hypothesis test. An alternative measure of precision of the estimate - a coverage interval based on a parametric bootstrap - is presented in Section 1.5\n\n\n1.3.3 A model for the dyestuff2 data\nFitting a similar model to the dyestuff2 data produces an estimate \\(\\widehat{\\sigma}_1=0\\).\n\ndsm02 = let\n  form = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, form, dyestuff2)\nend\nprintln(dsm02)\n\nLinear mixed model fit by maximum likelihood\n yield ~ 1 + (1 | batch)\n   logLik   -2 logLik     AIC       AICc        BIC    \n   -81.4365   162.8730   168.8730   169.7961   173.0766\n\nVariance components:\n            Column   Variance Std.Dev.\nbatch    (Intercept)   0.00000 0.00000\nResidual              13.34610 3.65323\n Number of obs: 30; levels of grouping factors: 6\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────\n              Coef.  Std. Error     z  Pr(>|z|)\n───────────────────────────────────────────────\n(Intercept)  5.6656    0.666986  8.49    <1e-16\n───────────────────────────────────────────────\n\n\nAn estimate of \\(0\\) for \\(\\sigma_1\\) does not mean that there is no variation between the groups. Indeed Figure 1.2 shows that there is some small amount of variability between the groups. The estimate, \\(\\widehat{\\sigma}_1=0\\), simply indicates that the level of “between-group” variability is not sufficient to warrant incorporating random effects in the model.\nThis point is worth reiterating. An estimate of zero for a variance component does not imply that there is no variability between the groups. There will always be variability between groups that is induced by the per-observation variability. If we arbitrarily divided 30 observations from a homogeneous distribution into six groups of five observations, which is likely the way that this sample was simulated, the sample averages would inherit a level of variability from the original sample. What is being estimated in the variance component for batch is the excess variability between groups beyond that induced by the residual variability.\nIn other words, an estimate \\(\\widehat{\\sigma}_1=0\\) is not inconsistent with the model. The important point to take away from this example is that we must allow for the estimates of variance components to be zero.\nWe describe such a model as being degenerate, in the sense that the estimated distribution of the random effects is a degenerate distribution, representing a point mass at zero. It corresponds to a linear model in which we have removed the random effects associated with batch.\nDegenerate models can and do occur in practice. Even when the final fitted model is not degenerate, we must allow for such models when determining the parameter estimates through numerical optimization.\nTo reiterate, this model can be reduced to a linear model because the random effects are inert, in the sense that they have a variance of zero."
  },
  {
    "objectID": "intro.html#sec-modelformulation",
    "href": "intro.html#sec-modelformulation",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.4 Model formulation",
    "text": "1.4 Model formulation\nA common way of writing the statistical model being fit in dsm01 and dsm02 is as an expression for the i’th observation in the j’th batch \\[\ny_{i,j}=\\mu+b_j+\\epsilon_{i,j},\\quad i = 1,\\dots,5;\\;j=1,\\dots,6\n\\] with the further specification that the per-observation noise terms, \\(\\epsilon_{i,j}\\), are independently and identically distributed as \\(\\mcN(0, \\sigma^2)\\) and the random effects, \\(b_j\\), are independently and identically distributed as \\(\\mcN(0, \\sigma_1^2)\\). The three parameters in the model are the overall mean, \\(\\mu\\), the standard deviation of the random effects, \\(\\sigma_1\\), and the standard deviation of the per-observation noise, \\(\\sigma\\).\nGeneralizing this formulation to unbalanced data sets and more complex models can become unwieldy. For the range of models shown in this book it is more convenient to use a matrix-vector representation in which the random effects are described as a \\(q\\)-dimensional multivariate Gaussian random variable \\[\n\\mcB\\sim\\mcN(\\mathbf{0},\\sigma_1^2\\bbI) .\n\\qquad(1.1)\\] The multivariate Gaussian distribution is described in more detail in Section A.2.3. At this point the important thing to know is that Equation 1.1 describes a vector of independent Gaussian random variables, each of which has mean \\(0\\) and variance \\(\\sigma_1^2\\).\nThe random effects are associated with the observations through a model matrix, \\(\\bbZ\\). For a model with \\(n\\) observations and a total of \\(q\\) random effects, \\(\\bbZ\\) will be of size \\(n\\times q\\). Furthermore, it is sparse - meaning that most of the elements of the matrix are zeros. In this case, \\(\\bbZ\\) is the indicator matrix for the batch covariate.\n\nZdsm01 = Int.(Matrix(only(dsm01.reterms)))\n\n30×6 Matrix{Int64}:\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 1  0  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  1  0  0  0  0\n 0  0  1  0  0  0\n 0  0  1  0  0  0\n 0  0  1  0  0  0\n ⋮              ⋮\n 0  0  0  1  0  0\n 0  0  0  1  0  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  1  0\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n 0  0  0  0  0  1\n\n\nThe expression to produce that output first extracted the random-effects terms information from dsm01, extracted the term for the first grouping factor, checking that there is only one, converted it to a matrix and converted the matrix to integers to save on space when printing.\nIn cases like this we will often print out the transpose of the model matrix to save more space\n\nZdsm01'\n\n6×30 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  1  1  1  1  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  1  1  1  1  1  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  1  1  1     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  1  1  1  1  1  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  1  1  1  1  1\n\n\nor, even more compact, as a sparse matrix pattern\n\nsparse(Zdsm01')\n\n6×30 SparseArrays.SparseMatrixCSC{Int64, Int64} with 30 stored entries:\n⠉⠉⠑⠒⠒⠤⠤⢄⣀⣀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠑⠒⠒\n\n\nThe fixed-effects parameters, of which there is only one here, are gathered into a vector, \\(\\bbbeta\\), of length \\(p\\), and multiplied by another model matrix, \\(\\bbX\\), of size \\(n\\times p\\).\n\ndsm01.β\n\n1-element Vector{Float64}:\n 1527.4999999999989\n\n\n\nInt.(dsm01.X')\n\n1×30 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n\n\nA linear model without random effects can be described as \\(\\mcY\\sim\\mcN(\\bbX\\bbbeta, \\sigma^2\\bbI)\\). That is, the expected value of the response vector is the linear predictor, \\(\\bbX\\bbbeta\\), and the covariance matrix of the multivariate Gaussian distribution for the response is \\(\\sigma^2\\bbI\\), corresponding to independent, constant variance per-observation noise terms.\nFor the linear mixed model, we describe the conditional distribution of the response, given a particular value, \\(\\bbb\\), of the random effects as multivariate Gaussian with mean determined by a linear predictor expression involving both the assumed random effects value, \\(\\bbb\\), and the fixed-effects parameter vector \\(\\bbbeta\\). \\[\n(\\mcY|\\mcB=\\bbb)\\sim\\mcN(\\bbX\\bbbeta+\\bbZ\\bbb,\\sigma^2\\bbI)\n\\]\nThe complete model can be written as \\[\n\\begin{aligned}\n(\\mcY|\\mcB=\\bbb)&\\sim\\mcN(\\bbX\\bbbeta+\\bbZ\\bbb,\\sigma^2\\bbI)\\\\\n\\mcB&\\sim\\mcN(\\mathbf{0},\\bbSigma_\\theta) .\n\\end{aligned}\n\\qquad(1.2)\\]\nFrom these two distributions, the joint distribution of \\(\\mcY\\) and \\(\\mcB\\), which is also multivariate Gaussian, can be determined and from that the likelihood for the parameters, given the observed value \\(\\bby\\) of \\(\\mcY\\).\nDetails on the derivation and evaluation of the log-likelihood are given in Section A.7. At this point the important results are that the profiled log-likelihood for models dsm01 and dsm02 can be evaluated directly (i.e. without an iterative optimization) from a single parameter, \\(\\theta=\\sigma_1/\\sigma\\).\nFurthermore, this profiled log-likelihood can be evaluated from a matrix of \\(\\bbX'\\bbX\\)-like products, \\[\n\\bbA=\n\\begin{bmatrix}\n\\bbZ'\\bbZ & \\bbZ'\\bbX & \\bbZ'\\bby \\\\\n\\bbX'\\bbZ & \\bbX'\\bbX & \\bbX'\\bby \\\\\n\\bby'\\bbZ & \\bby'\\bbX & \\bby'\\bby\n\\end{bmatrix}\n\\] which is stored in three pieces\n\nfirst(dsm01.A)\n\n6×6 LinearAlgebra.Diagonal{Float64, Vector{Float64}}:\n 5.0   ⋅    ⋅    ⋅    ⋅    ⋅ \n  ⋅   5.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅    ⋅   5.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅    ⋅   5.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅    ⋅   5.0\n\n\n\ndsm01.A[2]\n\n2×6 Matrix{Float64}:\n    5.0     5.0     5.0     5.0     5.0     5.0\n 7525.0  7640.0  7820.0  7490.0  8000.0  7350.0\n\n\n\nlast(dsm01.A)\n\n2×2 Matrix{Float64}:\n    30.0  45825.0\n 45825.0      7.01129e7\n\n\nThe first, diagonal matrix is \\(\\bbZ'\\bbZ\\). A simple, scalar random effects term like (1|batch) in the formula for models dsm01 and dsm02 results in \\(\\bbZ\\) being the indicator matrix for the levels of the grouping factor and, hence, in a diagonal \\(\\bbZ'\\bbZ\\), whose diagonal elements are the frequencies of the levels. Thus this block shows that there are exactly 5 observations on each of the 6 batches.\nThe second, rectangular matrix is \\(\\begin{bmatrix} \\bbZ'\\bbX \\\\ \\bbZ'\\bby \\end{bmatrix}\\). Again, the fact that \\(\\bbZ\\) is the indicator matrix for the levels of the grouping factor and that \\(\\bbX\\) is a single column of 1’s results in the frequencies of the levels of the factor in the first row of this block. The second row of this block consists of the sums of the yields for each batch.\nThe third, square symmetric matrix is \\[\n\\begin{bmatrix}\n\\bbX'\\bbX & \\bbX'\\bby \\\\\n\\bby'\\bbX & \\bby'\\bby\n\\end{bmatrix}\n\\]\nThe parameters enter into the computation via a relative covariance factor, \\(\\bbLambda_{\\boldsymbol\\theta}\\), which is the \\(6\\times 6\\) matrix \\(\\theta\\,\\bbI\\) in this case.\nThe method hinges on being able to evaluate efficiently the Cholesky factor (see Section A.2.2 for details) of \\[\n\\begin{bmatrix}\n\\bbLambda_\\theta'\\mathbf{Z'Z}\\bbLambda_\\theta+\\bbI &\n\\bbLambda_\\theta'\\mathbf{Z'X} & \\bbLambda_\\theta'\\mathbf{Z'y} \\\\\n\\mathbf{X'Z}\\bbLambda_\\theta & \\mathbf{X'X} & \\mathbf{X'y} \\\\\n\\mathbf{y'Z}\\bbLambda_\\theta & \\mathbf{y'X} & \\mathbf{y'y}\n\\end{bmatrix}\n\\]\nThe optimal value of \\(\\theta\\) for model dsm01 is\n\nonly(dsm01.θ)\n\n0.7525806394967323\n\n\nproducing a lower Cholesky factor of\n\nsparseL(dsm01; full=true)\n\n8×8 SparseArrays.SparseMatrixCSC{Float64, Int32} with 21 stored entries:\n    1.95752      ⋅           ⋅       …      ⋅           ⋅          ⋅ \n     ⋅          1.95752      ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅          1.95752         ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅              ⋅           ⋅          ⋅ \n     ⋅           ⋅           ⋅       …     1.95752      ⋅          ⋅ \n    1.92228     1.92228     1.92228        1.92228     2.79804     ⋅ \n 2893.03     2937.24     3006.45        2825.75     4274.01     271.178"
  },
  {
    "objectID": "intro.html#sec-furtherassess",
    "href": "intro.html#sec-furtherassess",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.5 Variability of parameter estimates",
    "text": "1.5 Variability of parameter estimates\nThe parameter estimates in a statistical model represent our “best guess” at the unknown values of the model parameters and, as such, are important results in statistical modeling. However, they are not the whole story. Statistical models characterize the variability in the data and we should assess the effect of this variability on the precision of the parameter estimates and of predictions made from the model.\nOne method of assessing the variability in the parameter estimates is through a parametric bootstrap, a process where a large number of data sets are simulated from the assumed model using the estimated parameter values as the “true” parameter values. The distribution of the parameter estimators is inferred from the distribution of the parameter estimates from these generated data sets.\nThis method is well-suited to Julia code because refitting an existing model to a simulated data set can be very fast.\nFor methods that involve simulation, it is best to initialize a random number generator to a known state so that the “random” sample can be reproduced if desired. Beginning with Julia v1.7.0 the default random number generator is the Xoshiro generator, which we initialize to an arbitrary, but reproducible, value.\nThe object returned by a call to parametricbootstrap has a somewhat complex internal structure to allow for the ability to simulate from complex models while still maintaining a comparatively small storage footprint. To examine the distribution of the parameter estimates we extract a table of all the estimated parameters and convert it to a DataFrame.\n\nconst hide_progress = true # hide the progress bar when sampling\nRandom.seed!(4321234)      # random number generator\ndsm01samp = parametricbootstrap(10_000, dsm01; hide_progress)\ndsm01pars = DataFrame(dsm01samp.allpars)\nfirst(dsm01pars, 7)\n\n7 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411βmissing(Intercept)1515.3421σbatch(Intercept)10.181731σresidualmissing54.280442βmissing(Intercept)1502.8152σbatch(Intercept)35.398362σresidualmissing48.369273βmissing(Intercept)1529.74\n\n\n\nlast(dsm01pars, 7)\n\n7 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6419998σresidualmissing42.822729999βmissing(Intercept)1519.0439999σbatch(Intercept)11.033649999σresidualmissing58.4295510000βmissing(Intercept)1474.47610000σbatch(Intercept)23.4016710000σresidualmissing37.5372\n\n\nPlots of the bootstrap estimates for individual parameters are obtained by extracting subsets of the rows of this dataframe using subset methods from the DataFrames package or the @subset macro from the DataFramesMacros package. For example,\n\nβdf = @subset(dsm01pars, :type == \"β\")\n\n10,000 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411βmissing(Intercept)1515.3422βmissing(Intercept)1502.8133βmissing(Intercept)1529.7444βmissing(Intercept)1537.3455βmissing(Intercept)1516.7766βmissing(Intercept)1522.1677βmissing(Intercept)1523.2788βmissing(Intercept)1527.5299βmissing(Intercept)1518.531010βmissing(Intercept)1538.921111βmissing(Intercept)1518.341212βmissing(Intercept)1537.791313βmissing(Intercept)1533.771414βmissing(Intercept)1505.311515βmissing(Intercept)1518.941616βmissing(Intercept)1541.71717βmissing(Intercept)1531.811818βmissing(Intercept)1535.221919βmissing(Intercept)1541.562020βmissing(Intercept)1498.152121βmissing(Intercept)1528.542222βmissing(Intercept)1516.482323βmissing(Intercept)1504.052424βmissing(Intercept)1506.832525βmissing(Intercept)1492.012626βmissing(Intercept)1526.452727βmissing(Intercept)1523.552828βmissing(Intercept)1523.32929βmissing(Intercept)1518.013030βmissing(Intercept)1506.34⋮⋮⋮⋮⋮⋮\n\n\nWe begin by examining density plots constructed using the AlgebraOfGraphics package. (In general we “fold” the code used to generate plots as it tends to have considerable detail that may distract from the plot itself. You can check the details by clicking on the “Code” button in the HTML version of the plot or in the quarto files on the github repository for this book.)\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"β\")) *\n  mapping(\n    :value => \"Bootstrap samples of β\";\n    color=(:names => \"Names\"),\n  ) *\n  AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.3: Kernel density plot of bootstrap fixed-effects parameter estimates from dsm01\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFind a way to use :compact=true when interpolating numeric values into the text\n\n\n\n\nThe distribution of the estimates of β₁ is more-or-less a Gaussian (or “normal”) shape, with a mean value of 1528.021212 which is close to the estimated β₁ of 1527.500000.\nSimilarly the standard deviation of the simulated β values, 17.711433258971017 is close to the standard error of the parameter, 17.694552727788437.\n\n\nIn other words, the estimator of the fixed-effects parameter in this case behaves as we would expect. The estimates are approximately normally distributed centered about the “true” parameter value with a standard deviation given by the standard error of the parameter.\nThe situation is different for the estimates of the standard deviation parameters, \\(\\sigma\\) and \\(\\sigma_1\\), as shown in Figure 1.4.\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"σ\")) *\n  mapping(\n    :value => \"Bootstrap samples of σ\";\n    color=(:group => \"Group\"),\n  ) *\n  AoG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.4: Kernel density plot of bootstrap variance-component parameter estimates from model dsm01\n\n\n\n\nThe estimator for the residual standard deviation, \\(\\sigma\\), is approximately normally distributed but the estimator for \\(\\sigma_1\\), the standard deviation of the batch random effects is bimodal (i.e. has two “modes” or local maxima).\n\n\nThere is one peak around the “true” value for the simulation, 37.26034309083298, and another peak at zero.\n\n\nThe apparent distribution of the estimates of \\(\\sigma_1\\) in Figure 1.4 is being distorted by the method of approximating the density. A kernel density estimate approximates a probability density from a finite sample by blurring or smearing the positions of the sample values according to a kernel such as a narrow Gaussian distribution (see the linked article for details). In this case the distribution of the estimates is a combination of a continuous distribution and a spike or point mass at zero as shown in a histogram, Figure 1.5.\n\n\n\n\n\n\nNote\n\n\n\nUse a lower alpha in the colors for multiple histograms so the bars behind another color are more visible\n\n\n\n\nCode\ndraw(\n  data(@subset(dsm01pars, :type == \"σ\")) *\n  mapping(\n    :value => \"Bootstrap parameter estimates of σ\";\n    color=(:group => \"Group\"),\n  ) *\n  AoG.histogram(; bins=80);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.5: Histogram of bootstrap variance-components as standard deviations from model dsm01\n\n\n\n\nNearly 1000 of the 10,000 bootstrap fits are “singular” in that one (or more) of the estimated unconditional distribution(s) of the random effects are degenerate. For this model the only way singularity can occur is for \\(\\sigma_1\\) to be zero.\n\ncountmap(issingular(dsm01samp))\n\nDict{Bool, Int64} with 2 entries:\n  0 => 9007\n  1 => 993\n\n\nThe distribution of the estimator of \\(\\sigma_1\\) with this point mass at zero is not at all like a Gaussian or normal distribution. This is why characterizing the uncertainty of these parameter estimates with a standard error is misleading. Quoting an estimate and a standard error for the estimate is only meaningful if these values adequately characterize the distribution of the values of the estimator.\nIn many cases standard errors are quoted for estimates of the variance components, \\(\\sigma_1^2\\) and \\(\\sigma^2\\), whose distribution (Figure 1.6) in the bootstrap sample is even less like a normal or Gaussian distribution than is the distribution of estimates of \\(\\sigma_1\\).\n\n\nCode\ndraw(\n  data(@transform(@subset(dsm01pars, :type == \"σ\"), abs2(:value))) *\n  mapping(\n    :value_abs2 => \"Bootstrap sample of estimates of σ²\",\n    color=:group,\n  ) *\n  AoG.histogram(; bins=200);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1.6: Histogram of bootstrap variance-components from model dsm01\n\n\n\n\nThe approach of creating coverage intervals from a bootstrap sample of parameter estimates, described in the next section, does accomodate non-normal distributions.\n\n1.5.1 Confidence intervals on the parameters\nA bootstrap sample of parameter estimates also provides us with a method of creating bootstrap coverage intervals, which can be regarded as confidence intervals on the parameters.\nFor, say, a 95% coverage interval based on 10,000 parameter estimates we determine an interval that contains 95% (9,500, in this case) of the estimated parameter values.\nThere are many such intervals. For example, from the sorted parameter values we could return the interval from the smallest up to the 9,500th largest, or from the second smallest up to the 9,501 largest, and so on.\nThe shortestcovint method returns the shortest of all these potential intervals which will correspond to the interval with the highest empirical density.\n\nDataFrame(shortestcovint(dsm01samp))\n\n3 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)1493.651563.52σbatch(Intercept)0.054.30613σresidualmissing35.349962.6078\n\n\nFor the (Intercept) fixed-effects parameter the interval is similar to an “asymptotic” or Wald interval constructed as the estimate plus or minus two standard errors.\n\nshow(only(dsm01.β) .+ [-2, 2] * only(dsm01.stderror))\n\n[1492.110894544422, 1562.8891054555756]\n\n\nThe interval on the residual standard deviation, \\(\\sigma\\), is reasonable, given that there are only 30 observations, but the interval of the standard deviation of the random effects, \\(\\sigma_1\\), extends all the way down to zero.\n\n\n1.5.2 Tracking the progress of the iterations\nThe optional argument, thin=1, in a call to fit causes all the values of \\(\\boldsymbol\\theta\\) and the corresponding value of objective from the iterative optimization to be stored in the optsum property.\n\n\n\n\n\n\nNote\n\n\n\nFix the recording of the fitlog so that the initial value is not recorded twice\n\n\n\ndsm01trace = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  thin=1,\n)\nDataFrame(dsm01trace.optsum.fitlog)\n\n19 rows × 2 columns12Array…Float641[1.0]327.7672[1.0]327.7673[1.75]331.0364[0.25]330.6465[0.97619]327.6956[0.928569]327.5667[0.833327]327.3838[0.807188]327.3539[0.799688]327.34710[0.792188]327.34111[0.777188]327.33312[0.747188]327.32713[0.739688]327.32914[0.752777]327.32715[0.753527]327.32716[0.752584]327.32717[0.752509]327.32718[0.752591]327.32719[0.752581]327.327\n\n\n\n\nHere the algorithm converges after 18 function evaluations to a profiled deviance of 327.327059881143 at θ = 0.7525806394967323."
  },
  {
    "objectID": "intro.html#sec-assessRE",
    "href": "intro.html#sec-assessRE",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.6 Assessing the random effects",
    "text": "1.6 Assessing the random effects\nWhat are sometimes called the BLUPs (or best linear unbiased predictors) of the random effects, \\(\\mcB\\), are the mode (location of the maximum probability density) of the conditional distribution, \\((\\mcB|\\mcY=\\bby)\\), evaluated at the parameter estimates and the observed response vector, \\(\\bby\\). Although BLUP is an appealing acronym, we don’t find the term particularly instructive (what is a “linear unbiased predictor” and in what sense are these the “best”?) and prefer the term “conditional mode” to describe this value.\nThese values are often considered as some sort of “estimates” of the random effects. It can be helpful to think of them this way but it can also be misleading. As we have stated, the random effects are not, strictly speaking, parameters — they are unobserved random variables. We don’t estimate the random effects in the same sense that we estimate parameters. Instead, we consider the conditional distribution of \\(\\mcB\\) given the observed data, \\((\\mcB|\\mcY=\\bby)\\).\nBecause the unconditional distribution, \\(\\mcB\\sim\\mcN(\\mathbf{0},\\Sigma_\\theta)\\) is continuous, the conditional distribution, \\((\\mcB|\\mcY=\\bby)\\) will also be continuous. In general, the mode of a probability density is the point of maximum density, so the phrase “conditional mode” refers to the point at which this conditional density is maximized. Because this definition relates to the probability model, the values of the parameters are assumed to be known. In practice, of course, we don’t know the values of the parameters (if we did there would be no purpose in forming the parameter estimates), so we use the estimated values of the parameters to evaluate the conditional modes.\nThose who are familiar with the multivariate Gaussian distribution may recognize that, because both \\(\\mcB\\) and \\((\\mcY|\\mcB=\\bbb)\\) are multivariate Gaussian, \\((\\mcB|\\mcY=\\bby)\\) will also be multivariate Gaussian and the conditional mode will also be the conditional mean of \\(\\mcB\\), given \\(\\mcY=\\bby\\). This is the case for a linear mixed model but it does not carry over to other forms of mixed models. In the general case all we can say about \\(\\tilde{\\bbb}\\) is that they maximize a conditional density, which is why we use the term “conditional mode” to describe these values. We will only use the term “conditional mean” and the symbol, \\(\\bbmu\\), in reference to \\(\\mathrm{E}(\\mcY|\\mcB=\\bbb)\\), which is the conditional mean of \\(\\mcY\\) given \\(\\mcB\\), and an important part of the formulation of all types of mixed-effects models.\nThe conditional modes are available as a vector of matrices\n\nonly(dsm01.b)\n\n1×6 Matrix{Float64}:\n -16.6282  0.369516  26.9747  -21.8014  53.5798  -42.4943\n\n\nIn this case the vector consists of a single matrix because there is only one random-effects term, (1|batch), in the model and, hence, only one grouping factor, batch, for the random effects. There is only one row in the matrix because the random-effects term, (1|batch), is a simple, scalar term.\nTo make this more explicit, random-effects terms in the model formula are those that contain the vertical bar (|) character. The variable or expression on the right of the | is the grouping factor for the random effects generated by this term. If the expression on the left of the vertical bar is 1, as it is here, we describe the term as a simple, scalar, random-effects term. The designation “scalar” means there will be exactly one random effect generated for each level of the grouping factor. A simple, scalar term generates a block of indicator columns — the indicators for the grouping factor — in \\(\\bbZ\\). Because there is only one random-effects term in this model and because that term is a simple, scalar term, the model matrix \\(\\bbZ\\) for this model is the indicator matrix for the levels of batch, as shown earlier in this section.\nIn the next chapter we fit models with multiple simple, scalar terms and, in subsequent chapters, we extend random-effects terms beyond simple, scalar terms. When we have only simple, scalar terms in the model, each term has a unique grouping factor and the elements of the list returned by can be considered as associated with terms or with grouping factors. In more complex models a particular grouping factor may occur in more than one term. In such cases the terms associated with the same grouping factor are internally amalgamated into a single term. Thus internally the random effects are associated with grouping factors, not the terms in the model formula.\nGiven the data, \\(\\bby\\), and the parameter estimates, we can evaluate a measure of the dispersion of \\((\\mcB|\\mcY=\\bby)\\). In the case of a linear mixed model, this is the conditional standard deviation, from which we can obtain a prediction interval. A plot of these prediction intervals is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar when there are many levels of the grouping factor.\n\n\nCode\ncaterpillar!(\n  Figure(resolution=(800, 250)),\n  ranefinfo(dsm01, :batch),\n)\n\n\n\n\n\nFigure 1.7: Caterpillar plot of prediction intervals for dsm01 random effects\n\n\n\n\nThe caterpillar function returns a plot with linear spacing of the levels on the y axis. An alternative\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 250)),\n  ranefinfo(dsm01, :batch),\n)\n\n\n\n\n\nFigure 1.8: Quantile caterpillar plot of prediction intervals for dsm01 random effects\n\n\n\n\nreturns a plot where the intervals are plotted with vertical spacing corresponding to the quantiles of the standard normal distribution.\nThe caterpillar plot is preferred when there are only a few levels of the grouping factor, as in this case. When there are hundreds or thousands of random effects the qqcaterpillar form is preferred because it focuses attention on the “important few” at the extremes and de-emphasizes the “trivial many” that are close to zero."
  },
  {
    "objectID": "intro.html#sec-stylistic",
    "href": "intro.html#sec-stylistic",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.7 Some stylistic conventions",
    "text": "1.7 Some stylistic conventions\nTo make it easier to recognize the form of the many models that will be fit in this book, we will adopt certain conventions regarding the argument specifications. In particular we will establish the convention of specifying contrasts for any categorical covariates and the possibility of using certain transformations, such as centering and scaling, of numeric covariates.\nThe name contrasts is used in a general sense here to specify certain transformations that are to take place during the process of converting a formula and the structure, or schema, of the data into model matrices.\nThe StatsModels.jl package allows for contrasts to be specified as a key-value dictionary where the keys are symbols and the values are of a type that specializes StatsModels.AbstractContrasts. The MixedModels.jl package provides for a Grouping() contrast and the StandardizedPredictors.jl package allows transformations to be expressed as contrasts.\nFor models dsm01 and dsm02 the only covariate in the model formula is batch, which is a grouping factor for the random effects. Thus the desired contrasts specification is\n\ncontrasts = Dict(:batch => Grouping())\n\n(Symbols in Julia can be written as a colon followed by the name of the symbol. The colon creates an expression but, if the expression consists of a single name, then the expression is the symbol.)\nIt is best to get into the habit of specifying Grouping() contrasts for any grouping factors in the data. Doing so is not terribly important when the number of levels of the grouping factor is small, as in the examples in this chapter. However, when the number of levels is large, as for some of the models in later chapters, failure to specify Grouping() contrasts can cause memory faults when constructing the numeric represention of the model.\nThere is an advantage in assigning the contrasts dictionary to the name contrasts because a call to fit with the optional, named argument contrasts\n\ndsm01 = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  contrasts=contrasts,\n)\n\ncan be condensed to\n\ndsm01 = fit(\n  MixedModel,\n  @formula(yield ~ 1 + (1 | batch)),\n  dyestuff;\n  contrasts,\n)\n\nThat is, if the name of the object to be passed as a named argument is the same as the name of the argument, like contrasts=contrasts, then the name does not need to be repeated. Note that the comma after the positional arguments must be changed to a semicolon in this convention. This is necessary because it indicates that arguments following the semicolon are named arguments, not positional.\nAnother convention we will use is assigning the formula separately from the call to fit as part of a let block. Often the formula can become rather long, sometimes needing multiple lines in the call, and it becomes difficult to keep track of the other arguments. Assigning the formula separately helps in keeping track of the arguments to fit.\nA let block is a way of making temporary assignments that do not affect the global state. An assignment to a variable name inside a let block is local to the block.\nThus dsm01 can be assigned as\n\ndsm01 = let\n  form = @formula(yield ~ 1 + (1 | batch))\n  fit(MixedModel, form, dyestuff; contrasts)\nend"
  },
  {
    "objectID": "intro.html#sec-ChIntroSummary",
    "href": "intro.html#sec-ChIntroSummary",
    "title": "1  A Simple, Linear, Mixed-Effects Model",
    "section": "1.8 Chapter summary",
    "text": "1.8 Chapter summary\nA considerable amount of material has been presented in this chapter, especially considering the word “simple” in its title (it’s the model that is simple, not the material). A summary may be in order.\nA mixed-effects model incorporates fixed-effects parameters and random effects, which are unobserved random variables, \\(\\mcB\\). In a linear mixed model, both the unconditional distribution of \\(\\mcB\\) and the conditional distribution, \\((\\mcY|\\mcB=\\bbb)\\), are multivariate Gaussian distributions. Furthermore, this conditional distribution is a spherical Gaussian with mean, \\(\\bbmu\\), determined by the linear predictor, \\(\\bbZ\\bbb+\\bbX\\bbbeta\\). That is, \\[\n(\\mcY|\\mcB=\\bbb)\\sim\n\\mcN(\\bbZ\\bbb+\\bbX\\bbbeta, \\sigma^2\\bbI_n) .\n\\] The unconditional distribution of \\(\\mcB\\) has mean \\(\\mathbf{0}\\) and a parameterized \\(q\\times q\\) variance-covariance matrix, \\(\\Sigma_\\theta\\).\nIn the models we considered in this chapter, \\(\\Sigma_\\theta\\), is a scalar multiple of the identity matrix, \\(\\bbI_6\\). This matrix is always a multiple of the identity in models with just one random-effects term that is a simple, scalar term. The reason for introducing all the machinery that we did is to allow for more general model specifications.\nThe maximum likelihood estimates of the parameters are obtained by minimizing the deviance. For linear mixed models we can minimize the profiled deviance, which is a function of \\(\\bbtheta\\) only, thereby considerably simplifying the optimization problem.\nTo assess the precision of the parameter estimates we use a parametric bootstrap sample, when feasible. Re-fitting a simple model, such as those shown in this chapter, to a randomly generated response vector is quite fast and it is reasonable to work with bootstrap samples of tens of thousands of replicates. With larger data sets and more complex models, large bootstrap samples of parameter estimates could take much longer.\nPrediction intervals from the conditional distribution of the random effects, given the observed data, allow us to assess the precision of the random effects.\n\n\n\n\nBox, G. E. P., & Tiao, G. C. (1973). Bayesian inference in statistical analysis. Addison-Wesley.\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible high-performance data visualization for julia. Journal of Open Source Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical methods in research and production (4th ed.). Hafner.\n\n\nSakamoto, Y., Ishiguro, M., & Kitagawa, G. (1986). Akaike information criterion statistics (p. 290). Reidel.\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6, 461–464."
  },
  {
    "objectID": "multiple.html",
    "href": "multiple.html",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "",
    "text": "\\[\n\\newcommand\\bbA{{\\mathbf{A}}}\n\\newcommand\\bbb{{\\mathbf{b}}}\n\\newcommand\\bbI{{\\mathbf{I}}}\n\\newcommand\\bbL{{\\mathbf{L}}}\n\\newcommand\\bbR{{\\mathbf{R}}}\n\\newcommand\\bbX{{\\mathbf{X}}}\n\\newcommand\\bbx{{\\mathbf{x}}}\n\\newcommand\\bby{{\\mathbf{y}}}\n\\newcommand\\bbZ{{\\mathbf{Z}}}\n\\newcommand\\bbbeta{{\\boldsymbol{\\beta}}}\n\\newcommand\\bbeta{{\\boldsymbol{\\eta}}}\n\\newcommand\\bbLambda{{\\boldsymbol{\\Lambda}}}\n\\newcommand\\bbOmega{{\\boldsymbol{\\Omega}}}\n\\newcommand\\bbmu{{\\boldsymbol{\\mu}}}\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\newcommand\\bbtheta{{\\boldsymbol{\\theta}}}\n\\newcommand\\mcN{{\\mathcal{N}}}\n\\newcommand\\mcB{{\\mathcal{B}}}\n\\newcommand\\mcY{{\\mathcal{Y}}}\n\\]\nAttach the packages to be used in this chapter\nThe mixed models considered in the previous chapter had only one random-effects term, which was a simple, scalar random-effects term, and a single fixed-effects coefficient. Although such models can be useful, it is with the facility to use multiple random-effects terms and to use random-effects terms beyond a simple, scalar term that we can begin to realize the flexibility and versatility of mixed models.\nIn this chapter we consider models with multiple simple, scalar random-effects terms, showing examples where the grouping factors for these terms are in completely crossed or nested or partially crossed configurations. For ease of description we will refer to the random effects as being crossed or nested although, strictly speaking, the distinction between nested and non-nested refers to the grouping factors, not the random effects."
  },
  {
    "objectID": "multiple.html#sec-crossedre",
    "href": "multiple.html#sec-crossedre",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.1 A model with crossed random effects",
    "text": "2.1 A model with crossed random effects\nOne of the areas in which the methods in the MixedModels.jl package are particularly effective is in fitting models to cross-classified data where several factors have random effects associated with them. For example, in many experiments in psychology a reaction time for each of a group of subjects exposed to some or all of a group of stimuli or items is measured. If the subjects are considered to be a sample from a population of subjects and the items are a sample from a population of items, then it would make sense to associate random effects with both these factors.\nIn the past it was difficult to fit mixed models with multiple, crossed grouping factors to large, possibly unbalanced, data sets. The methods in MixedModels.jl are able to do this. To introduce the methods let us first consider a small, balanced data set with crossed grouping factors.\n\n2.1.1 The penicillin data\nThe data are derived from Table 6.6, p. 144 of Davies & Goldsmith (1972) where they are described as coming from an investigation to\n\nassess the variability between samples of penicillin by the B. subtilis method. In this test method a bulk-innoculated nutrient agar medium is poured into a Petri dish of approximately 90 mm. diameter, known as a plate. When the medium has set, six small hollow cylinders or pots (about 4 mm. in diameter) are cemented onto the surface at equally spaced intervals. A few drops of the penicillin solutions to be compared are placed in the respective cylinders, and the whole plate is placed in an incubator for a given time. Penicillin diffuses from the pots into the agar, and this produces a clear circular zone of inhibition of growth of the organisms, which can be readily measured. The diameter of the zone is related in a known way to the concentration of penicillin in the solution.\n\nAs with the dyestuff data, we examine the structure\n\npenicillin = MixedModels.dataset(:penicillin)\n\nArrow.Table with 144 rows, 3 columns, and schema:\n :plate     String\n :sample    String\n :diameter  Int8\n\n\nand a summary\n\npenicillin = DataFrame(penicillin)\ndescribe(penicillin)\n\n3 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1plateax0String2sampleAF0String3diameter22.97221823.0270Int8\n\n\nof the data, then plot it\n\n\nCode\n\"\"\"\n    _meanrespfrm(df, :resp::Symbol, :grps::Symbol; sumryf::Function=mean)\n\nReturns a `DataFrame` created from df with the levels of `grps` reordered according to\n`combine(groupby(df, grps), resp => sumryf)` and this summary DataFrame, also with the\nlevels of `grps` reordered.\n\"\"\"\nfunction _meanrespfrm(\n  df,\n  resp::Symbol,\n  grps::Symbol;\n  sumryf::Function=mean,\n)\n  # ensure the relevant columns are types that Makie can deal with \n  df = transform(\n    df,\n    resp => Array,\n    grps => CategoricalArray;\n    renamecols=false,\n  )\n  # create a summary table by mean resp\n  sumry =\n    sort!(combine(groupby(df, grps), resp => sumryf => resp), resp)\n  glevs = string.(sumry[!, grps])   # group levels in ascending order of mean resp\n  levels!(df[!, grps], glevs)\n  levels!(sumry[!, grps], glevs)\n  return df, sumry\nend\n\nlet\n  df, _ = _meanrespfrm(penicillin, :diameter, :plate)\n  sort!(df, [:plate, :sample])\n\n  mp = mapping(\n    :diameter => \"Diameter of inhibition zone [mm]\",\n    :plate => \"Plate\";\n    color=:sample,\n  )\n  draw(\n    data(df) * mp * visual(ScatterLines; marker='○', markersize=12),\n  )\nend\n\n\n\n\n\nFigure 2.1: Diameter of inhibition zone by plate and sample. Plates are ordered by increasing mean response.\n\n\n\n\nThe variation in the diameter is associated with the plates and with the samples. Because each plate is used only for the six samples shown here we are not interested in the contributions of specific plates as much as we are interested in the variation due to plates, and in assessing the potency of the samples after accounting for this variation. Thus, we will use random effects for the plate factor. We will also use random effects for the sample factor because, as in the dyestuff example, we are more interested in the sample-to-sample variability in the penicillin samples than in the potency of a particular sample.\nIn this experiment each sample is used on each plate. We say that the plate and sample factors are crossed, as opposed to nested factors, which we will describe in the next section. By itself, the designation “crossed” just means that the factors are not nested. If we wish to be more specific, we could describe these factors as being completely crossed, which means that we have at least one observation for each combination of a level of plate and a level of sample. We can see this in Figure 2.1. Alternatively, because there are moderate numbers of levels in these factors, we could also check by a cross-tabulation of these factors.\nLike the dyestuff data, the factors in the penicillin data are balanced. That is, there are exactly the same number of observations on each plate and for each sample and, furthermore, there is the same number of observations on each combination of levels. In this case there is exactly one observation for each combination of sample and plate. We would describe the configuration of these two factors as an unreplicated, completely balanced, crossed design.\nIn general, balance is a desirable but precarious property of a data set. We may be able to impose balance in a designed experiment but we typically cannot expect that data from an observation study will be balanced. Also, as anyone who analyzes real data soon finds out, expecting that balance in the design of an experiment will produce a balanced data set is contrary to Murphy’s law. That’s why statisticians allow for missing data. Even when we apply each of the six samples to each of the 24 plates, something could go wrong for one of the samples on one of the plates, leaving us without a measurement for that combination of levels and thus an unbalanced data set.\n\n\n2.1.2 A model for the penicillin data\nA model incorporating random effects for both the plate and the sample is straightforward to specify — we include simple, scalar random effects terms for both these factors.\n\npnm01 = let\n  contrasts = Dict(:plate => Grouping(), :sample => Grouping())\n  form = @formula(diameter ~ 1 + (1 | plate) + (1 | sample))\n  fit(MixedModel, form, penicillin; contrasts)\nend\n\n\u001b[32mMinimizing 45   Time: 0:00:00 (10.69 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_plate\nσ_sample\n\n\n\n\n(Intercept)\n22.9722\n0.7446\n30.85\n<1e-99\n0.8456\n1.7706\n\n\nResidual\n0.5499\n\n\n\n\n\n\n\n\n\n\nThis model display indicates that the sample-to-sample variability has the greatest contribution, then plate-to-plate variability and finally the “residual” variability that cannot be attributed to either the sample or the plate. These conclusions are consistent with what we see in the data plot (Figure 2.1).\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 480)),\n  ranefinfo(pnm01, :plate),\n)\n\n\n\n\n\nFigure 2.2: Conditional modes and 95% prediction intervals of random effects for plate in model pnm01\n\n\n\n\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 160)),\n  ranefinfo(pnm01, :sample),\n)\n\n\n\n\n\nFigure 2.3: Conditional modes and 95% prediction intervals of random effects for sample in model pnm01\n\n\n\n\nThe prediction intervals on the random effects (Figure 2.2 and Figure 2.3) confirm that the conditional distribution of the random effects for plate much less variability than does the conditional distribution of the random effects for sample. (Note that the horizontal scales on these two plots are different.) However, the conditional distribution of the random effect for a particular sample, say sample F, has less variability than the conditional distribution of the random effect for a particular plate, say plate m. That is, the lines in Figure 2.3 are wider than the lines in Figure 2.2, even after taking the different axis scales into account. This is because the conditional distribution of the random effect for a particular sample depends on 24 responses while the conditional distribution of the random effect for a particular plate depends on only 6 responses.\nIn Section 1 we saw that a model with a single, simple, scalar random-effects term generated a random-effects model matrix, \\(\\bbZ\\), that is the matrix of indicators of the levels of the grouping factor. When we have multiple, simple, scalar random-effects terms, as in model pnm01, each term generates a matrix of indicator columns and these sets of indicators are concatenated to form the model matrix \\(\\bbZ\\) whose transpose is\n\nvcat(transpose.(sparse.(pnm01.reterms))...)\n\n30×144 SparseArrays.SparseMatrixCSC{Float64, Int32} with 288 stored entries:\n⠉⠙⠒⠒⠒⠤⠤⠤⢄⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠒⠒⠒⠦⠤⠤⢤⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠓⠒⠒⠢⠤⠤⠤⣀⣀⣀⡀⠀\n⠑⢌⠢⡜⢦⠓⢌⠢⡘⢦⠓⢌⠢⡘⢦⠳⢌⠢⡑⢦⠳⢌⠢⡑⢦⠳⡌⠢⡑⢤⠳⡌⠢⡑⢤⠳⡜⠢⡙⢍\n\n\nThe relative covariance factor, \\(\\bbLambda_{\\bbtheta}\\), for this model is block diagonal, with two blocks, one of size 24 and one of size 6, each of which is a multiple of the identity. The diagonal elements of the two blocks are \\(\\theta_1\\) and \\(\\theta_2\\), respectively. The numeric values of these parameters can be obtained as\n\npnm01.θ'\n\n1×2 adjoint(::Vector{Float64}) with eltype Float64:\n 1.53758  3.21975\n\n\nThe first parameter is the relative standard deviation of the random effects for plate, which has the value \\(0.845565/0.549933=1.53758\\) at convergence, and the second is the relative standard deviation of the sample random effects (\\(1.770648/0.549933=3.21975\\)).\nBecause \\(\\bbLambda_{\\bbtheta}\\) is diagonal, the pattern of non-zeros in \\(\\bbLambda_\\bbtheta'\\bbZ'\\bbZ\\bbLambda_\\bbtheta+\\bbI\\) will be the same as that in \\(\\bbZ'\\bbZ\\). The sparse Cholesky factor, \\(\\bbL\\), is lower triangular and has non-zero elements in the lower right hand corner in positions where \\(\\bbZ'\\bbZ\\) has systematic zeros.\n\nsparseL(pnm01)\n\n30×30 SparseArrays.SparseMatrixCSC{Float64, Int32} with 189 stored entries:\n⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀\n⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠓\n\n\nWe say that “fill-in” has occurred when forming the sparse Cholesky decomposition. In this case there is a relatively minor amount of fill but in other cases there can be a substantial amount of fill. The computational methods are tuned to reduce the amount of fill.\n\n\n2.1.3 Precision of parameter estimates in the Pencillin model\nA parametric bootstrap sample of the parameter estimates\n\n\nCode\nbsrng = Random.seed!(9876789)\npnm01samp = parametricbootstrap(bsrng, 10_000, pnm01; hide_progress)\npnm01pars = DataFrame(pnm01samp.allpars);\n\n\ncan be used to create shortest 95% coverage intervals for the parameters in the model.\n\nDataFrame(shortestcovint(pnm01samp))\n\n4 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)21.499324.44022σplate(Intercept)0.5867661.09853σsample(Intercept)0.6275962.551064σresidualmissing0.4753310.61701\n\n\nAs for model dsm01 the bootstrap parameter estimates of the fixed-effects parameter have approximately a “normal” or Gaussian shape, as shown in the kernel density plot (Figure 2.4)\n\n\nCode\ndraw(\n  data(@subset(pnm01pars, :type == \"β\")) *\n  mapping(\n    :value => \"Bootstrap samples of β\";\n    color=(:names => \"Names\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.4: Parametric bootstrap estimates of fixed-effects parameters in model pnm01\n\n\n\n\nand the shortest coverage interval on this parameter is close to the Wald interval\n\n\nCode\nshow(only(pnm01.beta) .+ [-2, 2] * only(pnm01.stderror))\n\n\n[21.483030272252066, 24.461414172196214]\n\n\nThe densities of the variance-components, on the scale of the standard deviation parameters, are diffuse but do not exhibit point masses at zero.\n\n\nCode\ndraw(\n  data(@subset(pnm01pars, :type == \"σ\")) *\n  mapping(\n    :value => \"Bootstrap samples of σ\";\n    color=(:group => \"Group\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.5: Parametric bootstrap estimates of variance components in model pnm01\n\n\n\n\nThe lack of precision in the estimate of \\(\\sigma_2\\), the standard deviation of the random effects for sample, is a consequence of only having 6 distinct levels of the sample factor. The plate factor, on the other hand, has 24 distinct levels. In general it is more difficult to estimate a measure of spread, such as the standard deviation, than to estimate a measure of location, such as a mean, especially when the number of levels of the factor is small. Six levels are about the minimum number required for obtaining sensible estimates of standard deviations for simple, scalar random effects terms."
  },
  {
    "objectID": "multiple.html#sec-NestedRE",
    "href": "multiple.html#sec-NestedRE",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.2 A model with nested random effects",
    "text": "2.2 A model with nested random effects\nIn this section we again consider a simple example, this time fitting a model with nested grouping factors for the random effects.\n\n2.2.1 The pastes data\nThe third example from Davies & Goldsmith (1972, Table 6.5, p. 138) is described as coming from\n\ndeliveries of a chemical paste product contained in casks where, in addition to sampling and testing errors, there are variations in quality between deliveries …As a routine, three casks selected at random from each delivery were sampled and the samples were kept for reference. …Ten of the delivery batches were sampled at random and two analytical tests carried out on each of the 30 samples.\n\nThe structure and summary of the data object are\n\npastes = MixedModels.dataset(:pastes)\n\nArrow.Table with 60 rows, 3 columns, and schema:\n :batch     String\n :cask      String\n :strength  Float64\n\n\n\npastes =\n  @transform(DataFrame(pastes), :sample = string(:batch, :cask))\ndescribe(pastes)\n\n4 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1batchAJ0String2caskac0String3strength60.053354.259.366.00Float644sampleAaJc0String\n\n\nAs stated in the description in Davies & Goldsmith (1972), there are 30 samples, three from each of the 10 delivery batches. We have created a sample factor by concatenating the label of the batch factor with ‘a’, ‘b’ or ‘c’ to distinguish the three samples taken from that batch.\nWhen plotting the strength versus batch and cask in the data we should remember that we have two strength measurements on each of the 30 samples. It is tempting to use the cask designation (‘a’, ‘b’ and ‘c’) to determine, say, the plotting symbol within a batch. It would be fine to do this within a batch but the plot would be misleading if we used the same symbol for cask ‘a’ in different batches. There is no relationship between cask ‘a’ in batch ‘A’ and cask ‘a’ in batch ‘B’. The labels ‘a’, ‘b’ and ‘c’ are used only to distinguish the three samples within a batch; they do not have a meaning across batches.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=8, height=8)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\npp <- $pastes\npp <- within(pp, bb <- reorder(batch, strength))\nprint(\n  dotplot(sample ~ strength | bb, pp, pch = 21, strip = FALSE,\n    strip.left = TRUE, layout = c(1, 10),\n    scales = list(y = list(relation = \"free\")),\n    ylab = \"Sample within batch\", type = c(\"p\", \"a\"),\n    xlab = \"Paste strength\", jitter.y = TRUE)\n)\n\"\"\";\n\n\n\n\n\nFigure 2.6: Strength of paste preparations according to sample within batch\n\n\n\n\nIn Figure 2.6 we plot the two strength measurements on each of the samples within each of the batches and join up the average strength for each sample. The perceptive reader will have noticed that the levels of the factors on the vertical axis in this figure, and in Figure 1.1, and Figure 2.1, have been reordered according to increasing average response. In all these cases there is no inherent ordering of the levels of the covariate such as batch or plate. Rather than confuse our interpretation of the plot by determining the vertical displacement of points according to a random ordering, we impose an ordering according to increasing mean response. This allows us to more easily check for structure in the data, including undesirable characteristics like increasing variability of the response with increasing mean level of the response.\nIn Figure 2.6 we order the samples within each batch separately then order the batches according to increasing mean strength.\nFigure 2.6 shows considerable variability in strength between samples relative to the variability within samples. There is some indication of variability between batches, in addition to the variability induced by the samples, but not a strong indication of a batch effect. For example, batches I and D, with low mean strength relative to the other batches, each contained one sample (I:b and D:c, respectively) that had high mean strength relative to the other samples. Also, batches H and C, with comparatively high mean batch strength, contain samples H:a and C:a with comparatively low mean sample strength. In Section 2.2.4 we will examine the need for incorporating batch-to-batch variability, in addition to sample-to-sample variability, in the statistical model.\n\n\n2.2.2 Nested Factors\nBecause each level of sample occurs with one and only one level of batch we say that sample is nested within batch. Some presentations of mixed-effects models, especially those related to multilevel modeling (Rasbash et al., 2000) or hierarchical linear models (Raudenbush & Bryk, 2002), leave the impression that one can only define random effects with respect to factors that are nested. This is the origin of the terms “multilevel”, referring to multiple, nested levels of variability, and “hierarchical”, also invoking the concept of a hierarchy of levels. To be fair, both those references do describe the use of models with random effects associated with non-nested factors, but such models tend to be treated as a special case.\nThe blurring of mixed-effects models with the concept of multiple, hierarchical levels of variation results in an unwarranted emphasis on “levels” when defining a model and leads to considerable confusion. It is perfectly legitimate to define models having random effects associated with non-nested factors. The reasons for the emphasis on defining random effects with respect to nested factors only are that such cases do occur frequently in practice and that some of the computational methods for estimating the parameters in the models can only be easily applied to nested factors.\nThis is not the case for the methods used MixedModels.jl. Indeed there is nothing special done for models with random effects for nested factors. When random effects are associated with multiple factors exactly the same computational methods are used whether the factors form a nested sequence or are partially crossed or are completely crossed.\nThere is, however, one aspect of nested grouping factors that we should emphasize, which is the possibility of a factor that is implicitly nested within another factor. Suppose, for example, that the factor sample had been defined as having three levels instead of 30 with the implicit assumption that sample is nested within batch. It may seem silly to try to distinguish 30 different batches with only three levels of a factor but, unfortunately, data are frequently organized and presented like this, especially in text books. The factor cask in the data is exactly such an implicitly nested factor. If we cross-tabulate cask and batch we get the impression that these factors are crossed, not nested. If we know that the cask should be considered as nested within the batch then we should create a new categorical variable giving the batch-cask combination, which is exactly what the sample factor is.\nIn a small data set like we can quickly detect a factor being implicitly nested within another factor and take appropriate action. In a large data set, such as from a multi-center study, it is often assumed that, say, subject identifiers are unique to each center. Frequently this is not the case. Especially when dealing with large data sets, assumptions about nested identifiers should be checked carefully.\n\n\n2.2.3 Fitting a model with nested random effects\nFitting a model with simple, scalar random effects for nested factors is done in exactly the same way as fitting a model with random effects for crossed grouping factors. We include random-effects terms for each factor, as in\n\npsm01 = let\n  contrasts = Dict(:sample => Grouping(), :batch => Grouping())\n  form = @formula(strength ~ 1 + (1 | sample) + (1 | batch))\n  fit(MixedModel, form, pastes; contrasts)\nend\n\n\u001b[32mMinimizing 36   Time: 0:00:00 ( 5.72 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_sample\nσ_batch\n\n\n\n\n(Intercept)\n60.0533\n0.6421\n93.52\n<1e-99\n2.9041\n1.0951\n\n\nResidual\n0.8234\n\n\n\n\n\n\n\n\n\n\nNot only is the model specification similar for nested and crossed factors, the internal calculations are performed according to the methods described in for each model type. Comparing the patterns in the matrices \\(\\bbLambda\\), \\(\\bbZ'\\bbZ\\) and \\(\\bbL\\) and the block structure\n\nsparseL(psm01; full=true)\n\n42×42 SparseArrays.SparseMatrixCSC{Float64, Int32} with 153 stored entries:\n⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀\n⠤⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀\n⠀⠀⠀⠉⠑⠒⠤⢄⣀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠒⠤⢄⣀⠀⠀⠀⠑⢄⠀\n⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠛⠓\n\n\n\nBlockDescription(psm01)\n\n\n\n\nrows\nsample\nbatch\nfixed\n\n\n\n\n30\nDiagonal\n\n\n\n\n10\nSparse\nDiagonal\n\n\n\n2\nDense\nDense\nDense\n\n\n\n\n\nof psm01 to that of pnm01 shows that models with nested factors produce simple repeated structures along the diagonal of the sparse Cholesky factor, \\(\\bbL\\). This type of structure has the desirable property that there is no “fill-in” during calculation of the Cholesky factor. In other words, the number of non-zeros in \\(\\bbL\\) is the same as the number of non-zeros in the lower triangle of the matrix being factored, \\(\\bbLambda'\\bbZ'\\bbZ\\bbLambda+\\bbI\\) (which, because \\(\\bbLambda\\) is diagonal, has the same structure as \\(\\bbZ'\\bbZ\\)).\nFill-in of the Cholesky factor is not an important issue when we have a few dozen random effects, as we do here. It is an important issue when we have millions of random effects in complex configurations, as has been the case in some of the models that have been fit using MixedModels.\n\n\n2.2.4 Assessing parameter estimates in psm01\nThe parameter estimates are: \\(\\widehat{\\sigma_1}\\), the standard deviation of the random effects for sample; \\(\\widehat{\\sigma_2}\\), the standard deviation of the random effects for batch; \\(\\widehat{\\sigma}\\), the standard deviation of the residual noise term; and \\(\\widehat{\\beta_1}=\\), the overall mean response, which is labeled (Intercept) in these models.\nThe estimated standard deviation for sample is nearly three times as large as that for batch, which confirms what we saw in Figure 2.6. Indeed our conclusion from Figure 2.6 was that there may not be a significant batch-to-batch variability in addition to the sample-to-sample variability.\nPlots of the prediction intervals of the random effects (Figure 2.7)\n\n\nCode\ncaterpillar!(\n  Figure(resolution=(800, 300)),\n  ranefinfo(psm01, :batch),\n)\n\n\n\n\n\nFigure 2.7: Plot of batch prediction intervals from psm01\n\n\n\n\nconfirm this impression in that all the prediction intervals for the random effects for contain zero.\nFurthermore, kernel density estimates from a parametric bootstrap sample of the estimated standard deviations of the random effects and residuals\n\nRandom.seed!(4567654)\npsm01samp = parametricbootstrap(10_000, psm01; hide_progress)\npsm01pars = DataFrame(psm01samp.allpars);\n\n\n\nCode\ndraw(\n  data(@subset(psm01pars, :type == \"σ\")) *\n  mapping(\n    :value => \"Bootstrap samples of σ\";\n    color=(:group => \"Group\"),\n  ) *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.8: Kernel density plots of bootstrap estimates of σ for model psm01\n\n\n\n\nBecause there are several indications that \\(\\sigma_2\\) could reasonably be zero, resulting in a simpler model incorporating random effects for only, we perform a statistical test of this hypothesis.\n\n\n2.2.5 Testing \\(H_0:\\sigma_2=0\\) versus \\(H_a:\\sigma_2>0\\)\nOne of the many famous statements attributed to Albert Einstein is “Everything should be made as simple as possible, but not simpler.” In statistical modeling this principal of parsimony is embodied in hypothesis tests comparing two models, one of which contains the other as a special case. Typically, one or more of the parameters in the more general model, which we call the alternative hypothesis, is constrained in some way, resulting in the restricted model, which we call the null hypothesis. Although we phrase the hypothesis test in terms of the parameter restriction, it is important to realize that we are comparing the quality of fits obtained with two nested models. That is, we are not assessing parameter values per se; we are comparing the model fit obtainable with some constraints on parameter values to that without the constraints. Because the more general model, \\(H_a\\), must provide a fit that is at least as good as the restricted model, \\(H_0\\), our purpose is to determine whether the change in the quality of the fit is sufficient to justify the greater complexity of model \\(H_a\\). This comparison is often reduced to a p-value, which is the probability of seeing a difference in the model fits as large as we did, or even larger, when, in fact, \\(H_0\\) is adequate. Like all probabilities, a p-value must be between 0 and 1. When the p-value for a test is small (close to zero) we prefer the more complex model, saying that we “reject \\(H_0\\) in favor of \\(H_a\\)”. On the other hand, when the p-value is not small we “fail to reject \\(H_0\\)”, arguing that there is a non-negligible probability that the observed difference in the model fits could reasonably be the result of random chance, not the inherent superiority of the model \\(H_a\\). Under these circumstances we prefer the simpler model, \\(H_0\\), according to the principal of parsimony.\nThese are the general principles of statistical hypothesis tests. To perform a test in practice we must specify the criterion for comparing the model fits, the method for calculating the p-value from an observed value of the criterion, and the standard by which we will determine if the p-value is “small” or not. The criterion is called the test statistic, the p-value is calculated from a reference distribution for the test statistic, and the standard for small p-values is called the level of the test.\nIn Section 1.3.2 we referred to likelihood ratio tests (LRTs) for which the test statistic is the difference in the deviance. That is, the LRT statistic is \\(d_0-d_a\\) where \\(d_a\\) is the deviance in the more general (\\(H_a\\)) model fit and \\(d_0\\) is the deviance in the constrained (\\(H_0\\)) model. An approximate reference distribution for an LRT statistic is the \\(\\chi^2_\\nu\\) distribution where \\(\\nu\\), the degrees of freedom, is determined by the number of constraints imposed on the parameters of \\(H_a\\) to produce \\(H_0\\).\nThe restricted model fit\n\npsm02 = let\n  form = @formula(strength ~ 1 + (1 | sample))\n  contrasts = Dict(:sample => Grouping())\n  fit(MixedModel, form, pastes; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_sample\n\n\n\n\n(Intercept)\n60.0533\n0.5765\n104.16\n<1e-99\n3.1037\n\n\nResidual\n0.8234\n\n\n\n\n\n\n\n\n\nis compared to model psm01 as\n\nMixedModels.likelihoodratiotest(psm02, psm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nstrength ~ 1 + (1 | sample)\n3\n248\n\n\n\n\n\nstrength ~ 1 + (1 | sample) + (1 | batch)\n4\n248\n0\n1\n0.5234\n\n\n\n\n\nwhich provides a p-value of 52%. Because typical standards for “small” p-values are 5% or 1%, a p-value over 50% would not be considered significant at any reasonable level.\nWe do need to be cautious in quoting this p-value, however, because the parameter value being tested, \\(\\sigma_2=0\\), is on the boundary of set of possible values, \\(\\sigma_2\\ge 0\\), for this parameter. The argument for using a \\(\\chi^2_1\\) distribution to calculate a p-value for the change in the deviance does not apply when the parameter value being tested is on the boundary. As shown in Pinheiro & Bates (2000, Sect. 2.5), the p-value from the \\(\\chi^2_1\\) distribution will be “conservative” in the sense that it is larger than a simulation-based p-value would be. In the worst-case scenario the \\(\\chi^2\\)-based p-value will be twice as large as it should be but, even if that were true, an effective p-value of 26% would not cause us to reject \\(H_0\\) in favor of \\(H_a\\).\n\n\n2.2.6 Assessing the reduced model, psm02\nComparing the coverage intervals for models psm01 and psm02\n\n\nCode\nDataFrame(shortestcovint(psm01samp))\n\n\n4 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)58.773261.30252σsample(Intercept)1.934993.619493σbatch(Intercept)0.02.08944σresidualmissing0.6089041.02399\n\n\n\n\nCode\npsm02samp = parametricbootstrap(\n  Random.seed!(9753579),\n  10_000,\n  psm02;\n  hide_progress=true,\n)\nDataFrame(shortestcovint(psm02samp))\n\n\n3 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)58.920161.18862σsample(Intercept)2.249083.832073σresidualmissing0.611451.02324\n\n\nThe confidence intervals on \\(\\sigma\\) and \\(\\beta_0\\) are similar for the two models. The confidence interval on \\(\\sigma_1\\) is slightly wider and incorporates larger values in model psm02 than in model psm01, because the variability that is attributed to batch in psm01 is incorporated into the variability due to sample in psm02."
  },
  {
    "objectID": "multiple.html#sec-partially",
    "href": "multiple.html#sec-partially",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.3 A model with partially crossed random effects",
    "text": "2.3 A model with partially crossed random effects\nEspecially in observational studies with multiple grouping factors, the configuration of the factors frequently ends up neither nested nor completely crossed. We describe such situations as having partially crossed grouping factors for the random effects.\nStudies in education, in which test scores for students over time are also associated with teachers and schools, usually result in partially crossed grouping factors. If students with scores in multiple years have different teachers for the different years, the student factor cannot be nested within the teacher factor. Conversely, student and teacher factors are not expected to be completely crossed. To have complete crossing of the student and teacher factors it would be necessary for each student to be observed with each teacher, which would be unusual. A longitudinal study of thousands of students with hundreds of different teachers inevitably ends up partially crossed.\nIn this section we consider an example with thousands of students and instructors where the response is the student’s evaluation of the instructor’s effectiveness. These data, like those from most large observational studies, are quite unbalanced.\n\n2.3.1 The insteval data\nThe data are from a special evaluation of lecturers by students at the Swiss Federal Institute for Technology–Zürich (ETH–Zürich), to determine who should receive the “best-liked professor” award. These data have been slightly simplified and identifying labels have been removed, so as to preserve anonymity.\nThe variables\n\ninsteval = MixedModels.dataset(:insteval)\n\nArrow.Table with 73421 rows, 7 columns, and schema:\n :s        String\n :d        String\n :dept     String\n :studage  String\n :lectage  String\n :service  String\n :y        Int8\n\n\n\ninsteval = DataFrame(insteval)\ndescribe(insteval)\n\n7 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1sS0001S29720String2dI0001I21600String3deptD01D150String4studage280String5lectage160String6serviceNY0String7y3.2057413.050Int8\n\n\nhave somewhat cryptic names. Factor s designates the student and d the instructor. The factor dept is the department for the course and service indicates whether the course was a service course taught to students from other departments.\nAlthough the response, y, is on a scale of 1 to 5,\n\n\nCode\ndraw(\n  data(DataFrame(response=1:5, count=counts(insteval.y))) *\n  mapping(:response => \"Rating\", :count => \"Count\") *\n  visual(BarPlot);\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2.9: Histogram of instructor ratings in the insteval data\n\n\n\n\nit is sufficiently diffuse to warrant treating it as if it were a continuous response.\nAt this point we will fit models that have random effects for student, instructor, and department (or the combination of department and service) to these data.\n\ncontrasts =\n  Dict(:s => Grouping(), :d => Grouping(), :dept => Grouping())\niem01 = let\n  form = @formula(y ~ 1 + (1 | s) + (1 | d) + (1 | dept))\n  fit(MixedModel, form, insteval; contrasts)\nend\n\n\u001b[32mMinimizing 98   Time: 0:00:00 ( 7.93 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_s\nσ_d\nσ_dept\n\n\n\n\n(Intercept)\n3.2519\n0.0279\n116.70\n<1e-99\n0.3264\n0.5173\n0.0773\n\n\nResidual\n1.1777\n\n\n\n\n\n\n\n\n\n\n\nAll three estimated standard deviations of the random effects are less than \\(\\widehat{\\sigma}\\), with \\(\\widehat{\\sigma}_3\\), the estimated standard deviation of the random effects for the dept, less than one-tenth the estimated residual standard deviation.\nIt is not surprising that zero is within most of the prediction intervals on the random effects for this factor (Figure 2.10).\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 400)), ranefinfo(iem01, :dept))\n\n\n\n\n\nFigure 2.10: Prediction intervals on random effects for department in model iem01\n\n\n\n\nHowever, the p-value for the LRT of \\(H_0:\\sigma_3=0\\) versus \\(H_a:\\sigma_3>0\\)\n\niem02 = let\n  form = @formula(y ~ 1 + (1 | s) + (1 | d))\n  fit(MixedModel, form, insteval; contrasts)\nend\nMixedModels.likelihoodratiotest(iem02, iem01)\n\n\u001b[32mMinimizing 48   Time: 0:00:00 ( 6.76 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\ny ~ 1 + (1 | s) + (1 | d)\n4\n237778\n\n\n\n\n\ny ~ 1 + (1 | s) + (1 | d) + (1 | dept)\n5\n237770\n8\n1\n0.0043\n\n\n\n\n\nis highly significant. That is, we have very strong evidence that we should reject \\(H_0\\) in favor of \\(H_a\\).\nThe seeming inconsistency of these conclusions is due to the large sample size (\\(n=73421\\)). When a model is fit to a large sample even the most subtle of differences can be highly “statistically significant”. The researcher or data analyst must then decide if these terms have practical significance, beyond the apparent statistical significance.\nThe large sample size also helps to assure that the parameters have good normal approximations. We could profile this model fit but doing so would take a very long time and, in this particular case, the analysts are more interested in a model that uses fixed-effects parameters for the instructors.\n\n\n2.3.2 Structure of L for model iem01\nBefore leaving this model we examine the sparse Cholesky factor, \\(\\bbL\\), which is of size \\(4116\\times4116\\).\n\nsparseL(iem01; full=true)\n\n4116×4116 SparseArrays.SparseMatrixCSC{Float64, Int32} with 741328 stored entries:\n⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣶⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀\n⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄\n\n\nEven as a sparse matrix this factor requires a considerable amount of memory, but as a triangular dense matrix it would require nearly 10 times as much. There are \\((4116\\times 4117)/2\\) elements on and below the diagonal, each of which would require 8 bytes of storage.\n\n\n2.3.3 Effect of service courses\nIt is sometimes felt that it is more difficult to achieve favorable ratings from students in a service course (i.e. a course taught to students majoring in another program) as opposed to students taking a course in their major.\nThere are several ways in which service can be incorporated in a model like this. The simplest approach is to add service to the fixed-effects specification\n\niem03 = let\n  form = @formula(y ~ 1 + service + (1 | s) + (1 | d) + (1 | dept))\n  fit(MixedModel, form, insteval; contrasts)\nend\n\n\u001b[32mMinimizing 110      Time: 0:00:01 ( 9.68 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_s\nσ_d\nσ_dept\n\n\n\n\n(Intercept)\n3.2826\n0.0284\n115.55\n<1e-99\n0.3255\n0.5150\n0.0785\n\n\nservice: Y\n-0.0926\n0.0134\n-6.92\n<1e-11\n\n\n\n\n\nResidual\n1.1775\n\n\n\n\n\n\n\n\n\n\n\nIn model iem03 the effect of service is considered to be constant across departments and is modeled with a single fixed-effects parameter, which is the difference in a typical rating in a service course to a non-service course. This parameter also affects the interpretation of the (Intercept) coefficient. With the service term in the model the (Intercept) becomes a typical rating at the reference level (i.e. non-service or service: N) because the default coding for the service term is zero at the reference level and one for service: Y.\nThe coding can be changed by specifying a non-default contrast for service. For example, the EffectsCoding and HelmerCoding contrasts will both assign -1 to the first level (N in this case) and +1 to the second level (Y).\n\n\n2.3.4 “Best-liked”\nA qqcaterpillar plot of the instructor (i.e. factor d in the data) random effects, Figure 2.11,\n\n\nCode\nqqcaterpillar!(\n  Figure(; resolution=(800, 650)),\n  ranefinfo(iem01, :d),\n)\n\n\n\n\n\nFigure 2.11: Caterpillar plot of the instructor BLUPS for model iem01 versus standard normal quantiles\n\n\n\n\nshows the differences in precision due to different numbers of observations for different instructors.\n\n\nCode\ndraw(\n  data(combine(groupby(insteval, :d), nrow => :n)) *\n  mapping(:n => \"Number of observations\") *\n  AlgebraOfGraphics.histogram(; bins=410);\n  figure=(; resolution=(800, 600)),\n)\n\n\n\n\n\nFigure 2.12: Histogram of the number of observations per instructor in the insteval data\n\n\n\n\nThe precision of the conditional distributions of the random effects, as measured by the width of the intervals, varies considerably between instructors.\nWe can determine that instructor I1258 has the largest mean of the conditional distributions of the random effects\n\nlast(\n  sort(DataFrame(ranefinfotable(ranefinfo(iem01, :d))), :cmode),\n  5,\n)\n\n5 rows × 4 columnsnamelevelcmodecstddevStringStringFloat64Float641(Intercept)I00661.031510.1054722(Intercept)I01931.040270.1908263(Intercept)I08441.055020.1699144(Intercept)I18661.066250.1233215(Intercept)I12581.172050.188496\n\n\nbut the conditional distribution of this random effect clearly overlaps significantly with others."
  },
  {
    "objectID": "multiple.html#sec-MultSummary",
    "href": "multiple.html#sec-MultSummary",
    "title": "2  Models With Multiple Random-effects Terms",
    "section": "2.4 Chapter summary",
    "text": "2.4 Chapter summary\nA simple, scalar random effects term in an model formula is of the form , where is an expression whose value is the grouping factor of the set of random effects generated by this term. Typically, F is simply the name of a factor, as in most of the examples in this chapter. However, the grouping factor can be the value of an expression, such as in the last example.\nBecause simple, scalar random-effects terms can differ only in the description of the grouping factor we refer to configurations such as crossed or nested as applying to the terms or to the random effects, although it is more accurate to refer to the configuration as applying to the grouping factors.\nA model formula can include several such random effects terms. Because configurations such as nested or crossed or partially crossed grouping factors are a property of the data, the specification in the model formula does not depend on the configuration. We simply include multiple random effects terms in the formula specifying the model.\nOne apparent exception to this rule occurs with implicitly nested factors, in which the levels of one factor are only meaningful within a particular level of the other factor. In the pastes data, levels of the factor cask are only meaningful within a particular level of the factor batch. A model formula of would result in a fitted model that did not appropriately reflect the sources of variability in the data. Following the simple rule that the factor should be defined so that distinct experimental or observational units correspond to distinct levels of the factor will avoid such ambiguity.\nFor convenience, a model with multiple, nested random-effects terms can be specified as which internally is re-expressed as We will avoid terms of the form , preferring instead an explicit specification with simple, scalar terms based on unambiguous grouping factors.\nThe insteval data, described in Section 2.3.1, illustrate some of the characteristics of the real data to which mixed-effects models are now fit. There is a large number of observations associated with several grouping factors; two of which, student and instructor, have a large number of levels and are partially crossed. Such data are common in sociological and educational studies but until recently it has been very difficult to fit models that appropriately reflect such a structure. Much of the literature on mixed-effects models leaves the impression that multiple random effects terms can only be associated with nested grouping factors. The resulting emphasis on hierarchical or multilevel configurations is an artifact of the computational methods used to fit the models, not the models themselves.\nThe parameters of the models fit to small data sets have properties similar to those for the models in the previous chapter. That is, profile-based confidence intervals on the fixed-effects parameter, \\(\\beta_0\\), are symmetric about the estimate but overdispersed relative to those that would be calculated from a normal distribution and the logarithm of the residual standard deviation, \\(\\log(\\sigma)\\), has a good normal approximation. Profile-based confidence intervals for the standard deviations of random effects (\\(\\sigma_1\\), \\(\\sigma_2\\), etc.) are symmetric on a logarithmic scale except for those that could be zero.\nAnother observation from the last example is that, for data sets with a very large numbers of observations, a term in a model may be “statistically significant” even when its practical significance is questionable.\n\n\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical methods in research and production (4th ed.). Hafner.\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in S and S-PLUS. Springer.\n\n\nRasbash, J., Browne, W., Goldstein, H., Yang, M., & Plewis, I. (2000). A user’s guide to MLwiN. Multilevel Models Project, Institute of Education, University of London.\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (2nd ed.). Sage."
  },
  {
    "objectID": "longitudinal.html",
    "href": "longitudinal.html",
    "title": "3  Models for Longitudinal Data",
    "section": "",
    "text": "\\[\n\\newcommand\\bbA{{\\mathbf{A}}}\n\\newcommand\\bbb{{\\mathbf{b}}}\n\\newcommand\\bbI{{\\mathbf{I}}}\n\\newcommand\\bbR{{\\mathbf{R}}}\n\\newcommand\\bbX{{\\mathbf{X}}}\n\\newcommand\\bbx{{\\mathbf{x}}}\n\\newcommand\\bby{{\\mathbf{y}}}\n\\newcommand\\bbZ{{\\mathbf{Z}}}\n\\newcommand\\bbbeta{{\\boldsymbol{\\beta}}}\n\\newcommand\\bbeta{{\\boldsymbol{\\eta}}}\n\\newcommand\\bbLambda{{\\boldsymbol{\\Lambda}}}\n\\newcommand\\bbOmega{{\\boldsymbol{\\Omega}}}\n\\newcommand\\bbmu{{\\boldsymbol{\\mu}}}\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\newcommand\\bbtheta{{\\boldsymbol{\\theta}}}\n\\newcommand\\mcN{{\\mathcal{N}}}\n\\newcommand\\mcB{{\\mathcal{B}}}\n\\newcommand\\mcY{{\\mathcal{Y}}}\n\\]\nLoad the packages to be used.\nLongitudinal data consist of repeated measurements on the same subject, or some other observational unit, taken over time. Generally we wish to characterize the time trends within subjects and between subjects. The data will always include the response, the time covariate and the indicator of the subject on which the measurement has been made. If other covariates are recorded, say whether the subject is in the treatment group or the control group, we may wish to relate the within- and between-subject trends to such covariates.\nIn this chapter we introduce graphical and statistical techniques for the analysis of longitudinal data by applying them to a simple example."
  },
  {
    "objectID": "longitudinal.html#the-elstongrizzle-data",
    "href": "longitudinal.html#the-elstongrizzle-data",
    "title": "3  Models for Longitudinal Data",
    "section": "3.1 The elstongrizzle data",
    "text": "3.1 The elstongrizzle data\nData from a dental study measuring the lengths of the ramus bone (mm) in 20 boys at 8, 8.5, 9, and 9.5 years of age were reported in Elston & Grizzle (1962) and in Davis (2002).\n\nelstongrizzle = Arrow.Table(\"./data/elstongrizzle.arrow\")\n\nArrow.Table with 80 rows, 3 columns, and schema:\n :Subj  String\n :time  Float64\n :resp  Float64\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"      => \"Ramus bone lengths of boys from 8 to 9.5 years of age\"\n  \"references\" => \"@davis2002, Table 3.1, p. 52 and @elstongrizzle1962\"\n  \"sourcefile\" => \"EG.DAT\"\n\n\nConverting the table to a data frame provides the description\n\n\n3 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS01S200String2time8.758.08.759.50Float643resp50.087545.049.6555.50Float64\n\n\nA common way of plotting such longitudinal data is response versus time on a single axis with the observations for each individual joined by a line, Figure 3.1 (see also Figure 3.2, p. 52 of Davis (2002)).\n\n\nCode\negplt =\n  data(egdf) *\n  mapping(:time => \"Age (yr)\", :resp => \"Ramus bone length (mm)\")\ndraw(\n  egplt *\n  mapping(; color=:Subj) *\n  (visual(Scatter) + visual(Lines)),\n)\n\n\n\n\n\nFigure 3.1: Length of ramus bone versus age for a sample of 20 boys.\n\n\n\n\nUnfortunately, unless there are very few subjects, such figures, sometimes called “spaghetti plots”, are difficult to decipher.\nA preferred alternative is to plot response versus time with each subject’s data in a separate panel (Figure 3.2).\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=8, height=7)\nR\"\"\"\nprint(\n  lattice::xyplot(\n    resp ~ time|Subj,\n    $egdf,\n    type=c(\"g\",\"p\",\"r\"),\n    aspect=\"xy\",\n    index.cond=function(x,y) coef(lm(y ~ x)) %*% c(1,8),\n    xlab=\"Age (yr)\",\n    ylab=\"Ramus bone length (mm)\",\n    ),\n  )\n\"\"\";\n\n\n\n\n\nFigure 3.2: Length of ramus bone versus age for a sample of 20 boys. The panels are ordered rowwise, starting at the bottom left, by increasing bone length at age 8.\n\n\n\n\nTo aid comparisons between subjects the axes are the same in every panel and the order of the panels is chosen systematically - in Figure 3.2 the order is by increasing bone length at 8 years of age. This ordering makes it easier to examine the patterns in the rate of increase versus the initial bone length.\nAnd there doesn’t seem to be a strong relationship. Some subjects, e.g. S03 and S04, with shorter initial bone lengths have low growth rates. Others, e.g. S10 and S20, have low initial bone lengths and a high growth rate. Similarly, S06 and S11 have longer initial bone lengths and a low growth rate while S07 and S11 have longer initial bone lengths and a high growth rate."
  },
  {
    "objectID": "longitudinal.html#random-effects-for-slope-and-intercept",
    "href": "longitudinal.html#random-effects-for-slope-and-intercept",
    "title": "3  Models for Longitudinal Data",
    "section": "3.2 Random effects for slope and intercept",
    "text": "3.2 Random effects for slope and intercept\nAlthough it seems that there isn’t a strong correlation between initial bone length and growth rate in these data, a model with an overall linear trend and possibly correlated random effects for intercept and slope by subject estimates a strong negative correlation (-0.97) between these random effects.\n\ncontrasts[:Subj] = Grouping()\negm01 = let\n  form = @formula(resp ~ 1 + time + (1 + time | Subj))\n  fit(MixedModel, form, egdf; contrasts)\nend\nprintln(egm01)\n\n\u001b[32mMinimizing 236      Time: 0:00:00 ( 0.86 ms/it)\u001b[39m\n\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time + (1 + time | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n\n\n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n            Column    Variance Std.Dev.   Corr.\nSubj     (Intercept)  90.337725 9.504616\n         time          1.162283 1.078092 -0.97\nResidual               0.194451 0.440966\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n\n\n─────────────────────────────────────────────────\n               Coef.  Std. Error      z  Pr(>|z|)\n─────────────────────────────────────────────────\n(Intercept)  33.4975    2.2616    14.81    <1e-48\ntime          1.896     0.256695   7.39    <1e-12\n─────────────────────────────────────────────────\n\n\nThe reason for this seemingly unlikely result is that the (Intercept) term in the fixed effects and the random effects represents the bone length at age 0, which is not of interest here. Notice that the fixed-effects (Intercept) estimate is about 33.5 mm, which is far below the observed range of the data (45.0 to 55.5 mm.)\nExtrapolation from the observed range of ages, 8 years to 9.5 years, back to 0 years, will almost inevitably result is a negative correlation between slope and intercept.\nA caterpillar plot of the random effects for intercept and slope, Figure 3.3, shows both the negative correlation between intercept and slope conditional means and wide prediction intervals on the random effects for the intercept.\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 500)), ranefinfo(egm01, :Subj))\n\n\n\n\n\nFigure 3.3: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model egm01\n\n\n\n\n\n3.2.1 Centering the time values\nThe problem of estimates of intercepts representing extrapolation beyond the observed range of the data is a common one for longitudinal data. If the time covariate represents an age, as it does here, or, say, a year in the range 2000 to 2020, the intercept, which corresponds to an age or year of zero, is rarely of interest.\nThe way to avoid extrapolation beyond the range of the data is to center the time covariate at an age or date that is of interest. For example, we may wish to consider “time in study” instead of age as the time covariate.\nIn discussing Figure 3.2 we referred to the bone length at 8 years of age, which was the time of first measurement for each of the subjects, as the “initial” bone length. If the purpose of the experiment is to create a predictive model for the growth rate that can be applied to boys who enter this age range then we could center the time at 8 years.\nAlternatively, we could center at the average observed time, 8.75 years, or at some other value of interest.\nThe important thing is to make clear what the (Itercept) parameter estimates represent. The StandardizedPredictors.jl package allows for convenient representations of several standardizing transformations in a contrasts specification for the model. An advantage of this method of coding a transformation is that the coefficient names include a concise description of the transformation.\n(In model specifications in R and later in Julia the name contrasts has been come to be applied to ways of specifying the association between covariates in the data and parameters in a model. This is an extension of the original mathematical definition of contrasts amongst the levels of a categorical covariate.)\nA model with time centered at 8 years of age can be fit as\n\ncontrasts[:time] = Center(8)\negm02 = let\n  form = @formula(resp ~ 1 + time + (1 + time | Subj))\n  fit(MixedModel, form, egdf; contrasts)\nend\nprintln(egm02)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time(centered: 8) + (1 + time(centered: 8) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n               Column      Variance Std.Dev.   Corr.\nSubj     (Intercept)        6.096574 2.469124\n         time(centered: 8)  1.162295 1.078098 -0.23\nResidual                    0.194452 0.440967\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n───────────────────────────────────────────────────────\n                     Coef.  Std. Error      z  Pr(>|z|)\n───────────────────────────────────────────────────────\n(Intercept)        48.6655    0.558242  87.18    <1e-99\ntime(centered: 8)   1.896     0.256696   7.39    <1e-12\n───────────────────────────────────────────────────────\n\n\nComparing the parameter estimates from models egm01 and egm02, we find that the only differences are in the estimates for the (Intercept) terms in the fixed-effects parameters and the variance component parameters and in the correlation of the random effects. In terms of the predictions from the model and the likelihood at the parameter estimates, egm01 and egm02 are the same model.\nA caterpillar plot, Figure 3.4, for egm02 shows much smaller spread and more precision in the distribution of the random effects for (Intercept) but the same spread and precision for the time random effects, although these random effects are displayed in a different order.\n\n\nCode\ncaterpillar!(Figure(resolution=(800, 550)), ranefinfo(egm02, :Subj))\n\n\n\n\n\nFigure 3.4: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model egm02\n\n\n\n\nA third option is to center the time covariate at the mean of the original time values.\n\ncontrasts[:time] = Center()\negm03 = let\n  form = @formula(resp ~ 1 + time + (1 + time | Subj))\n  fit(MixedModel, form, egdf; contrasts)\nend\nprintln(egm03)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + time(centered: 8.75) + (1 + time(centered: 8.75) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -117.2404   234.4809   246.4809   247.6316   260.7730\n\nVariance components:\n\n\n\n                Column        Variance Std.Dev.   Corr.\nSubj     (Intercept)           5.826532 2.413821\n         time(centered: 8.75)  1.162282 1.078092 +0.10\nResidual                       0.194452 0.440967\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────────────\n                        Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────────────\n(Intercept)           50.0875    0.541994  92.41    <1e-99\ntime(centered: 8.75)   1.896     0.256695   7.39    <1e-12\n──────────────────────────────────────────────────────────\n\n\nBecause the default for the Center contrast is to center about the mean, this contrasts argument can be written\nNotice that in this model the estimated correlation of the random effects for Subj is positive."
  },
  {
    "objectID": "longitudinal.html#shrinkage-plots",
    "href": "longitudinal.html#shrinkage-plots",
    "title": "3  Models for Longitudinal Data",
    "section": "3.3 Shrinkage plots",
    "text": "3.3 Shrinkage plots\nOne way of assessing a random-effects term in a linear mixed model is with a caterpillar plot, which shows two important characteristics, location and spread, of the conditional distribution \\((\\mcB|\\mcY=\\bby)\\).\nAnother plot of interest is to show the extent to which the conditional means have been “shrunk” towards the origin by the mixed-model, which represents a compromise between fidelity to the data, measured by the sum of squared residuals, and simplicity of the model.\nThe model with the highest fidelity to the data corresponds to a fixed-effects model with the random effects model matrix, \\(\\bbZ\\), incorporated into the fixed-effects model matrix, \\(\\bbX\\). There are technical problems (rank deficiency) with trying to estimate parameters in this model but such “unconstrained” random effects can be approximated as the conditional means for a very large \\(\\bbSigma\\) matrix. (In practice we use a large multiple, say 1000, of the identity matrix as the value of \\(\\bbSigma\\).)\nAt the other end of the spectrum is the limit as \\(\\bbSigma\\rightarrow\\mathbf{0}\\), which is the simplest model, involving only the fixed-effects parameters, but usually with a comparatively poor fit.\nA shrinkage plot shows the conditional means of the random effects from the model that was fit and those for a “large” \\(\\bbSigma\\).\n\n\nCode\nshrinkageplot!(Figure(; resolution=(600, 600)), egm02)\n\n\n\n\n\nFigure 3.5: Shrinkage plot of the random effects for model egm02. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nFigure 3.5 reinforces some of the conclusions from Figure 3.4. In particular, the random effects for the (Intercept) are reasonably precisely determined. We see this in Figure 3.4 because the intervals in the left panel are narrow. In Figure 3.5 there is little movement in the horizontal direction between the “unconstrained” within-subject estimates and the final random-effect locations."
  },
  {
    "objectID": "longitudinal.html#nonlinear-growth-curves",
    "href": "longitudinal.html#nonlinear-growth-curves",
    "title": "3  Models for Longitudinal Data",
    "section": "3.4 Nonlinear growth curves",
    "text": "3.4 Nonlinear growth curves\nAs seen in Figure 3.2 some of the growth curves are reasonably straight (e.g. S03, S11, and S15) whereas others are concave-up (e.g. S04, S13, and S17) or concave-down (e.g. S05, S07, and S19). One way of allowing for curvature in individual growth curves is to include a quadratic term for time in both the fixed and random effects.\nThe usual cautions about polynomial terms in regression models apply even more emphatically to linear mixed models.\n\nInterpretation of polynomial coefficients depends strongly upon the location of the zero point in the time axis.\nExtrapolation of polynomial models beyond the observed range of the time values is very risky.\n\nFor balanced data like egdf we usually center the time axis about the mean, producing\n\negm04 = let\n  form = @formula(\n    resp ~ 1 + ctime + ctime^2 + (1 + ctime + ctime^2 | Subj)\n  )\n  dat = @transform(egdf, :ctime = :time - 8.75)\n  fit(MixedModel, form, dat; contrasts)\nend\nprintln(egm04)\n\nLinear mixed model fit by maximum likelihood\n resp ~ 1 + ctime + :(ctime ^ 2) + (1 + ctime + :(ctime ^ 2) | Subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -116.6187   233.2374   253.2374   256.4258   277.0577\n\nVariance components:\n\n\n\n            Column   Variance Std.Dev.   Corr.\nSubj     (Intercept)  6.048887 2.459449\n         ctime        1.168134 1.080802 +0.08\n         ctime ^ 2    0.056772 0.238268 -0.62 +0.65\nResidual              0.187157 0.432616\n Number of obs: 80; levels of grouping factors: 20\n\n  Fixed-effects parameters:\n────────────────────────────────────────────────\n              Coef.  Std. Error      z  Pr(>|z|)\n────────────────────────────────────────────────\n(Intercept)  50.1      0.555373  90.21    <1e-99\nctime         1.896    0.256696   7.39    <1e-12\nctime ^ 2    -0.04     0.200674  -0.20    0.8420\n────────────────────────────────────────────────\n\n\nWe see that the estimate for the population quadratic coefficient, -0.04, is small, relative to its standard error, 0.20, indicating that it is not significantly different from zero. This is not unexpected because some of the growth curves in Figure 3.2 are concave-up, while others are concave-down, and others don’t show a noticeable curvature.\nA shrinkage plot, Figure 3.6, shows that the random effects for the quadratic term (vertical axis in the bottom row of panels) are highly attenuated relative to the unconstrained, “per-subject”, values.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), egm04)\n\n\n\n\n\nFigure 3.6: Shrinkage plot of the random effects for model egm04. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nBoth of these results lead to the conclusion that linear growth, over the observed range of ages, should be adequate - a conclusion reinforced by a likelihood ratio test.\n\nMixedModels.likelihoodratiotest(egm03, egm04)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nresp ~ 1 + time(centered: 8.75) + (1 + time(centered: 8.75) | Subj)\n6\n234\n\n\n\n\n\nresp ~ 1 + ctime + :(ctime ^ 2) + (1 + ctime + :(ctime ^ 2) | Subj)\n10\n233\n1\n4\n0.8709"
  },
  {
    "objectID": "longitudinal.html#longitudinal-data-with-treatments",
    "href": "longitudinal.html#longitudinal-data-with-treatments",
    "title": "3  Models for Longitudinal Data",
    "section": "3.5 Longitudinal data with treatments",
    "text": "3.5 Longitudinal data with treatments\nOften the “subjects” on which longitudinal measurements are made are divided into different treatment groups. Many of the examples cited in Davis (2002) are of this type, including one from Box (1950)\n\nbox1950 = Arrow.Table(\"./data/box.arrow\")\n\nArrow.Table with 135 rows, 4 columns, and schema:\n :Group  String\n :Subj   String\n :time   Int8\n :resp   Int16\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"      => \"Body weight by week of rats in three treatment groups\"\n  \"references\" => \"@davis2002, Table 2.12, p. 32 and Problem 2.4, p. 32, and @b…\n  \"sourcefile\" => \"BOX.DAT\"\n\n\n\nbxdf = DataFrame(box1950)\ndescribe(bxdf)\n\n4 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1GroupControlThyroxin0String2SubjS01S270String3time2.002.040Int84resp100.80746100.01890Int16\n\n\nThere are three treatment groups\n\nshow(levels(bxdf.Group))\n\n[\"Control\", \"Thioracil\", \"Thyroxin\"]\n\n\nand each “subject” (rat, in this case) is in only one of the treatment groups. This can be checked by comparing the number of unique Subj levels to the number of unique combinations of Subj and Group.\n\nnrow(unique(select(bxdf, :Group, :Subj))) ==\nlength(unique(bxdf.Subj))\n\ntrue\n\n\nBecause the number of combinations of Subj and Group is equal to the number of subjects, each subject occurs in only one group.\nThese data are balanced with respect to time (i.e. each rat is weighed at the same set of times) but not with respect to treatment, as can be seen by checking the number of rats in each treatment group.\n\ncombine(\n  groupby(unique(select(bxdf, :Group, :Subj)), :Group),\n  nrow => :n,\n)\n\n3 rows × 2 columnsGroupnStringInt641Control102Thioracil103Thyroxin7\n\n\n\n3.5.1 Within-group variation\nConsidering first the control group, whose trajectories can be plotted in a “spaghetti plot”, Figure 3.7\n\n\nCode\nbxaxes =\n  mapping(:time => \"Time in trial (wk)\", :resp => \"Body weight (g)\")\ndraw(\n  data(@subset(bxdf, :Group == \"Control\")) *\n  bxaxes *\n  mapping(; color=:Subj) *\n  (visual(Scatter) + visual(Lines)),\n)\n\n\n\n\n\nFigure 3.7: Weight (g) of rats in the control group of bxdf versus time in trial (wk).\n\n\n\n\nor in separate panels ordered by initial weight, Figure 3.8.\n\n\nCode\nlet\n  df = @subset(bxdf, :Group == \"Control\")\n  df.orderedsubj = CategoricalArray(\n    df.Subj;\n    levels=sort(@subset(df, iszero(:time)), :resp).Subj,\n    ordered=true,\n  )\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(; layout=:orderedsubj) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.8: Weight (g) of rats in the control group versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nThe panels in Figure 3.8 show a strong linear trend with little evidence of systematic curvature.\nA multi-panel plot for the Thioracil group, Figure 3.9,\n\n\nCode\nlet\n  df = @subset(bxdf, :Group == \"Thioracil\")\n  df.orderedsubj = CategoricalArray(\n    df.Subj;\n    levels=sort(@subset(df, iszero(:time)), :resp).Subj,\n    ordered=true,\n  )\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(; layout=:orderedsubj) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.9: Weight (g) of rats in the Thioracil group versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nshows several animals (S18, S19, S21, and S24) whose rate of weight gain decreases as the trial goes on.\nBy contrast, in the Thyroxin group, Figure 3.10,\n\n\nCode\nlet\n  df = @subset(bxdf, :Group == \"Thyroxin\")\n  df.orderedsubj = CategoricalArray(\n    df.Subj;\n    levels=sort(@subset(df, iszero(:time)), :resp).Subj,\n    ordered=true,\n  )\n  draw(\n    data(df) *\n    bxaxes *\n    mapping(; layout=:orderedsubj) *\n    visual(ScatterLines);\n    axis=(height=250, width=150),\n  )\nend\n\n\n\n\n\nFigure 3.10: Weight (g) of rats in the Thyroxingroup versus time in trial (wk). Panels are ordered by increasing initial weigth.\n\n\n\n\nif there is any suggestion of curvature it would be concave-up.\n\n\n3.5.2 Models with interaction terms\nLongitudinal data in which the observational units, each rat in this case, are in different treatment groups, require careful consideration of the origin on the time axis. If, as here, the origin on the time axis is when the treatments of the different groups began and the subjects have been randomly assigned to the treatment groups, we do not expect differences between groups at time zero.\nUsually, when a model incorporates an effect for time and a time & Group interaction - checking for different underlying slopes of the response with respect to time for each level of Group, we will also include a “main effect” for Group. This is sometimes called the hierarchical principle regarding interactions - a significant higher-order interaction usually forces inclusion of any lower-order interactions or main effects contained in it.\nOne occasion where the hierarchical principle does not apply is when the main effect for Group would represent systematic differences in the response before the treatments began. Similarly in dose-response data; when a zero dose is included we could have a main effect for dose and a dose & Group interaction without a main effect for Group, because zero dose of a treatment is the same as zero dose of a placebo.\nWe can begin with a main effect for Group, as in\n\ndelete!(contrasts, :time)\nbxm01 = let\n  form = @formula(\n    resp ~\n      (1 + time + time^2) * Group + (1 + time + time^2 | Subj)\n  )\n  fit(MixedModel, form, bxdf; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.1086\n1.5368\n35.21\n<1e-99\n4.0286\n\n\ntime\n24.0229\n1.5639\n15.36\n<1e-52\n3.7538\n\n\ntime ^ 2\n0.6143\n0.3988\n1.54\n0.1235\n0.9973\n\n\nGroup: Thioracil\n0.9086\n2.1733\n0.42\n0.6759\n\n\n\nGroup: Thyroxin\n0.6302\n2.3949\n0.26\n0.7924\n\n\n\ntime & Group: Thioracil\n-1.6271\n2.2116\n-0.74\n0.4619\n\n\n\ntime & Group: Thyroxin\n-2.1861\n2.4371\n-0.90\n0.3697\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.9357\n0.5640\n-3.43\n0.0006\n\n\n\ntime ^ 2 & Group: Thyroxin\n0.7122\n0.6215\n1.15\n0.2518\n\n\n\nResidual\n2.8879\n\n\n\n\n\n\n\n\n\nbut we expect that a model without the main effect for Group,\n\nbxm02 = let\n  form = @formula(\n    resp ~\n      1 + (time + time^2) & Group + (1 + time + time^2 | Subj)\n  )\n  fit(MixedModel, form, bxdf; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n0.9382\n58.21\n<1e-99\n4.0466\n\n\ntime & Group: Control\n24.1725\n1.5206\n15.90\n<1e-56\n\n\n\ntime & Group: Thioracil\n22.2734\n1.5206\n14.65\n<1e-47\n\n\n\ntime & Group: Thyroxin\n21.7977\n1.8081\n12.06\n<1e-32\n\n\n\ntime ^ 2 & Group: Control\n0.5655\n0.3806\n1.49\n0.1374\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.2815\n0.3806\n-3.37\n0.0008\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.3393\n0.4510\n2.97\n0.0030\n\n\n\ntime\n\n\n\n\n3.7538\n\n\ntime ^ 2\n\n\n\n\n0.9977\n\n\nResidual\n2.8887\n\n\n\n\n\n\n\n\n\nwill be adequate, as confirmed by\n\nMixedModels.likelihoodratiotest(bxm02, bxm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nresp ~ 1 + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n14\n836\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) + Group + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n16\n835\n0\n2\n0.9135\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nUnfortunately, the interpretations of some of the fixed-effects coefficients change between models bxm01 and bxm02. In model bxm01 the coefficient labelled time is the estimated slope at time zero for the Control group. In model bxm02 this coefficient is labelled time & Group: Control.\nIn model bxm01 the coefficient labelled time & Group: Thioracil is the change in the estimated slope at time zero between the Thioracil group and the Control group. In model bxm02 the coefficient with this label is the estimated slope at time zero in the Thioracil group.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIs there a way to write the formula for bxm02 to avoid this?\n\n\nWe see the effect of this changing interpretation in the p-values associated with these coefficients. In model bxm01 the only coefficients with low p-values are the (Intercept), the time, representing a typical rate of weight gain in the control group at time zero, and the change in the quadratic term from the Control group to the Thioracil group.\nA model without systematic differences between groups in the initial weight and the initial slope but with differences between groups in the quadratic coefficient is sensible. It would indicate that the groups are initially homogeneous both in weight and growth rate but, as the trial proceeds, the different treatments change the rate of growth.\n\nbxm03 = let\n  form = @formula(\n    resp ~ 1 + time + time^2 & Group + (1 + time + time^2 | Subj)\n  )\n  fit(MixedModel, form, bxdf; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n0.9349\n58.41\n<1e-99\n4.0129\n\n\ntime\n22.8534\n0.9626\n23.74\n<1e-99\n3.8079\n\n\ntime ^ 2 & Group: Control\n0.7854\n0.3240\n2.42\n0.0154\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.3781\n0.3240\n-4.25\n<1e-04\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1630\n0.3691\n3.15\n0.0016\n\n\n\ntime ^ 2\n\n\n\n\n0.9954\n\n\nResidual\n2.9094\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm03, bxm02)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n\n\n\n\n\nresp ~ 1 + time & Group + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n14\n836\n1\n2\n0.5328\n\n\n\n\n\n\n\n3.5.3 Possible simplification of random effects\nA caterpillar plot created from the conditional means and standard deviations of the random effects by Subj for model bxm03, Figure 3.11, indicates that all of the random-effects terms generate some prediction intervals that do not contain zero.\n\n\nCode\ncaterpillar(bxm03)\n\n\n\n\n\nFigure 3.11: Conditional modes and 95% prediction intervals on the random effects for slope and intercept in model bxm03\n\n\n\n\nA shrinkage plot, Figure 3.12\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), bxm03)\n\n\n\n\n\nFigure 3.12: Shrinkage plot of the random effects for model bxm03. Blue dots are the conditional means of the random effects at the parameter estimates. Red dots are the corresponding unconstrained estimates.\n\n\n\n\nshows that the random effects are considerably shrunk towards the origin, relative to the “unconstrained” values from within-subject fits, but none of the panels shows them collapsing to a line.\nNevertheless, the estimated unconditional distribution of the random effects in model bxm03 is a degenerate distribution.\nThat is, the estimated within-subject covariance of the random effects is singular.\n\nissingular(bxm03)\n\ntrue\n\n\nOne way to see this is because the relative covariance factor, \\(\\boldsymbol{\\lambda}\\), of the within-subject random effects is\n\nMatrix(only(bxm03.λ))\n\n3×3 Matrix{Float64}:\n  1.37931   0.0       0.0\n  1.15547   0.614804  0.0\n -0.301045  0.162566  0.0\n\n\nand we see that the third column is all zeros.\nThus, in a three-dimensional space the conditional means of the random effects for each rat, lie on a plane, even though each of the two-dimensional projections in Figure 3.12 show a scatter.\nThree-dimensional plots of the conditional means of the random effects, Figure 3.13, can help to see that these points lie on a plane.\n\n\nCode\nlet\n  bpts = Point3f.(eachcol(only(bxm03.b)))\n  Upts = Point3f.(eachcol(svd(only(bxm03.λ)).U))\n  origin = Point3(zeros(Float32, 3))\n  xlabel, ylabel, zlabel = only(bxm03.reterms).cnames\n  zlabel = \"time²\"\n  perspectiveness = 0.5\n  aspect = :data\n  f = Figure(; resolution=(1200, 500))\n  u, v, w = -Upts[2]    # second principle direction flipped to get positive w\n  elevation = asin(w)\n  azimuth = atan(v, u)\n  ax1 =\n    Axis3(f[1, 1]; aspect, xlabel, ylabel, zlabel, perspectiveness)\n  ax2 = Axis3(\n    f[1, 2];\n    aspect,\n    xlabel,\n    ylabel,\n    zlabel,\n    perspectiveness,\n    elevation,\n    azimuth,\n  )\n  scatter!(ax1, bpts; marker='∘', markersize=20)\n  scatter!(ax2, bpts; marker='∘', markersize=20)\n  for p in Upts\n    seg = [origin, p]\n    lines!(ax1, seg)\n    lines!(ax2, seg)\n  end\n  f\nend\n\n\n\n\n\nFigure 3.13: Two views of the conditional means of the random effects from model bxm03. The lines from the origin are the principal axes of the unconditional distribution of the random effects. The panel on the right is looking in along the negative of the second principle axis (red line in left panel).\n\n\n\n\nIn each of the panels the three orthogonal lines from the origin are the three principle axes of the unconditional distribution of the random effects, corresponding to the columns of\n\nsvd(only(bxm03.λ)).U\n\n3×3 Matrix{Float64}:\n -0.723711   0.568469   0.39126\n -0.675845  -0.698491  -0.235254\n  0.139557  -0.434687   0.889703\n\n\nThe panel on the right is oriented so the viewpoint is along the negative of the second principle axis, showing that there is considerable variation in the first principle direction and zero variation in the third principle direction.\nWe see that the distribution of the random effects in model bxm03 is degenerate but it is not clear how to simplify the model.\nCoverage intervals from a parametric bootstrap sample for this model\n\n\nCode\nbxm03samp = parametricbootstrap(\n  Xoshiro(8642468),\n  10_000,\n  bxm03;\n  hide_progress=true,\n)\nbxm03pars = DataFrame(bxm03samp.allpars)\nDataFrame(shortestcovint(bxm03samp))\n\n\n12 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)52.73156.4212βmissingtime20.91724.71623βmissingtime ^ 2 & Group: Control0.1544881.437134βmissingtime ^ 2 & Group: Thioracil-2.02241-0.7244255βmissingtime ^ 2 & Group: Thyroxin0.4224951.901466σSubj(Intercept)2.485125.489037σSubjtime2.255045.43368ρSubj(Intercept), time0.3809831.09σSubjtime ^ 20.5468051.3949310ρSubj(Intercept), time ^ 2-1.0-0.44488911ρSubjtime, time ^ 2-0.898154-0.18458412σresidualmissing2.325123.27124\n\n\nshows that the coverage intervals for both of the correlation parameters involving the (Intercept) extend out to one of the limits of the allowable range [-1, 1] of correlations.\nA kernel density plot, Figure 3.14, of the parametric bootstrap estimates of the correlation coefficients reinforces this conclusion.\n\n\nCode\ndraw(\n  data(@subset(bxm03pars, :type == \"ρ\")) *\n  mapping(\n    :value => \"Bootstrap replicates of correlation estimates\";\n    color=(:names => \"Variables\"),\n  ) *\n  AlgebraOfGraphics.density();\n)\n\n\n\n\n\nFigure 3.14: Kernel density plots of parametric bootstrap estimates of correlation estimates from model bxm03\n\n\n\n\nEven on the scale of Fisher’s z transformation, Figure 3.15, these estimates are highly skewed.\n\n\nCode\nlet\n  dat = @transform(\n    @subset(bxm03pars, :type == \"ρ\"),\n    :z = atanh(clamp(:value, -0.99999, 0.99999))\n  )\n  mp = mapping(\n    :z => \"Fisher's z transformation of correlation estimates\";\n    color=(:names => \"Variables\"),\n  )\n  draw(\n    data(dat) * mp * AlgebraOfGraphics.density();\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\nFigure 3.15: Kernel density plots of Fisher’s z transformation of parametric bootstrap estimates of correlation estimates from model bxm03\n\n\n\n\nBecause of these high correlations, trying to deal with the degenerate random effects distribution by simply removing the random effects for time ^ 2 reduces the model too much.\n\nbxm04 = let\n  form =\n    @formula(resp ~ 1 + time + time^2 & Group + (1 + time | Subj))\n  fit(MixedModel, form, bxdf; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n1.2587\n43.38\n<1e-99\n5.5780\n\n\ntime\n22.8534\n1.0454\n21.86\n<1e-99\n3.6245\n\n\ntime ^ 2 & Group: Control\n0.7633\n0.2526\n3.02\n0.0025\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.3807\n0.2526\n-5.47\n<1e-07\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1984\n0.2890\n4.15\n<1e-04\n\n\n\nResidual\n3.6290\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm04, bxm03)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time | Subj)\n9\n867\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n30\n3\n<1e-05\n\n\n\n\n\nas does eliminating within-subject correlations between the random-effects for the time^2 random effect and the other random effects.\n\nbxm05 = let\n  form = @formula(\n    resp ~\n      1 +\n      time +\n      time^2 & Group +\n      (1 + time | Subj) +\n      (0 + time^2 | Subj)\n  )\n  fit(MixedModel, form, bxdf; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n54.6085\n1.0433\n52.34\n<1e-99\n4.6060\n\n\ntime\n22.8534\n0.8167\n27.98\n<1e-99\n2.5577\n\n\ntime ^ 2 & Group: Control\n0.8262\n0.3471\n2.38\n0.0173\n\n\n\ntime ^ 2 & Group: Thioracil\n-1.4171\n0.3471\n-4.08\n<1e-04\n\n\n\ntime ^ 2 & Group: Thyroxin\n1.1605\n0.4054\n2.86\n0.0042\n\n\n\ntime ^ 2\n\n\n\n\n0.9240\n\n\nResidual\n3.0374\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(bxm05, bxm03)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time | Subj) + (0 + :(time ^ 2) | Subj)\n10\n850\n\n\n\n\n\nresp ~ 1 + time + :(time ^ 2) & Group + (1 + time + :(time ^ 2) | Subj)\n12\n837\n13\n2\n0.0017\n\n\n\n\n\n\n\n3.5.4 Some consequences of changing the random-effects structure\nThe three models bxm03, bxm04, and bxm05 have the same fixed-effects structure. The random effects specification varies from the most complicated (bxm03), which produces a singular estimate of the covariance, to the simplest (bxm04) structure to an intermediate structure (bxm05).\nThe likelihood ratio tests give evidence for preferring bxm03, the most complex of these models, but also one with a degenerate distribution of the random effects.\nIf we assume that the purpose of the experiment is to compare the effects of the two treatments versus the Control group on the weight gain of the rats, then interest would focus on the fixed-effects parameters and, in particular, on those associated with the groups. The models give essentially the same predictions of weight versus time for a “typical” animal in each group, Figure 3.16.\n\n\nCode\nlet\n  times = Float32.((0:256) / 64)\n  times² = abs2.(times)\n  z257 = zeros(Float32, 257)\n  tmat = hcat(\n    ones(Float32, 257 * 3),\n    repeat(times, 3),\n    vcat(times², z257, z257),\n    vcat(z257, times², z257),\n    vcat(z257, z257, times²),\n  )\n  grp = repeat([\"Control\", \"Thioracil\", \"Thyroxin\"]; inner=257)\n  draw(\n    data(\n      append!(\n        append!(\n          DataFrame(;\n            times=tmat[:, 2],\n            wt=tmat * Float32.(bxm03.beta),\n            Group=grp,\n            model=\"bxm03\",\n          ),\n          DataFrame(;\n            times=tmat[:, 2],\n            wt=tmat * Float32.(bxm04.beta),\n            Group=grp,\n            model=\"bxm04\",\n          ),\n        ),\n        DataFrame(;\n          times=tmat[:, 2],\n          wt=tmat * Float32.(bxm05.beta),\n          Group=grp,\n          model=\"bxm05\",\n        ),\n      ),\n    ) *\n    mapping(\n      :times => \"Time in trial (wk)\",\n      :wt => \"Weight (gm)\";\n      color=:Group,\n      col=:model,\n    ) *\n    visual(Lines);\n    axis=(width=350, height=350),\n  )\nend\n\n\n\n\n\nFigure 3.16: Typical weight curves from models bxm03, bxm04, and bxm05 for each of the three treatment groups.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOther points to make:\n1. Standard errors are different, largest for `bxm05`, smallest for `bxm04`\n2. Real interest should center on the differences in the quadratic coef - trt vs control\n3. Not sure how to code that up\n4. More complex models require more iterations and more work per iteration\n5. Probably go with `bxm03` in this case, even though it gives a degenerate dist'n of random effects.  The idea is that the random effects are absorbing a form of rat-to-rat variability.  The residual standard deviation does reflect this.\n\n\n\n\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear curves. Biometrics, 6(4), 362. https://doi.org/10.2307/3001781\n\n\nDavis, C. S. (2002). Statistical methods for the analysis of repeated measurements. In Springer Texts in Statistics (pp. xxiv + 415). New York, NY: Springer. https://doi.org/10.1007/b97287\n\n\nElston, R. C., & Grizzle, J. E. (1962). Estimation of time-response curves and their confidence bands. Biometrics, 18, 148–159. https://doi.org/10.2307/2527453"
  },
  {
    "objectID": "largescale.html",
    "href": "largescale.html",
    "title": "4  A large-scale study",
    "section": "",
    "text": "Load the packages to be used.\nAs with many techniques in data science, the place where “the rubber meets the road”, as they say in the automotive industry, for mixed-effects models is when working on large-scale studies.\nOne such study is the English Lexicon Project (Balota et al., 2007) — a multicenter study incorporating both a lexical decision task and a word recognition task. Different groups of subjects participated in the different tasks.\nCompared to our previous examples, these are large data sets. Data manipulation and model fitting in cases like these requires considerable care."
  },
  {
    "objectID": "largescale.html#trial-level-data-from-the-ldt",
    "href": "largescale.html#trial-level-data-from-the-ldt",
    "title": "4  A large-scale study",
    "section": "4.1 Trial-level data from the LDT",
    "text": "4.1 Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = Arrow.Table(\"./data/ELP_ldt_trial.arrow\")\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  Int16\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"     => \"Trial-level data from Lexical Discrimination Task in the Engl…\n  \"reference\" => \"Balota et al. (2007), The English Lexicon Project, Behavior R…\n  \"source\"    => \"https://osf.io/n63s2\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nCome up with a better way of defining a path to ./data/, perhaps\ndatadir(paths::AbstractString...) = joinpath(\".\", \"data\", paths...)\n\n\n\nldttrial = @transform!(DataFrame(ldttrial), :S2 = :seq > 2000)\ndescribe(ldttrial)\n\n6 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6S20.40712800.010Bool\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day. The S2 variable added to the data frame using @transform! is a Boolean value indicating if the trial is in the second session."
  },
  {
    "objectID": "largescale.html#sec-ldtinitialexplore",
    "href": "largescale.html#sec-ldtinitialexplore",
    "title": "4  A large-scale study",
    "section": "4.2 Initial data exploration",
    "text": "4.2 Initial data exploration\nFrom the basic summary of ldttrial we can see that there are questionable response times, such as negative values and values over 32 seconds. Because of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\n4.2.1 Summaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‘i’ in the names of these summaries (and an ‘s’ in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),             # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(:wrdlen = length(:item), :ipropacc = :iacc / :ni)\nend\n\n80,962 rows × 7 columnsitemniimissiaccimedianrtwrdlenipropaccStringInt64Int64Int64Float64Int64Float641a35026743.010.7428572e35019824.010.5428573aah34021770.530.6176474aal34032702.530.9411765Aaron33031625.050.9393946Aarod33023810.050.696977aback34015710.050.4411768ahack34034662.051.09abacus34017671.560.510alacus34029640.060.85294111abandon34032641.070.94117612acandon34033725.570.97058813abandoned34031667.590.91176514adandoned34011760.590.32352915abandoning34034662.0101.016abantoning34030848.5100.88235317abandonment35035734.0111.018apandonment35030817.0110.85714319abase34123750.550.67647120abose34023805.550.67647121abasement33017850.090.51515222afasement33030649.090.90909123abash32022727.550.687524adash32025784.550.7812525abate34024687.050.70588226abape34032675.050.94117627abated34023775.060.67647128agated34014897.560.41176529abbess3407837.560.20588230abbass34028788.060.823529⋮⋮⋮⋮⋮⋮⋮⋮\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n8 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1itemAarodzuss0String2ni33.91663034.0370Int643imiss0.016921500.020Int644iacc29.0194031.0370Int645imedianrt753.069458.0737.51691.00Float646wrdlen7.998818.0210Int647ipropacc0.8556160.00.9117651.00Float648isword0.500.510Bool\n\n\nThis table shows that some of the items were never identified correctly. These are\n\n@subset(byitem, iszero(:iacc))\n\n9 rows × 8 columnsitemniimissiaccimedianrtwrdlenipropacciswordStringInt64Int64Int64Float64Int64Float64Bool1baobab3400616.560.012haulage3400708.570.013leitmotif3500688.090.014miasmal3500774.070.015peahen3400684.060.016plosive3400663.070.017plugugly3300709.080.018poshest3400740.070.019servo3300697.050.01\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a “left join”. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n8 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6S20.40712800.010Bool7wrdlen7.9983518.0210Union{Missing, Int64}8isword0.49999500.010Union{Missing, Bool}\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n8 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6S20.40712800.010Bool7wrdlen7.9983518.0210Int648isword0.49999500.010Bool\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\nA histogram of the word lengths, Figure 4.1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\ndraw(\n  data(byitem) *\n  mapping(:wrdlen => \"Length of word\") *\n  histogram(; bins=0.5:21.5),\n)\n\n\n\n\n\nFigure 4.1: Histogram of word lengths in the items used in the lexical decision task.\n\n\n\n\nTo examine trends in accuracy by word length we use a scatterplot smoother on the binary response, as described in Section 5.1.1. The resulting plot, Figure 4.2, shows the accuracy of identifying words is more-or-less constant at around 84%, but accuracy decreases with increasing word length for the nonwords.\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc))) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :acc => \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.2: Smoothed curves of accuracy versus word length in the lexical decision task.\n\n\n\n\nFigure 4.2 may be a bit misleading because the largest discrepancies in proportion of accurate identifications of words and nonwords occur for the longest words, of which there are few. Over 95% of the words are between 4 and 13 characters in length\n\ncount(x -> 4 ≤ x ≤ 13, byitem.wrdlen) / nrow(byitem)\n\n0.9654899829549666\n\n\nIf we restrict the smoother curves to this range, as in Figure 4.3,\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc), 4 ≤ :wrdlen ≤ 13)) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :acc => \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.3: Smoothed curves of accuracy versus word length in the range 4 to 13 characters in the lexical decision task.\n\n\n\n\nthe differences are less dramatic.\n\n\n4.2.2 Summaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),             # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n814 rows × 6 columnssubjnssmisssaccsmedianrtspropaccInt16Int64Int64Int64Float64Float6411337403158554.00.93598122337213031960.00.89887333337233006813.00.89145944337413062619.00.90752855337402574677.00.76289366337402927855.00.86751677337442877918.50.852697883372127311310.00.809905993374132669657.00.7910491010337402722757.00.8067581111337402894632.00.8577361212337442979692.00.88292813133374229801114.00.8832251414337412697603.00.7993481515337202957729.00.8769281616337402924710.00.8666271717337412947755.00.8734441818337402851617.00.8449911919337402890724.00.856552020337202905858.00.86150721213372030511041.00.9048042222337222756972.50.8173192323337432543629.50.7537052424337402995644.00.887672525337202988732.50.8861212626337403024830.00.89626627273374127741099.50.822172828337212898823.50.85943129293372030221052.50.8962043030337402946680.00.873148⋮⋮⋮⋮⋮⋮⋮\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n6 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolFloat64RealFloat64RealInt64DataType1subj409.3111409.58160Int162ns3373.4133703374.033740Int643smiss1.6830501.0220Int644sacc2886.3317272928.032860Int645smedianrt760.992205.0735.01804.00Float646spropacc0.8556130.5118550.8680310.9739180Float64\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure 4.4, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc => \"Proportion accurate\",\n    :smedianrt => \"Median response time\",\n  ) *\n  (smooth() + visual(Scatter));\n)\n\n\n\n\n\nFigure 4.4: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(<(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(@subset(bysubj, :spropacc > 0.9).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant’s mean response.\nAs described in Section 4.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ≤ :rt ≤ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n9 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1subj409.8021410.08160Int162seq1684.5611684.033740Int163acc0.85988401.010Bool4rt838.712200733.030000Int165itemAarodzuss0String6S20.4066300.010Bool7wrdlen7.9924418.0210Int648isword0.50012601.010Bool9spropacc0.8571690.5118550.8692950.9739180Float64\n\n\n\n\n4.2.3 Choice of response time scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure 4.5, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt => \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.5: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\n\ndescribe(@transform!(pruned, :speed = 1000 / :rt)) # speed in s⁻¹\n\n10 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1subj409.8021410.08160Int162seq1684.5611684.033740Int163acc0.85988401.010Bool4rt838.712200733.030000Int165itemAarodzuss0String6S20.4066300.010Bool7wrdlen7.9924418.0210Int648isword0.50012601.010Bool9spropacc0.8571690.5118550.8692950.9739180Float6410speed1.373980.3333331.364265.00Float64\n\n\nThe density of the response speed, in responses per second, is shown in Figure 4.6.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:speed => \"Response speed (s⁻¹) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 4.6: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\nFigure 4.5 and Figure 4.6 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(r -> !ismissing(r.acc) && 200 < r.rt < 333, eachrow(ldttrial)) /\ncount(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure 4.7 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :rt => \"Response time (ms)\";\n    :color => :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure 4.7: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\nand Figure 4.8 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :speed => \"Speed of response (s⁻¹)\";\n    :color => :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure 4.8: Scatterplot smooths of speed time versus word length in the LDT.\n\n\n\n\nFor the most part the smoother lines in Figure 4.8 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study."
  },
  {
    "objectID": "largescale.html#sec-ldtinitialmodel",
    "href": "largescale.html#sec-ldtinitialmodel",
    "title": "4  A large-scale study",
    "section": "4.3 Models with scalar random effects",
    "text": "4.3 Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\n4.3.1 Establish the contrasts\nBecause there are a large number of items in the data set it is important to assign a Grouping() contrast to item (and, less importantly, to subj). For the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts[:subj] = Grouping();\ncontrasts[:item] = Grouping();\ncontrasts[:isword] = EffectsCoding(; base=false);\ncontrasts[:wrdlen] = Center(8);\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let\n  form = @formula(\n    speed ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  fit(MixedModel, form, pruned; contrasts)\nend\n\n\u001b[32mMinimizing 54   Time: 0:00:03 (71.03 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n<1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n<1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let\n  form = @formula(\n    speed ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  dat = @subset(pruned, :spropacc > 0.8)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\u001b[32mMinimizing 73   Time: 0:00:03 (49.33 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3611\n0.0088\n153.98\n<1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.73\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n<1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\nTo compare the conditional means of the random effects for item in these two models we incorporate them into the byitem table.\n\ndisallowmissing!(leftjoin!(\n  leftjoin!(\n    byitem,\n    rename!(DataFrame(raneftables(elm01).item), [\"item\", \"elm01\"]);\n    on=:item,\n  ),\n  rename!(DataFrame(raneftables(elm02).item), [\"item\", \"elm02\"]);\n  on=:item,\n))\n\n80,962 rows × 10 columns (omitted printing of 1 columns)itemniimissiaccimedianrtwrdlenipropacciswordelm01StringInt64Int64Int64Float64Int64Float64BoolFloat641a35026743.010.7428571-0.3304572e35019824.010.5428570-0.26213aah34021770.530.6176471-0.2471684aal34032702.530.9411760-0.06977395Aaron33031625.050.9393941-0.07521396Aarod33023810.050.696970-0.1253077aback34015710.050.4411761-0.13278ahack34034662.051.000.0302719abacus34017671.560.51-0.17860510alacus34029640.060.85294100.008683211abandon34032641.070.94117610.053232212acandon34033725.570.97058800.00019791113abandoned34031667.590.91176510.10386214adandoned34011760.590.32352900.050986315abandoning34034662.0101.010.099934816abantoning34030848.5100.8823530-0.0083686817abandonment35035734.0111.010.070656318apandonment35030817.0110.8571430-0.01612419abase34123750.550.6764711-0.20246520abose34023805.550.6764710-0.05310921abasement33017850.090.5151521-0.11737322afasement33030649.090.90909100.17362623abash32022727.550.68751-0.19465824adash32025784.550.781250-0.15270525abate34024687.050.7058821-0.089593726abape34032675.050.94117600.081188327abated34023775.060.6764711-0.17196628agated34014897.560.4117650-0.15719729abbess3407837.560.2058821-0.22255130abbass34028788.060.8235290-0.0246175⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data(byitem) * mapping(\n    :elm01 => \"Conditional means of item random effects for model elm01\",\n    :elm02 => \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  );\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\nFigure 4.9: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure 4.9. It happens that the search function on the Makie documentation is down now so I can’t look up how to do it.\n\n\nFigure 4.9 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(byitem, :elm01, :elm02)))\n\n2×2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nsaveoptsum(\"./optsums/elm01.json\", elm01);\n\n\nelm01a = restoreoptsum!(\n  let\n    form = @formula(\n      speed ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n    )\n    MixedModel(form, pruned; contrasts)\n  end,\n  \"./optsums/elm01.json\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n<1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n<1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(Arrow.Table(\"./data/ELP_ldt_item.arrow\"))\ndescribe(elpldtitem)\n\n9 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64Type1itemAarodzuss0String2Ortho_N1.5330901.0250Int83BG_Sum13938.41113026.059803177Union{Missing, Int32}4BG_Mean1921.255.51907.06910.0177Union{Missing, Float32}5BG_Freq_By_Pos2043.0801928.069854Union{Missing, Int16}6itemno40481.5140481.5809620Int327isword0.500.510Bool8wrdlen7.998818.0210Int89pairno20241.0120241.0404810Int32\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(Arrow.Table(\"./data/ELP_ldt_subj.arrow\"))\ndescribe(elpldtsubj)\n\n20 rows × 7 columns (omitted printing of 2 columns)variablemeanminmedianmaxSymbolUnion…AnyAnyAny1subj409.3111409.58162univKansasWayne State3sexfm4DOB1938-06-071984-11-145MEQ44.493219.044.075.06vision5.5116906.077hearing5.8610106.078educatn8.89681112.0289ncorrct29.8505530.04010rawscor31.99251332.04011vocabAge17.812310.317.821.012shipTime3.086103.0913readTime2.502150.02.015.014preshlth5.4870806.0715pasthlth4.9298905.0716S1start2001-03-16T13:49:272001-10-16T11:38:28.5002003-07-29T18:48:4417S2start2001-03-19T10:00:352001-10-19T14:24:19.5002003-07-30T13:07:4518MEQstrt2001-03-22T18:32:002001-10-23T11:26:132003-07-30T14:30:4919filename101DATA.LDTData998.LDT20frstLangEnglishother\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure 4.10,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n\n\nFigure 4.10: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\n\n\n\n\nBalota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The english lexicon project. Behavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/bf03193014\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statistical Society: Series B (Methodological), 26(2), 211–243. https://doi.org/10.1111/j.2517-6161.1964.tb00553.x"
  },
  {
    "objectID": "glmmbinomial.html",
    "href": "glmmbinomial.html",
    "title": "5  Generalized Linear Mixed Models for Binary Responses",
    "section": "",
    "text": "\\[\n\\newcommand\\bbA{{\\mathbf{A}}}\n\\newcommand\\bbb{{\\mathbf{b}}}\n\\newcommand\\bbI{{\\mathbf{I}}}\n\\newcommand\\bbR{{\\mathbf{R}}}\n\\newcommand\\bbX{{\\mathbf{X}}}\n\\newcommand\\bbx{{\\mathbf{x}}}\n\\newcommand\\bby{{\\mathbf{y}}}\n\\newcommand\\bbZ{{\\mathbf{Z}}}\n\\newcommand\\bbbeta{{\\boldsymbol{\\beta}}}\n\\newcommand\\bbeta{{\\boldsymbol{\\eta}}}\n\\newcommand\\bbLambda{{\\boldsymbol{\\Lambda}}}\n\\newcommand\\bbOmega{{\\boldsymbol{\\Omega}}}\n\\newcommand\\bbmu{{\\boldsymbol{\\mu}}}\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\newcommand\\bbtheta{{\\boldsymbol{\\theta}}}\n\\newcommand\\mcN{{\\mathcal{N}}}\n\\newcommand\\mcB{{\\mathcal{B}}}\n\\newcommand\\mcY{{\\mathcal{Y}}}\n\\]\nAttach the packages to be used in this chapter\nIn this chapter we consider mixed-effects models for data sets in which the response is binary, representing yes/no or true/false or correct/incorrect responses.\nBecause the response must be one of only two possible values we adapt our models to predict the probability of the positive response. As for linear models and linear mixed-effects models, the mean response, \\(\\bbmu\\), is determined by a linear predictor, \\[\n\\bbeta=\\bbX\\bbbeta+\\bbZ\\bbb\n\\qquad(5.1)\\] depending on the fixed-effects parameters, \\(\\bbbeta\\), the random effects, \\(\\bbb\\), and the model matrices, \\(\\bbX\\) and \\(\\bbZ\\). For a linear model the mean response, \\(\\bbmu\\), is the linear predictor, \\(\\bbeta\\). But for a generalized linear model \\(\\bbeta\\) determines \\(\\bbmu\\) according to a link function, \\(g\\). For historical reasons it is the function taking an element of \\(\\bbmu\\) to the corresponding element of \\(\\bbeta\\) that is called the link. The transformation in the opposite direction, from \\(\\bbeta\\) to \\(\\bbmu\\), is called the inverse link.\nAs in previous chapters, we will begin with an example to help illustrate these ideas."
  },
  {
    "objectID": "glmmbinomial.html#sec-contraception",
    "href": "glmmbinomial.html#sec-contraception",
    "title": "5  Generalized Linear Mixed Models for Binary Responses",
    "section": "5.1 Artificial contraception use in regions of Bangladesh",
    "text": "5.1 Artificial contraception use in regions of Bangladesh\nOne of the test data sets from the Center for Multilevel Modelling, University of Bristol is derived from the 1989 Bangladesh Fertility Survey, (Huq & Cleland, 1990). The data are a subsample of 1934 women selected from 60 of the 64 political districts or zila, available as the contra data set in the MixedModels package.\n\ncontra = @chain :contra begin\n  MixedModels.dataset\n  @aside println(_)\n  DataFrame\nend;\ndescribe(contra, :mean, :min, :median, :max)\n\nArrow.Table with 1934 rows, 5 columns, and schema:\n\n\n :dist   String\n :urban  String\n :livch  String\n :age    Float64\n :use    String\n\n\n5 rows × 5 columnsvariablemeanminmedianmaxSymbolUnion…AnyUnion…Any1distD01D612urbanNY3livch03+4age0.00204757-13.56-1.5619.445useNY\n\n\nThe response of interest is use — whether the woman chooses to use artificial contraception. The covariates include the district in which the woman resides, the number of live children she currently has, her age and whether she is in a rural or an urban setting.\nNote that the age variable is centered about a particular age so some values are negative. Regretably, the information on what the centering age was does not seem to be available.\n\n5.1.1 Plotting the binary response\nProducing informative graphical displays of a binary response as it relates to covariates is somewhat more challenging that the corresponding plots for responses on a continuous scale. If we were to plot the 1934 responses as 0/1 values versus, for example, the woman’s centered age, we would end up with a rather uninformative plot because all the points would fall on one of two horizontal lines.\nOne approach to illustrating the structure of the data more effectively is to add scatterplot smoother lines as in Figure 5.1\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 5.1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\nto show the trend in the response with respect to the covariate. Once we have the smoother lines in such a plot we can omit the data points themselves, as we did here, because they add very little information.\nThe first thing to notice about the plot is that the proportion of women using contraception is not linear in age, which, on reflection, makes sense. A woman in the middle of this age range (probably corresponding to an age around 25) is more likely to use artificial contraception than is a girl in her early teens or a woman in her forties. We also see that women in an urban setting are more likely to use contraception than those in a rural setting and that women with no live children are less likely than women who have live children. There do not seem to be strong differences between women who have 1, 2 or 3 or more children compared to the differences between women with children and those without children.\nInterestingly, the quadratic pattern with respect to age does not seem to have been noticed. Comparisons of model fits through different software systems, as provided by the Center for Multilevel Modelling, incorporate only a linear term in age, even though the pattern is clearly nonlinear. The lesson here is similar to what we have seen in other examples; careful plotting of the data should, whenever possible, precede attempts to fit models to the data.\n\n\n5.1.2 Initial GLMM fit to the contraception data\nAs for a linear mixed-model, the first three arguments in a call to fit a generalized linear mixed models are MixedModel, the model formula, and the name of the data frame. The fourth argument describes the type of conditional distribution of the response given the random effects.\n\ncontrasts = Dict(\n  :livch => EffectsCoding(; base=\"0\"),\n  :urban => HelmertCoding(),\n  :dist => Grouping(),\n)\ncom01 = let\n  form =\n    @formula(use ~ 1 + livch + (age + age^2) * urban + (1 | dist))\n  fit(MixedModel, form, contra, Bernoulli(); contrasts)\nend\nprintln(com01)\n\n\u001b[32mMinimizing 527      Time: 0:00:00 ( 0.95 ms/it)\u001b[39m\n\n\nGeneralized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n  use ~ 1 + livch + age + :(age ^ 2) + urban + age & urban + :(age ^ 2) & urban + (1 | dist)\n  Distribution: \n\n\nBernoulli{Float64}\n  Link: \n\n\nLogitLink()\n\n   logLik    deviance     AIC       AICc        BIC    \n -1185.4606  2370.9212  2390.9212  2391.0356  2446.5946\n\n\n\nVariance components:\n        Column   Variance Std.Dev. \ndist (Intercept)  0.226039 0.475436\n\n Number of obs: 1934; levels of grouping factors: 60\n\nFixed-effects parameters:\n\n\n──────────────────────────────────────────────────────────────\n                           Coef.   Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────────────────\n(Intercept)         -0.0115288    0.105467     -0.11    0.9130\nlivch: 1             0.153578     0.0981766     1.56    0.1177\nlivch: 2             0.256163     0.104455      2.45    0.0142\nlivch: 3+            0.258254     0.102217      2.53    0.0115\nage                  0.000663058  0.00954016    0.07    0.9446\nage ^ 2             -0.00474472   0.000774522  -6.13    <1e-09\nurban: Y             0.376827     0.0803239     4.69    <1e-05\nage & urban: Y      -0.00678813   0.00685774   -0.99    0.3222\nage ^ 2 & urban: Y  -0.000368095  0.000731401  -0.50    0.6148\n──────────────────────────────────────────────────────────────\n\n\nThe Bernoulli distribution is used for binary responses. Occasionally responses taken at the same covariate values are grouped together and modeled with a Binomial distribution, but that is the exception, not the rule.\nA fifth unnamed argument can be included to specify the link function but for most cases a canonical link function is defined and used. As we see in the output above, the canonical link for the Bernoulli distribution (and the Binomial distribution) is the logit link.\nThe interpretation of the coefficients in this model is somewhat different from the linear mixed models coefficients that we examined previously but many of the model-building steps are similar. A rough assessment of the utility of a particular term in the fixed-effects part of the model can be obtained from examining the estimates of the coefficients associated with it and their standard errors. To test whether a particular term is useful we omit it from the model, refit and compare the reduced model fit to the original according to the change in deviance.\nWe will examine the terms in the model first and discuss the interpretation of the coefficients in .\nRecall from ?sec-covariates that the default set of contrasts for a factor such as livch is offsets relative to the reference level, in this case women who do not have any live children. Although the coefficients labeled livch: 1, livch: 2, and livch: 3+ are all large relative to their standard errors, they are reasonably close to each other. This confirms our earlier impression that the main distinction is between women with children and those without and, for those who do have children, the number of children is not an important distinction.\nAfter incorporating a new variable ch — an indicator of whether the woman has any children — in the data\n\n@transform!(contra, :ch = :livch > \"0\")\ndescribe(contra)\n\n6 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1distD01D610String2urbanNY0String3livch03+0String4age0.00204757-13.56-1.5619.440Float645useNY0String6ch0.72595701.010Bool\n\n\nwe fit a reduced model.\n\ncom02 = let\n  form = @formula(use ~ 1 + ch + (age + age^2) * urban + (1 | dist))\n  fit(MixedModel, form, contra, Bernoulli(); contrasts)\nend\n\n\u001b[32mMinimizing 430      Time: 0:00:00 ( 0.52 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6531\n0.1634\n-4.00\n<1e-04\n0.4739\n\n\nch\n0.8683\n0.1482\n5.86\n<1e-08\n\n\n\nage\n0.0035\n0.0082\n0.43\n0.6684\n\n\n\nage ^ 2\n-0.0048\n0.0008\n-6.27\n<1e-09\n\n\n\nurban: Y\n0.3737\n0.0800\n4.67\n<1e-05\n\n\n\nage & urban: Y\n-0.0068\n0.0069\n-0.99\n0.3227\n\n\n\nage ^ 2 & urban: Y\n-0.0004\n0.0007\n-0.49\n0.6266\n\n\n\n\n\n\nComparing this model to the previous model\n\nMixedModels.likelihoodratiotest(com02, com01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nuse ~ 1 + ch + age + :(age ^ 2) + urban + age & urban + :(age ^ 2) & urban + (1 | dist)\n8\n2371\n\n\n\n\n\nuse ~ 1 + livch + age + :(age ^ 2) + urban + age & urban + :(age ^ 2) & urban + (1 | dist)\n10\n2371\n0\n2\n0.7827\n\n\n\n\n\nindicates that the reduced model is adequate.\nA plot of the smoothed observed proportions versus centered age according to urban and ch\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:ch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 5.2: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey. The livch factor has been collapsed to children/nochildren.\n\n\n\n\nindicates that all four groups have a quadratic trend with respect to age but the location of the peak proportion is shifted for those without children relative to those with children. Incorporating an interaction of age and ch allows for such a shift.\n\ncom03 = let\n  form = @formula(\n    use ~\n      1 + (ch + age) * urban + age^2 + age^2 & urban + (1 | dist)\n  )\n  fit(MixedModel, form, contra, Bernoulli(); contrasts)\nend\n\n\u001b[32mMinimizing 566      Time: 0:00:00 ( 0.51 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6443\n0.1642\n-3.92\n<1e-04\n0.4740\n\n\nch\n0.8506\n0.1506\n5.65\n<1e-07\n\n\n\nage\n0.0040\n0.0082\n0.49\n0.6263\n\n\n\nurban: Y\n0.4619\n0.1517\n3.04\n0.0023\n\n\n\nage ^ 2\n-0.0048\n0.0008\n-6.27\n<1e-09\n\n\n\nch & urban: Y\n-0.1031\n0.1502\n-0.69\n0.4927\n\n\n\nage & urban: Y\n-0.0037\n0.0082\n-0.45\n0.6497\n\n\n\nage ^ 2 & urban: Y\n-0.0005\n0.0008\n-0.67\n0.5017\n\n\n\n\n\n\nComparing this fitted model to the previous one\n\nMixedModels.likelihoodratiotest(com03, com02)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nuse ~ 1 + ch + age + :(age ^ 2) + urban + age & urban + :(age ^ 2) & urban + (1 | dist)\n8\n2371\n\n\n\n\n\nuse ~ 1 + ch + age + urban + :(age ^ 2) + ch & urban + age & urban + :(age ^ 2) & urban + (1 | dist)\n9\n2371\n0\n1\n0.4951\n\n\n\n\n\nconfirms the usefulness of this term.\nContinuing with the model-building we turn our attention to the random effects specification to see whether urban/rural differences vary significantly between districts and whether the distinction between childless women and women with children varies between districts.\n\ncom04 = let\n  form = @formula(use ~ 1 + urban + ch * age + age^2 + (1 | dist))\n  fit(MixedModel, form, contra, Bernoulli(); contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.9663\n0.2078\n-4.65\n<1e-05\n0.4723\n\n\nurban: Y\n0.3570\n0.0601\n5.94\n<1e-08\n\n\n\nch\n1.2108\n0.2070\n5.85\n<1e-08\n\n\n\nage\n-0.0473\n0.0218\n-2.17\n0.0303\n\n\n\nage ^ 2\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nch & age\n0.0684\n0.0254\n2.69\n0.0072\n\n\n\n\n\n\nWe fit a succession of models, described in the exercises for this chapter, before settling on model com05,\n\ncom05 = let\n  form = @formula(\n    use ~ 1 + urban + ch * age + age^2 + (1 | dist & urban)\n  )\n  fit(MixedModel, form, contra, Bernoulli(); contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist & urban\n\n\n\n\n(Intercept)\n-0.9474\n0.2088\n-4.54\n<1e-05\n0.5683\n\n\nurban: Y\n0.3934\n0.0853\n4.61\n<1e-05\n\n\n\nch\n1.2129\n0.2090\n5.80\n<1e-08\n\n\n\nage\n-0.0462\n0.0220\n-2.10\n0.0361\n\n\n\nage ^ 2\n-0.0056\n0.0008\n-6.67\n<1e-10\n\n\n\nch & age\n0.0665\n0.0256\n2.59\n0.0095\n\n\n\n\n\n\nNotice that although there are 60 distinct districts there are only 102 distinct combinations of represented in the data. In 15 of the 60 districts there are no rural women in the sample and in 3 districts there are no urban women in the sample, as shown in"
  },
  {
    "objectID": "glmmbinomial.html#sec-glmmlink",
    "href": "glmmbinomial.html#sec-glmmlink",
    "title": "5  Generalized Linear Mixed Models for Binary Responses",
    "section": "5.2 Link functions and interpreting coefficients",
    "text": "5.2 Link functions and interpreting coefficients\nTo this point the only difference we have encountered between and as model-fitting functions is the need to specify the distribution family in a call to fit. The formula specification is identical and the assessment of the significance of terms using likelihood ratio tests is similar. This is intentional. We have emphasized the use of likelihood ratio tests on terms, whether fixed-effects or random-effects terms, exactly so the approach will be general.\nHowever, the interpretation of the coefficient estimates in the different types of models is different. In a linear mixed model the linear predictor is the conditional mean (or “expected value”) of the response given the random effects. That is, if we assume that we know the values of the fixed-effects parameters and the random effects, then the expected response for a particular combination of covariate values is the linear predictor. Individual coefficients can be interpreted as slopes of the fitted response with respect to a numeric covariate or as shifts between levels of a categorical covariate.\nTo interpret the estimates of coefficients in a GLMM we must define and examine the link function that we mentioned earlier.\n\n5.2.1 The logit link function for binary responses\nThe probability model for a binary response is the Bernoulli distribution, which is about the simplest probability distribution we can concoct. There are only two possible values: 0 and 1. If the probability of the response 1 is \\(p\\) then the probability of 0 must be \\(1-p\\). It is easy to establish that the expected value is also \\(p\\). For consistency across distribution families we write this expected response as \\(\\mu\\) instead of \\(p\\). We should, however, keep in mind that, for this distribution, \\(\\mu\\) corresponds to a probability and hence must satisfy \\(0\\le\\mu\\le 1\\).\nIn general we don’t want to have restrictions on the values of the linear predictor so we equate the linear predictor to a function of \\(\\mu\\) that has an unrestricted range. In the case of the Bernoulli distribution with the canonical link function we equate the linear predictor to the log odds or logit of the positive response. That is \\[\n\\eta = \\log\\left(\\frac{\\mu}{1-\\mu}\\right) .\n\\qquad(5.2)\\]\nTo understand why this is called the “log odds” recall that \\(\\mu\\) corresponds to a probability in \\([0,1]\\). The corresponding odds ratio, \\(\\frac{\\mu}{1-\\mu}\\), is in \\([0,\\infty)\\) and the logarithm of the odds ratio, \\(\\mathrm{logit}(\\mu)\\), is in \\((-\\infty, \\infty)\\).\nThe inverse of the logit link function, \\[\n\\mu = \\frac{1}{1+\\exp(-\\eta)} ,\n\\qquad(5.3)\\] is called the logistic function and is shown in Figure 5.3.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      DataFrame(η=collect(-5.0:0.02:5.0)),\n      :μ = inv(1 + exp(-:η))\n    )\n  ) *\n  mapping(:η, :μ) *\n  visual(Lines);\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\nFigure 5.3: The logistic function, which is the inverse to the logit link function.\n\n\n\n\nThe inverse link takes a value on the unrestricted range, \\((-\\infty,\\infty)\\), and maps it to the probability range, \\([0,1]\\). It happens this function is also the cumulative distribution function for the standard logistic distribution, available in Distributions.jl as cdf(Logistic(), η). In some presentations the relationship between the logit link and the logistic distribution is emphasized but that often leads to questions of why we should focus on the logistic distribution. Also, it is not clear how this approach would generalize to other distributions such as the Poisson or the Gamma distributions.\n\n\n5.2.2 Canonical link functions\nA way of deriving the logit link that does generalize to a class of common distributions in what is called the exponential family is to consider the logarithm of the probability function (for discrete distributions) or the probability density function (for continuous distributions). The probability function for the Bernoulli distribution is \\(\\mu\\) for \\(y=1\\) and \\(1-\\mu\\) for \\(y=0\\). If we write this in a somewhat peculiar way as \\(\\mu^y+(1-\\mu)^{1-y}\\) for \\(y\\in\\{0,1\\}\\) then the logarithm of the probability function becomes \\[\n\\log\\left(\\mu^y+(1-\\mu)^{1-y}\\right) = \\log(1-\\mu) +\ny\\,\\log\\left(\\frac{\\mu}{1-\\mu}\\right) .\n\\qquad(5.4)\\] Notice that the logit link function is the multiple of \\(y\\) in the last term.\nFor members of the exponential family the logarithm of the probability or probability density function can be expressed as a sum of up to three terms: one that involves \\(y\\) only, one that involves the parameters only and the product of \\(y\\) and a function of the parameters. This function is the canonical link.\nIn the case of the Poisson distribution the probability function is \\(\\frac{e^{-\\mu}\\mu^y}{y!}\\) for \\(y\\in\\{0,1,2,\\dots\\}\\) so the log probability function is \\[\n-\\log(y!)-\\mu+y\\log(\\mu) .\n\\qquad(5.5)\\] and the canonical link function is \\(\\log(\\mu)\\).\n\n\n5.2.3 Interpreting coefficient estimates\nReturning to the interpretation of the estimated coefficients in model we apply exactly the same interpretation as for a linear mixed model but taking into account that slopes or differences in levels are with respect to the logit or log-odds function. If we wish to express results in the probability scale then we should apply the function to whatever combination of coefficients is of interest to us.\nFor example, we see from Figure 5.2 that the observed proportion of childless women with a centered age of 0 living in a rural setting who use artificial contraception is about 20%. The fitted value of the log-odds for a typical district (i.e. with a random effect of zero) is corresponding to a fitted probability of\nor %.\nSimilarly the predicted log-odds of a childless woman with a centered age of 0 in an urban setting of a typical district using artificial contraception is\ncorresponding to a probability of\nThe predicted log-odds and predicted probability for a woman with children and at the same age and location are\nWe should also be aware that the random effects are defined on the linear predictor scale and not on the probability scale. A normal probability plot of the conditional modes of the random effects for model\nshows that the smallest random effects are approximately -1 and the largest are approximately 1. The numerical values and the identifier of the combination of and for these extreme values can be obtained as\nand\nThe exponential of the random effect is the relative odds of a woman in a particular urban/district combination using artificial birth control compared to her counterpart (same age, same with/without children status, same urban/rural status) in a typical district. The odds of a rural woman in district 1 (i.e. the value of the interaction) using artifical contraception is\nor about 40% of that of her urban counterpart in a typical district.\nNotice that there is considerable variability in the lengths of the prediction intervals in\n\n\nCode\nqqcaterpillar(com05)\n\n\n\n\n\nFigure 5.4: Caterpillar plot of the conditional modes of the random-effects for model com05\n\n\n\n\nThis is to be expected with data from a highly unbalanced observational study.\nConsider the cross-tabulation of counts of interviewees by district and urban/rural status presented at the end of . The data contains responses from 54 rural women in district 1 but only 21 rural women from district 11. Thus the bottom line in Figure 5.4, from the level of the interaction, and based on 54 responses, is shorter than the line second from the bottom, for and based on 21 women only.\n\n\n\n\nHuq, N. M., & Cleland, J. (1990). Bangladesh fertility survey 1989 (main report). National Institute of Population Research; Training."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler,\nB., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., &\nTreiman, R. (2007). The english lexicon project. Behavior Research\nMethods, 39(3), 445–459. https://doi.org/10.3758/bf03193014\n\n\nBates, D., Maechler, M., Bolker, B. M., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4. Journal of Statistical\nSoftware, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBox, G. E. P. (1950). Problems in the analysis of growth and wear\ncurves. Biometrics, 6(4), 362. https://doi.org/10.2307/3001781\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations.\nJournal of the Royal Statistical Society: Series B\n(Methodological), 26(2), 211–243. https://doi.org/10.1111/j.2517-6161.1964.tb00553.x\n\n\nBox, G. E. P., & Tiao, G. C. (1973). Bayesian inference in\nstatistical analysis. Addison-Wesley.\n\n\nDanisch, S., & Krumbiegel, J. (2021). Makie.jl: Flexible\nhigh-performance data visualization for julia. Journal of Open\nSource Software, 6(65), 3349. https://doi.org/10.21105/joss.03349\n\n\nDavies, O. L., & Goldsmith, P. L. (Eds.). (1972). Statistical\nmethods in research and production (4th ed.). Hafner.\n\n\nDavis, C. S. (2002). Statistical methods for the analysis of repeated\nmeasurements. In Springer Texts in Statistics (pp. xxiv + 415).\nNew York, NY: Springer. https://doi.org/10.1007/b97287\n\n\nElston, R. C., & Grizzle, J. E. (1962). Estimation of time-response\ncurves and their confidence bands. Biometrics, 18,\n148–159. https://doi.org/10.2307/2527453\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and\nsex effects in physical fitness components of 108,295 third graders\nincluding 515 primary schools and 9 cohorts. Scientific\nReports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nHuq, N. M., & Cleland, J. (1990). Bangladesh fertility survey\n1989 (main report). National Institute of Population Research;\nTraining.\n\n\nKliegl, R., Kushela, J., & Laubrock, J. (2015). Object\norientation and target size modulate the speed of visual attention.\nDepartment of Psychology, University of Potsdam.\n\n\nKliegl, R., Wei, P., Dambacher, M., Yan, M., & Zhou, X. (2010).\nExperimental effects and individual differences in linear mixed models:\nEstimating the relationship between spatial, object, and attraction\neffects in visual attention. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2010.00238\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in\nS and S-PLUS. Springer.\n\n\nRasbash, J., Browne, W., Goldstein, H., Yang, M., & Plewis, I.\n(2000). A user’s guide to MLwiN. Multilevel Models\nProject, Institute of Education, University of London.\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear\nmodels: Applications and data analysis methods (2nd ed.). Sage.\n\n\nSakamoto, Y., Ishiguro, M., & Kitagawa, G. (1986). Akaike\ninformation criterion statistics (p. 290). Reidel.\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How\nto capitalize on a priori contrasts in linear (mixed) models: A\ntutorial. Journal of Memory and Language, 110, 104038.\nhttps://doi.org/10.1016/j.jml.2019.104038\n\n\nSchwarz, G. (1978). Estimating the dimension of a model. Annals of\nStatistics, 6, 461–464."
  },
  {
    "objectID": "linalg.html",
    "href": "linalg.html",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "",
    "text": "\\[\n\\newcommand\\bbA{{\\mathbf{A}}}\n\\newcommand\\bbb{{\\mathbf{b}}}\n\\newcommand\\bbI{{\\mathbf{I}}}\n\\newcommand\\bbR{{\\mathbf{R}}}\n\\newcommand\\bbX{{\\mathbf{X}}}\n\\newcommand\\bbx{{\\mathbf{x}}}\n\\newcommand\\bby{{\\mathbf{y}}}\n\\newcommand\\bbbeta{{\\boldsymbol{\\beta}}}\n\\newcommand\\bbeta{{\\boldsymbol{\\eta}}}\n\\newcommand\\bbLambda{{\\boldsymbol{\\Lambda}}}\n\\newcommand\\bbOmega{{\\boldsymbol{\\Omega}}}\n\\newcommand\\bbmu{{\\boldsymbol{\\mu}}}\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\newcommand\\bbtheta{{\\boldsymbol{\\theta}}}\n\\newcommand\\mcN{{\\mathcal{N}}}\n\\newcommand\\mcB{{\\mathcal{B}}}\n\\newcommand\\mcU{{\\mathcal{U}}}\n\\newcommand\\mcX{{\\mathcal{X}}}\n\\newcommand\\mcY{{\\mathcal{Y}}}\n\\newcommand\\mcZ{{\\mathcal{Z}}}\n\\]\nAttach the packages to be used in this appendix\nIn this appendix we describe properties of the multivariate Gaussian (or “normal”) distribution and how linear models and linear mixed models can be formulated in terms of this distribution.\nWe also describe some methods in numerical linear algebra that are particularly useful in working with linear models. One of the strengths of the Julia language is the LinearAlgebra package in the standard library. The implementation of a multi-dimensional array, including one-dimensional vectors and two-dimensional matrices, is part of the base language. The added value of the LinearAlgebra package is compact representations of special types of matrices and methods for and with matrix decompositions or factorizations.\nThe purpose of these descriptions is to motivate a representation of a linear mixed model that allows for fast and stable estimation of the parameters. The estimation process requires iterative optimization of some of the parameters in the model to minimize an objective function. Often this optimization requires hundreds or thousands of evaluations of the objective at different values of the parameters and the portion of time spent in these evaluations dominates the overall estimation time. Thus, a fast, efficient method for evaluating the objective is crucial to making the whole process fast."
  },
  {
    "objectID": "linalg.html#matrix-vector-representation-of-linear-models",
    "href": "linalg.html#matrix-vector-representation-of-linear-models",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.1 Matrix-vector representation of linear models",
    "text": "A.1 Matrix-vector representation of linear models\nA linear statistical model is often written in terms of each element of the \\(n\\)-dimensional response vector, \\(\\bby\\), as, e.g. \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\dots + \\beta_p x_{i,p} + \\epsilon_i, \\quad i=1,\\dots, n\n\\qquad(A.1)\\] and some additional description like “where the \\(\\epsilon_i,i=1,\\dots,n\\) are independently and identically distributed as \\(\\mcN(0, \\sigma^2)\\)”.\nAn alternative is to write the model in terms of the \\(n\\)-dimensional response vector, \\(\\bby\\), an \\(n\\times p\\) model matrix, \\(\\bbX\\), and a \\(p\\)-dimensional coefficient vector, \\(\\bbbeta\\), as \\[\n\\mcY\\sim\\mcN\\left(\\bbX\\bbbeta,\\sigma^2\\bbI\\right),\n\\qquad(A.2)\\] where \\(\\mcN\\) denotes the multivariate Gaussian distribution with mean \\(\\bbmu=\\bbX\\bbbeta\\) and variance-covariance matrix \\(\\bbSigma=\\sigma^2\\bbI\\). (In what follows we will refer to the variance-covariance matrix as simply the covariance matrix.)\nBefore considering properties of and computational methods for the model Equation A.2 we will describe some of the properties of the multivariate Gaussian distribution."
  },
  {
    "objectID": "linalg.html#the-multivariate-gaussian-distribution",
    "href": "linalg.html#the-multivariate-gaussian-distribution",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.2 The multivariate Gaussian distribution",
    "text": "A.2 The multivariate Gaussian distribution\nJust as a univariate Gaussian distribution can be written by specifying the (scalar) mean, \\(\\mu\\), and the variance, \\(\\sigma^2\\), as \\(\\mcN(\\mu, \\sigma^2)\\), a multivariate Gaussian distribution is characterized by its \\(n\\)-dimensional mean vector, \\(\\bbmu\\), and its \\(n\\times n\\) variance-covariance matrix, \\(\\bbSigma\\), as \\(\\mcN(\\bbmu, \\bbSigma)\\).\nThe density function for a univariate Gaussian distribution is the familiar “bell curve” \\[\nf(x; \\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-\\left(x-\\mu\\right)^2}{2\\sigma^2}\\right)\n\\qquad(A.3)\\] and probabilities defined by this density are most easily evaluated by standardizing the deviation, \\(x-\\mu\\), as \\(z=\\frac{x-\\mu}{\\sigma}\\). (This is why \\(\\sigma\\) is called the standard deviation.)\nTo be able to evaluate \\(\\sigma\\), the variance, \\(\\sigma^2\\), must be positive, or at least non-negative. If \\(\\sigma^2=0\\) then all the probability is concentrated at a single point, \\(x=\\mu\\), and we no longer have a probability density, in the usual way of thinking of one. The density shrinks to a point mass and the distribution is said to be degenerate.\nSimilar constraints apply to the covariance matrix, \\(\\bbSigma\\). Because the covariance of the i’th and j’th elements does not depend upon the order in which we write them, \\(\\bbSigma\\) must be symmetric. That is, \\[\n\\bbSigma' = \\bbSigma\n\\qquad(A.4)\\] Furthermore, to define a proper multivariate density, \\(\\bbSigma\\) must be positive definite, which means that for any non-zero vector, \\(\\bbx\\), the quadratic form defined by \\(\\bbSigma\\) must be positive. That is \\[\n\\bbx'\\bbSigma\\bbx>0,\\quad\\forall\\,\\bbx\\ne\\mathbf{0} .\n\\qquad(A.5)\\] (the symbol \\(\\forall\\) means “for all”). Positive definiteness implies that the precision matrix, \\(\\bbSigma^{-1}\\), exists and is also positive definite. It also implies that there are “matrix square roots” of \\(\\bbSigma\\) in the sense that there are matrices \\(\\mathbf{A}\\) such that \\(\\mathbf{A}'\\mathbf{A}=\\bbSigma\\). (The reason for writing \\(\\mathbf{A}'\\mathbf{A}\\) and not simply the square of \\(\\mathbf{A}\\) is that \\(\\mathbf{A}\\) is not required to be symmetric but \\(\\mathbf{A}'\\mathbf{A}\\) will be symmetric, even in \\(\\mathbf{A}\\) is not.)\nOne such “square root” of a positive definite \\(\\bbSigma\\) is the Cholesky factor, which corresponds to \\(n\\times n\\) upper-triangular matrix, \\(\\bbR\\), such that \\[\n\\bbSigma=\\bbR'\\bbR .\n\\qquad(A.6)\\] This factor is usually called \\(\\bbR\\) because it appears without the transpose as the right-hand multiplicant in Equation A.6. An alternative expression is written with the lower-triangular \\(\\mathbf{L}\\) on the left as \\[\n\\bbSigma=\\mathbf{L}\\mathbf{L}',\n\\qquad(A.7)\\] with the obvious relationship that \\(\\mathbf{L}=\\bbR'\\). To add to the confusion, the cholesky function in the LinearAlgebra package produces a factorization where the lower-triangular factor on the left is called L and the upper-triangular factor on the right is called U.\nThe factor \\(\\bbR\\) or \\(\\mathbf{L}\\) can be evaluated directly from the elements of \\(\\bbSigma\\). For example, the non-zeros in the first two rows of \\(\\mathbf{L}\\) are evaluated as \\[\n\\begin{aligned}\n\\mathbf{L}_{1,1}&=\\sqrt{\\bbSigma_{1,1}}\\\\\n\\mathbf{L}_{2,1}&=\\bbSigma_{2,1}/\\mathbf{L}_{1,1}\\\\\n\\mathbf{L}_{2,2}&=\\sqrt{\\bbSigma_{2,2}-\\mathbf{L}_{2,1}^2}\n\\end{aligned}\n\\qquad(A.8)\\] Evaluating the diagonal elements involves taking a square root. By convention we choose the positive square root for the Cholesky factor with the result that the diagonal elements of \\(\\mathbf{L}\\) are all positive.\n\nA.2.1 Some properties of triangular matrices\nA triangular matrix with non-zero diagonal elements is non-singular. One way to show this is because its determinant, written \\(\\left|\\mathbf{L}\\right|\\), which is the product of its diagonal elements, is non-zero. In the case of a Cholesky factor the determinant will be positive because all the diagonal elements are positive.\nA more straightforward way of showing that such a matrix is non-singular is to show how a triangular system of equations, like \\[\n\\mathbf{Lx}=\\mathbf{b}\n\\qquad(A.9)\\] can be solved. In the case of a lower-triangular system the method is called forward solution, with the sequence of scalar equations \\[\n\\begin{aligned}\nx_1&=b_1/\\mathbf{L}_{1,1}\\\\\nx_2&=\\left(b_2-x_1\\mathbf{L}_{2,1}\\right)/\\mathbf{L}_{2,2}\\\\\nx_3&=\\left(b_3-x_1\\mathbf{L}_{3,1}-x_2\\mathbf{L}_{3,2}\\right)/\\mathbf{L}_{3,3}\n\\end{aligned}\n\\qquad(A.10)\\] and so on.\nOne point to note here is that \\(b_1\\) is not needed after \\(x_1\\) is evaluated, \\(b_2\\) is not needed after \\(x_2\\) is evaluated, and so on. That is, the forward solution can be carried out in place with each element of \\(\\mathbf{b}\\) overwriting the corresponding element of \\(\\bbx\\). This property is useful for avoiding allocation of storage in each evaluation of the objective function.\nThe corresponding method of solving an upper-triangular system of equations is called backward solution, where \\(b_n\\) is evaluated first, then \\(b_{n-1}\\), and so on.\nRepeated forward solution (or backward solution for upper triangular) can be used to evaluate the inverse, \\(\\mathbf{L}^{-1}\\), of a lower triangular matrix, \\(\\mathbf{L}\\). However, a general rule in numerical linear algebra is that you rarely need to evaluate the full inverse of a matrix. Solving a triangular system like Equation A.9 by evaluating \\(\\mathbf{L}^{-1}\\) and forming the product \\[\n\\bbx = \\mathbf{L}^{-1}\\mathbf{b}\n\\qquad(A.11)\\] involves doing roughly \\(n\\) times as much work as solving the system directly, as in Equation A.10. Requiring that the inverse of a matrix must be evaluated to solve a linear system is like saying that a quotient, \\(a/b\\), must be evalated by calculating \\(b^{-1}\\), the reciprocal of \\(b\\), then evaluating the product \\(b^{-1}a\\), instead of evaluating the quotient directly.\nIn a derivation we may write an expression like \\(\\mathbf{L}^{-1}\\mathbf{b}\\) but the evaluation is performed by solving a system like Equation A.10.\n\n\nA.2.2 Positive definiteness and the Cholesky factor\nIt turns out that the ability to form the Cholesky factor, which means that all the quantities like \\(\\bbSigma_{2,2}-\\mathbf{L}_{2,1}^2\\), whose square roots form the diagonal of \\(\\mathbf{L}\\), evaluate to positive numbers, is equivalent to \\(\\bbSigma\\) being positive definite. It is straightforward to show that having a Cholesky factor implies that \\(\\bbSigma\\) is positive definite, because \\[\n\\bbx'\\bbSigma\\bbx = \\bbx'\\bbR'\\bbR\\bbx=\\left(\\mathbf{Rx}\\right)'\\mathbf{Rx}=\\left\\|\\mathbf{Rx}\\right\\|^2\n\\qquad(A.12)\\] where \\(\\left\\|\\mathbf{v}\\right\\|^2\\) is the squared length of the vector \\(\\mathbf{v}\\). Because \\(\\bbR\\) is non-singular, \\(\\bbx\\ne\\mathbf{0}\\implies\\mathbf{Rx}\\ne\\mathbf{0}\\) and the squared length in Equation A.12 is greater than zero.\nThe other direction is a bit more complicated to prove but essentially it amounts to showing that if the process of generating the Cholesky factor requires the square root of a non-positive number to obtain a diagonal element then there is a direction in which the quadratic form gives a non-positive result.\nIn practice, the easiest way to check a symmetric matrix to see if it is positive definite is to attempt to evaluate the Cholesky factor and check whether that succeeds. This is exactly what the isposdef methods in the LinearAlgebra package do.\n\n\nA.2.3 Density of the multivariate Gaussian\nFor the general multivariate normal distribution, \\(\\mcN(\\bbmu,\\bbSigma)\\), where \\(\\bbSigma\\) is positive definite with lower Cholesky factor \\(\\mathbf{L}\\), the probability density function is \\[\n\\begin{aligned}\nf(\\bbx;\\bbmu,\\bbSigma)&=\n\\frac{1}{\\sqrt{(2\\pi)^n\\left|\\bbSigma\\right|}}\n\\exp\\left(\\frac{-[\\bbx-\\bbmu]'\\bbSigma^{-1}[\\bbx-\\bbmu]}{2}\\right)\\\\\n&=\\frac{1}{\\sqrt{(2\\pi)^n}\\left|\\mathbf{L}\\right|}\n\\exp\\left(\\frac{-[\\bbx-\\bbmu]'{\\mathbf{L}'}^{-1}\\mathbf{L}^{-1}[\\bbx-\\bbmu]}{2}\\right)\\\\\n&=\\frac{1}{\\sqrt{(2\\pi)^n}\\left|\\mathbf{L}\\right|}\n\\exp\\left(\\frac{-\\left\\|\\mathbf{L}^{-1}[\\bbx-\\bbmu]\\right\\|^2}{2}\\right)\\\\\n\\end{aligned}\n\\qquad(A.13)\\] and the standardizing transformation becomes \\[\n\\mathbf{z}=\\mathbf{L}^{-1}[\\bbx-\\bbmu] ,\n\\qquad(A.14)\\] which, in practice, means using forward solution on the lower-triangular system of equations \\[\n\\mathbf{Lz}=\\bbx-\\bbmu .\n\\qquad(A.15)\\]\nNote that the standardizing transformation gives us a way to simulate values from a general \\(n\\)-dimensional multivariate Gaussian, \\(\\mcX\\sim\\mcN(\\bbmu,\\bbSigma)\\) as \\[\n\\bbx=\\bbmu+\\mathbf{L}\\mathbf{z}\n\\qquad(A.16)\\] where \\(\\mathbf{z}\\) is simulated from the \\(n\\)-dimensional standard multivariate Gaussian, \\(\\mcZ\\sim\\mcN(\\mathbf{0},\\bbI)\\), which is \\(n\\) independent univariate standard normal distributions.\n\n\nA.2.4 Linear functions of a multivariate Gaussian\nIn general, if \\(\\mcX\\) is an \\(n\\)-dimensional random variable with mean \\(\\bbmu\\) and covariance matrix \\(\\bbSigma\\), and \\(\\mathbf{A}\\) is a matrix with \\(n\\) columns then the mean and variance of \\(\\mcU=\\mathbf{A}\\mcX\\) are given by \\[\n\\require{unicode}\n𝔼\\left[\\mcU\\right] =\n𝔼\\left[\\mathbf{A}\\mcX\\right] =\n\\mathbf{A}𝔼\\left[\\mcX\\right] =\n\\mathbf{A}\\bbmu\n\\qquad(A.17)\\] and \\[\n\\begin{aligned}\n\\text{Var}\\left(\\mcU\\right)\n&=𝔼\\left[\\left(\\mcU-𝔼\\left[\\mcU\\right]\\right)\\left(\\mcU-𝔼\\left[\\mcU\\right]\\right)'\\right]\\\\\n&=𝔼\\left[\\left(\\mathbf{A}\\mcX-\\mathbf{A}\\bbmu\\right)\\left(\\mathbf{A}\\mcX-\\mathbf{A}\\bbmu\\right)'\\right]\\\\\n&=𝔼\\left[\\mathbf{A}\\left(\\mcX-\\bbmu\\right)\\left(\\mcX-\\bbmu\\right)'\\mathbf{A}'\\right]\\\\\n&=\\mathbf{A}\\,𝔼\\left[\\left(\\mcX-\\bbmu\\right)\\left(\\mcX-\\bbmu\\right)'\\right]\\mathbf{A}'\\\\\n&=\\mathbf{A}\\text{Var}(\\mcX)\\mathbf{A}'\\\\\n&=\\mathbf{A}\\bbSigma\\mathbf{A}'\n\\end{aligned}\n\\qquad(A.18)\\]\nA linear function, \\(\\mcU=\\mathbf{A}\\mcX\\), of a multivariate Gaussian distribution, \\(\\mcX\\sim\\mcN(\\bbmu,\\bbSigma)\\), is also Gaussian and these relationships imply that \\[\n\\mcU\\sim\\mcN(\\mathbf{A}\\bbmu, \\mathbf{A}\\bbSigma\\mathbf{A}')\n\\qquad(A.19)\\]\nFor the special case of \\(\\mathbf{A}\\) being of dimension \\(1\\times n\\) (i.e. a row vector), the expression for the \\(1\\times 1\\) covariance matrix is the quadratic form defined by \\(\\bbSigma\\), which is why \\(\\bbSigma\\) must be positive definite for the conditional distributions to be non-degenerate."
  },
  {
    "objectID": "linalg.html#back-at-the-linear-model",
    "href": "linalg.html#back-at-the-linear-model",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.3 Back at the linear model",
    "text": "A.3 Back at the linear model\nThe probability density function for the linear model, Equation A.2, is \\[\n\\begin{aligned}\nf(\\bby; \\bbbeta, \\sigma^2)&=\n\\frac{1}{\\sqrt{2\\pi\\left|\\sigma^2\\bbI\\right|}}\n\\exp\\left(\\frac{-[\\bby-\\bbX\\bbbeta]'\n\\left(\\sigma^2\\bbI\\right)^{-1}[\\bby-\\bbX\\bbbeta]}{2}\\right)\\\\\n&=\\left(2\\pi\\sigma^2\\right)^{-n/2}\\exp\\left(-\\left\\|\\bby-\\bbX\\bbbeta\\right\\|^2/\\left(2\\sigma^2\\right)\\right)\n\\end{aligned}\n\\qquad(A.20)\\]\nEquation A.20 describes the density of the random variable, \\(\\mcY\\), representing the observations, given the values of the parameters, \\(\\bbbeta\\) and \\(\\sigma^2\\). For parameter estimation we use the likelihood function, which is the same expression as Equation A.20 but regarded as function of the parameters, \\(\\bbbeta\\) and \\(\\sigma^2\\), with the observed response, \\(\\bby\\), fixed. \\[\nL(\\bbbeta,\\sigma^2;\\bby)=\n\\left(2\\pi\\sigma^2\\right)^{-n/2}\\exp\\left(-\\left\\|\\bby-\\bbX\\bbbeta\\right\\|^2/\\left(2\\sigma^2\\right)\\right)\n\\qquad(A.21)\\] The maximum likelihood estimates of the parameters are, as the name implies, the values of \\(\\bbbeta\\) and \\(\\sigma^2\\) that maximize the expression on the right of Equation A.21 .\nBecause the logarithm is a monotone increasing function, the maximum likelihood estimates will also maximize the log-likelihood \\[\n\\begin{aligned}\n\\ell(\\bbbeta,\\sigma^2;\\bby)\n&=\\log L(\\bbbeta,\\sigma^2;\\bby)\\\\\n&=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{\\left\\|\\bby-\\bbX\\bbbeta\\right\\|^2}{2\\sigma^2}\n\\end{aligned}\n\\qquad(A.22)\\] Usually the log-likelihood is easier to optimize, either algebraically or numerically, than the likelihood itself.\nTo avoid the negative signs and the factors of 2 in the denominator, we often convert the log-likelihood to the deviance scale, which is negative twice the log-likelihood, \\[\n\\begin{aligned}\nd(\\bbbeta,\\sigma^2;\\bby)\n&=-2\\ell(\\bbbeta,\\sigma^2; \\bby)\\\\\n&=n\\log(2\\pi\\sigma^2)+\\frac{\\left\\|\\bby-\\bbX\\bbbeta\\right\\|^2}{\\sigma^2} .\n\\end{aligned}\n\\qquad(A.23)\\] Because of the negative sign, the maximum likelihood estimates are those that minimize \\(d(\\bbbeta,\\sigma^2;\\bby)\\).\n(The term deviance scale is used for \\(d(\\bbbeta,\\sigma^2;\\bby)\\) rather than deviance because the deviance involves an additive shift, which is a correction for the saturated model - see the link. It is obvious what the saturated model should be for the linear model but not for the linear mixed model so, to avoid confusion, we refer to the log-likelihood on the deviance scale as the objective.)\nThe form of Equation A.23 makes it easy to determine the maximum likelihood estimates. Because \\(\\bbbeta\\) appears only in the sum of squared residuals expression, \\(\\|\\bby-\\bbX\\bbbeta\\|^2\\), we minimize that with respect to \\(\\bbbeta\\) \\[\n\\widehat{\\bbbeta}=\n\\arg\\min_{\\bbbeta}\\|\\bby-\\bbX\\bbbeta\\|^2 ,\n\\qquad(A.24)\\] where \\(\\arg\\min_{\\bbbeta}\\) means the value of \\(\\bbbeta\\) that minimizes the expression that follows.\nLet \\(r^2(\\widehat{\\bbbeta}) = \\left\\|\\bby-\\bbX\\widehat{\\bbbeta}\\right\\|^2\\) be the minimum sum of squared residuals. Substituting this value into Equation A.23, differentiating with respect to \\(\\sigma^2\\), and setting this derivative to zero gives \\[\n\\widehat{\\sigma^2}=\\frac{r^2(\\widehat{\\bbbeta})}{n}\n\\]"
  },
  {
    "objectID": "linalg.html#minimizing-the-sum-of-squared-residuals",
    "href": "linalg.html#minimizing-the-sum-of-squared-residuals",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.4 Minimizing the sum of squared residuals",
    "text": "A.4 Minimizing the sum of squared residuals\nA condition for \\(\\widehat{\\bbbeta}\\) to minimize the sum of squared residuals is that the gradient \\[\n\\nabla r^2(\\bbbeta)=-2\\bbX'(\\bby-\\bbX\\bbbeta)\n\\qquad(A.25)\\] be zero at \\(\\widehat{\\bbbeta}\\). This condition can be rewritten as \\[\n\\bbX'\\bbX\\widehat{\\bbbeta}=\\bbX'\\bby ,\n\\qquad(A.26)\\] which are called the normal equations.\nThe term normal in this expression comes from the fact that requiring the gradient, Equation A.25, to be zero is equivalent to requiring that the residual vector, \\(\\bby-\\bbX\\widehat{\\bbbeta}\\), be perpendicular, or normal, to the columns of \\(\\bbX\\).\nWhen the model matrix, \\(\\bbX\\), is of full column rank, which means \\[\n\\bbX\\mathbf{\\beta}\\ne\\mathbf{0}\\quad\\forall\\bbbeta\\ne\\mathbf{0} ,\n\\qquad(A.27)\\] then the quadratic form defined by \\(\\bbX'\\bbX\\) is positive definite and has a Cholesky factor, say \\(\\bbR_{XX}\\), and the normal equations can be solved in two stages. First, solve \\[\n\\bbR_{XX}'\\mathbf{r}_{Xy}=\\bbX'\\bby\n\\qquad(A.28)\\] for \\(\\mathbf{r}_{Xy}\\) using forward solution, then solve \\[\n\\bbR_{XX}\\widehat{\\bbbeta}=\\mathbf{r}_{Xy}\n\\qquad(A.29)\\] for \\(\\widehat{\\bbbeta}\\) using backward solution.\nAn alternative approach is to write the residual sum of squares as a quadratic form \\[\n\\begin{aligned}\nr^2(\\bbbeta)&=\\|\\bby-\\bbX\\bbbeta\\|^2\\\\\n&=(\\bby-\\bbX\\bbbeta)'(\\bby-\\bbX\\bbbeta)\\\\\n&=(\\bbX\\bbbeta-\\bby)'(\\bbX\\bbbeta-\\bby)\\\\\n&=\\begin{bmatrix}\\bbbeta&-1\\end{bmatrix}\n\\begin{bmatrix}\n\\bbX'\\bbX & \\bbX'\\bby\\\\\n\\bby'\\bbX & \\bby'\\bby\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbbeta\\\\\n-1\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}\\bbbeta&-1\\end{bmatrix}\n\\begin{bmatrix}\n\\bbR_{XX}' & \\mathbf{0}\\\\\n\\mathbf{r}_{Xy}' & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbR_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbbeta\\\\\n-1\n\\end{bmatrix}\\\\\n&=\\left\\|\n\\begin{bmatrix}\n\\bbR_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbbeta\\\\\n-1\n\\end{bmatrix}\\right\\|^2\\\\\n&=\\left\\|\\bbR_{XX}\\bbbeta-\\mathbf{r}_{Xy}\\right\\|^2+r_{yy}^2\n\\end{aligned}\n\\qquad(A.30)\\]\nThe first term, \\(\\left\\|\\bbR_{XX}\\bbbeta-\\mathbf{r}_{Xy}\\right\\|^2\\), is non-negative and can be made zero by solving Equation A.29 for \\(\\widehat{\\bbbeta}\\). Thus, the minimum sum of squared residuals is \\(r_{yy}^2\\).\nOne consequence of this derivation is that the minimum sum of squared residuals can be evaluated directly from the extended Cholesky factor \\[\n\\begin{bmatrix}\n\\bbR_{XX} & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\qquad(A.31)\\] without needing to solve for \\(\\widehat{\\bbbeta}\\) first. This is not terribly important for a linear model where the evaluation of \\(\\widehat{\\bbbeta}\\) and the residual is typically done only once. However, for the linear mixed model, a similar calculation must be done for every evaluation of the objective in the iterative optimization, and being able to evaluate the minimum penalized sum of squared residuals without solving for parameter values and without needing to evaluate the residual saves a non-negligible amount of time and effort."
  },
  {
    "objectID": "linalg.html#numerical-example",
    "href": "linalg.html#numerical-example",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.5 Numerical example",
    "text": "A.5 Numerical example\nSuppose we wish to fit a simple linear regression model to the reaction time as a function of days of sleep deprivation to the data from subject S372 in the sleepstudy dataset.\n\nS372 =\n  last(groupby(DataFrame(MixedModels.dataset(:sleepstudy)), :subj))\n\n10 rows × 3 columnssubjdaysreactionStringInt8Float641S3720269.4122S3721273.4743S3722297.5974S3723310.6325S3724287.1736S3725329.6087S3726334.4828S3727343.229S3728369.14210S3729364.124\n\n\nThe model matrix and the response vector can be constructed as\n\nX = hcat(ones(nrow(S372)), S372.days)\n\n10×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nand\n\ny = S372.reaction\nshow(y)\n\n[269.4117, 273.474, 297.5968, 310.6316, 287.1726, 329.6076, 334.4818, 343.2199, 369.1417, 364.1236]\n\n\nfrom which we obtain the Cholesky factor\n\nchfac = cholesky!(X'X)\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n2×2 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  14.2302\n  ⋅        9.08295\n\n\n(Recall that the upper triangular Cholesky factor is the U property of the Cholesky type.)\nThe \\ operator with a Cholesky factor on the left performs both the forward and backward solutions to obtain the least squares estimates\n\nβ̂ = chfac \\ (X'y)\n\n2-element Vector{Float64}:\n 267.04480000000007\n  11.298073333333324\n\n\nAlternatively, we could carry out the two solutions of the triangular systems explicitly by first solving for \\(\\mathbf{r}_{Xy}\\)\n\nrXy = ldiv!(chfac.L, X'y)\n\n2-element Vector{Float64}:\n 1005.2442073763814\n  102.6198471848582\n\n\nthen solving in-place to obtain \\(\\widehat{\\bbbeta}\\)\n\nldiv!(chfac.U, rXy)\n\n2-element Vector{Float64}:\n 267.04480000000007\n  11.298073333333324\n\n\nThe residual vector, \\(\\bby-\\bbX\\widehat{\\bbbeta}\\), is\n\nr = y - X * β̂\n\n10-element Vector{Float64}:\n   2.36689999999993\n  -4.868873333333397\n   7.955853333333266\n   9.692579999999964\n -25.06449333333336\n   6.072433333333322\n  -0.3514399999999682\n  -2.9114133333333143\n  11.712313333333327\n  -4.603859999999997\n\n\nwith geometric length or “norm”,\n\nnorm(r)\n\n31.91593290658026\n\n\nFor the extended Cholesky factor, create the extended matrix of sums of squares and cross products\n\ncrprod = let x = S372.days\n  Symmetric(\n    [\n      length(x) sum(x) sum(y)\n      0.0 sum(abs2, x) dot(x, y)\n      0.0 0.0 sum(abs2, y)\n    ],\n    :U,\n  )\nend\n\n3×3 Symmetric{Float64, Matrix{Float64}}:\n   10.0      45.0   3178.86\n   45.0     285.0  15237.0\n 3178.86  15237.0      1.02207e6\n\n\nThe call to Symmetric with the second argument the symbol :U indicates that the matrix should be treated as symmetric but only the upper triangle is given.\nThe Cholesky factor of the crprod reproduces \\(\\bbR_{XX}\\), \\(\\mathbf{r}_{Xy}\\), and the norm of the residual, \\(r_{yy}\\).\n\nextchfac = cholesky(crprod)\n\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 3.16228  14.2302   1005.24\n  ⋅        9.08295   102.62\n  ⋅         ⋅         31.9159\n\n\nand information from which the parameter estimates can be evaluated.\n\nβ̂ ≈ ldiv!(\n  UpperTriangular(view(extchfac.U, 1:2, 1:2)),\n  copy(view(extchfac.U, 1:2, 3)),\n)\n\ntrue\n\n\nThe operator ≈ is a check of approximate equality of floating point numbers or arrays. Exact equality of floating point results from “equivalent” calculations cannot be relied upon.\nSimilarly we check that the value of \\(r_{y,y}\\) is approximately equal to the norm of the residual vector.\n\nnorm(r) ≈ extchfac.U[3, 3]\n\ntrue"
  },
  {
    "objectID": "linalg.html#alternative-decompositions-of-x",
    "href": "linalg.html#alternative-decompositions-of-x",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.6 Alternative decompositions of X",
    "text": "A.6 Alternative decompositions of X\nThere are two other decompositions of the model matrix \\(\\bbX\\) or the augmented model matrix \\([\\mathbf{X,y}]\\) that can be used to evaluate the least squares estimates; the QR decomposition and the singular value decomposition (SVD).\nThe QR decomposition expresses \\(\\bbX\\) as the product of an orthogonal matrix, \\(\\mathbf{Q}\\), and an upper triangular matrix \\(\\bbR\\). The upper triangular \\(\\bbR\\) is related to the upper triangular Cholesky factor in that the numerical values are the same but the signs can be different. In particular, the usual way of creating \\(\\mathbf{Q}\\) and \\(\\bbR\\) using Householder transformations typically results in the first row of \\(\\bbR\\) from the qr function being the negative of the first row of the upper Cholesky factor.\n\nqrfac = qr(X)\nqrfac.R\n\n2×2 Matrix{Float64}:\n -3.16228  -14.2302\n  0.0        9.08295\n\n\nJust as the Cholesky factor can be used on the left of the \\ operator, so can the qr factor but with y on the right.\n\nb3 = qrfac \\ y\n\n2-element Vector{Float64}:\n 267.0447999999998\n  11.298073333333361\n\n\nThe matrix \\(\\bbR\\) is returned as a square matrix with the same number of columns as \\(\\bbX\\). That is, if \\(\\bbX\\) is of size \\(n\\times p\\) where \\(n>p\\), as in the example, then \\(\\bbR\\) is \\(p\\times p\\), as shown above.\nThe matrix \\(\\mathbf{Q}\\) is usually considered to be an \\(n\\times n\\) orthogonal matrix, which means that its transpose is its inverse \\[\n\\mathbf{Q'Q}=\\mathbf{QQ'}=\\bbI\n\\qquad(A.32)\\] To form the product \\(\\mathbf{QR}\\) the matrix \\(\\bbR\\) is treated as if it were \\(n\\times p\\) with zeros below the main diagonal.\nThe \\(n\\times n\\) matrix \\(\\mathbf{Q}\\) can be very large if \\(n\\), the number of observations, is large but it does not need to be explicitly evaluated. In practice \\(\\mathbf{Q}\\) is a “virtual” matrix represented as a product of Householder reflections that only require storage of the size of \\(\\bbX\\). The effect of multiplying a vector or matrix by \\(\\mathbf{Q}\\) or by \\(\\mathbf{Q}'\\) is achieved by applying the Householder reflections in a particular order.\n\nrXy2 = qrfac.Q'y\n\n10-element Vector{Float64}:\n -1005.2442073763813\n   102.61984718485846\n     8.057970700644871\n     9.321943315787063\n   -25.90788406907071\n     4.756288546071453\n    -2.1403388387862634\n    -5.173066223644142\n     8.977906391498067\n    -7.81102099335979\n\n\n\nb4 = ldiv!(UpperTriangular(qrfac.R), rXy2[1:2])\n\n2-element Vector{Float64}:\n 267.04479999999995\n  11.298073333333349\n\n\nForming the QR decomposition is a direct, non-iterative, calculation, like forming the Cholesky factor. Forming the SVD, by contrast, is usually an iterative calculation. (It should be noted that modern methods for evaluating the SVD are very fast for an iterative calculation.) The SVD consists of two orthogonal matrices, the \\(n\\times n\\) \\(\\mathbf{U}\\) and the \\(p\\times p\\) \\(\\mathbf{V}\\) and an \\(n\\times p\\) matrix \\(\\mathbf{S}\\) that is zero off the main diagonal, where \\[\n\\bbX=\\mathbf{USV'} .\n\\]\nUnlike the \\(\\mathbf{Q}\\) in the QR decomposition, the orthogonal matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are explicitly evaluated. Because of this, the default for the svd function is to produce a compact form where \\(\\mathbf{U}\\) is \\(n\\times p\\) and only the diagonal of \\(\\mathbf{S}\\) is returned.\n\nXsvd = svd(X)\n\nSVD{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nU factor:\n10×2 Matrix{Float64}:\n 0.00921331   0.587682\n 0.0669862    0.493961\n 0.124759     0.400241\n 0.182532     0.306521\n 0.240305     0.2128\n 0.298078     0.11908\n 0.355851     0.0253594\n 0.413623    -0.068361\n 0.471396    -0.162081\n 0.529169    -0.255802\nsingular values:\n2-element Vector{Float64}:\n 17.093167142525193\n  1.680368125649001\nVt factor:\n2×2 Matrix{Float64}:\n 0.157485   0.987521\n 0.987521  -0.157485\n\n\nIf all the singular values are non-zero, as is the case here, the least squares solution \\(\\widehat{\\bbbeta}\\) can be obtained as\n\\[\n\\mathbf{V}\\mathbf{S}^{-1}\\mathbf{U}'\\bby\n\\qquad(A.33)\\]\nfor the diagonal \\(\\mathbf{S}\\).\n\nb5 = Xsvd.V * (Xsvd.U'y ./ Xsvd.S)\n\n2-element Vector{Float64}:\n 267.0447999999999\n  11.298073333333349\n\n\nIn the extensions to linear mixed-effects models we will emphasize the Cholesky factorization over the QR decomposition or the SVD."
  },
  {
    "objectID": "linalg.html#sec-lmmtheory",
    "href": "linalg.html#sec-lmmtheory",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.7 Linear mixed-effects models",
    "text": "A.7 Linear mixed-effects models\nAs described in Bates et al. (2015) , a linear mixed-effects model is based on two vector-valued random variables: the \\(q\\)-dimensional vector of random effects, \\(\\mcB\\), and the \\(n\\)-dimensional response vector, \\(\\mcY\\). Equation 1.2 defines the unconditional distribution of \\(\\mcB\\) and the conditional distribution of \\(\\mcY\\), given \\(\\mcB=\\mathbf{b}\\), as multivariate Gaussian distributions of the form \\[\n\\begin{aligned}\n(\\mcY|\\mcB=\\mathbf{b})&\\sim\\mcN(\\bbX\\bbbeta+\\mathbf{Z}\\mathbf{b},\\sigma^2\\bbI)\\\\\n\\mcB&\\sim\\mcN(\\mathbf{0},\\bbSigma_\\theta) .\n\\end{aligned}\n\\]\nThe \\(q\\times q\\), symmetric, variance-covariance matrix, \\(\\mathrm{Var}(\\mcB)=\\bbSigma_\\theta\\), depends on the variance-component parameter vector, \\(\\bbtheta\\), through a lower triangular relative covariance factor, \\(\\bbLambda_\\theta\\) as\n\\[\n\\bbSigma_\\theta=\\sigma^2\\bbLambda_\\theta\\bbLambda_\\theta' .\n\\qquad(A.34)\\]\n(Recall that the lower Cholesky factor is generally written \\(\\mathbf{L}\\). In this case the lower Cholesky factor contains parameters and is named with the corresponding Greek letter, \\(\\bbLambda\\).)\nMany computational formulas for linear mixed models are written in terms of the precision matrix, \\(\\bbSigma_\\theta^{-1}\\). Such formulas will become unstable as \\(\\bbSigma_\\theta\\) approaches singularity. And it can do so. It is a fact that singular (i.e. non-invertible) \\(\\bbSigma_\\theta\\) can and do occur in practice, as we have seen in some of the examples in earlier chapters. Moreover, during the course of the numerical optimization by which the parameter estimates are determined, it is frequently the case that the deviance or the REML criterion will need to be evaluated at values of \\(\\bbtheta\\) that produce a singular \\(\\bbSigma_\\theta\\). Because of this we will take care to use computational methods that can be applied even when \\(\\bbSigma_\\theta\\) is singular and are stable as \\(\\bbSigma_\\theta\\) approaches singularity.\nAccording to Equation A.34, \\(\\bbSigma\\) depends on both \\(\\sigma\\) and \\(\\theta\\), and we should write it as \\(\\bbSigma_{\\sigma,\\theta}\\). However, we will blur that distinction and continue to write \\(\\text{Var}(\\mcB)=\\bbSigma_\\theta\\).\nAnother technicality is that the common scale parameter, \\(\\sigma\\), could, in theory, be zero. However, the only way for its estimate, \\(\\widehat{\\sigma}\\), to be zero is for the fitted values from the fixed-effects only, \\(\\bbX\\widehat{\\bbbeta}\\), to be exactly equal to the observed data. This occurs only with data that have been (incorrectly) simulated without error. In practice we can safely assume that \\(\\sigma>0\\). However, \\(\\bbLambda_\\theta\\), like \\(\\bbSigma_\\theta\\), can be singular.\nThe computational methods in the MixedModels package are based on \\(\\bbLambda_\\theta\\) and do not require evaluation of \\(\\bbSigma_\\theta\\). In fact, \\(\\bbSigma_\\theta\\) is explicitly evaluated only at the converged parameter estimates.\nThe spherical random effects, \\(\\mcU\\sim\\mcN(\\mathbf{0},\\sigma^2\\bbI_q)\\), determine \\(\\mcB\\) as\n\\[\n\\mcB=\\bbLambda_\\theta\\mcU .\n\\qquad(A.35)\\]\nAlthough it may seem more intuitive to write \\(\\mcU\\) as a linear transformation of \\(\\mcB\\), we cannot do that when \\(\\bbLambda_\\theta\\) is singular, which is why Equation A.35 is in the form shown.\nWe can easily verify that Equation A.35 provides the desired distribution for \\(\\mcB\\). As a linear transformation of a multivariate Gaussian random variable, \\(\\mcB\\) will also be multivariate Gaussian with mean\n\\[\n𝔼\\left[\\mcB\\right]=\n𝔼\\left[\\bbLambda_\\theta\\mcU\\right]=\n\\bbLambda_\\theta\\,𝔼\\left[\\mcU\\right]=\n\\bbLambda_\\theta\\mathbf{0}=\\mathbf{0}\n\\]\nand covariance matrix\n\\[\n\\text{Var}(\\mcB)=\n\\bbLambda_\\theta\\text{Var}(\\mcU)\\bbLambda\\theta'=\n\\sigma^2\\bbLambda_\\theta\\bbLambda_\\theta'=\\bbSigma_\\theta\n\\]\nJust as we concentrate on how \\(\\bbtheta\\) determines \\(\\bbLambda_\\theta\\), not \\(\\bbSigma_\\theta\\), we will concentrate on properties of \\(\\mcU\\) rather than \\(\\mcB\\). In particular, we now define the model according to the distributions\n\\[\n\\begin{aligned}\n(\\mcY|\\mcU=\\mathbf{u})&\\sim\\mcN(\\mathbf{Z}\\bbLambda_\\theta\\mathbf{u}+\\bbX\\beta,\\sigma^2\\bbI_n)\\\\\n\\mcU&\\sim\\mcN(\\mathbf{0},\\sigma^2\\bbI_q) .\n\\end{aligned}\n\\qquad(A.36)\\]\nThe joint density for \\(\\mcY\\) and \\(\\mcU\\) is the product of densities of the two distributions shown in Equation A.36. That is\n\\[\nf_{\\mcY,\\mcU}(\\bby,\\mathbf{u})=\n\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{-(n+q)/2}}\\exp\n\\left(\\frac{\\left\\|\\bby-\\bbX\\bbbeta\n-\\mathbf{Z}\\bbLambda_\\theta\\mathbf{u}\\right\\|^2+\n\\left\\|\\mathbf{u}\\right\\|^2}{-2\\sigma^2}\\right) .\n\\qquad(A.37)\\]\nTo evaluate the likelihood for the parameters, \\(\\bbtheta\\), \\(\\bbbeta\\), and \\(\\sigma^2\\), given the observed response, \\(\\bby\\), we must evaluate the marginal distribution of \\(\\mcY\\), which is the integral of \\(f_{\\mcY,\\mcU}(\\bby,\\mathbf{u})\\) with respect to \\(\\mathbf{u}\\).\nThis is much simpler if we first rewrite the penalized sum of squared residuals, \\(\\left\\|\\bby-\\bbX\\bbbeta -\\mathbf{Z}\\bbLambda_\\theta\\mathbf{u}\\right\\|^2+ \\left\\|\\mathbf{u}\\right\\|^2\\), in Equation A.37, which is a quadratic form in \\(\\mathbf{u}\\), to isolate the dependence on \\(\\mathbf{u}\\)\n\\[\n\\begin{aligned}\nr^2_\\theta(\\mathbf{u},\\bbbeta)\n&=\n\\|\\bby-\\bbX\\bbbeta-\\mathbf{Z}\\bbLambda_\\theta\\mathbf{u}\\|^2+\\|\\mathbf{u}\\|^2 \\\\\n&=\n\\left\\|\n\\begin{bmatrix}\n\\mathbf{Z}\\bbLambda_\\theta & \\bbX & \\bby \\\\\n-\\bbI_q & \\mathbf{0} & \\mathbf{0}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-\\bbbeta \\\\\n1\n\\end{bmatrix}\n\\right\\|^2 \\\\\n&=\n\\begin{bmatrix}\n-\\mathbf{u'} &\n-\\bbbeta' &\n1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbLambda'\\mathbf{Z}'\\mathbf{Z}\\bbLambda+\\bbI & \\bbLambda'\\mathbf{Z}'\\bbX & \\bbLambda'\\mathbf{Z}'\\bby \\\\\n\\bbX'\\mathbf{Z}\\bbLambda & \\bbX'\\bbX & \\bbX'\\bby \\\\\n\\bby'\\mathbf{Z}\\bbLambda & \\bby'\\bbX & \\bby'\\bby\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-\\bbbeta \\\\\n1\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n-\\mathbf{u'} &\n-\\bbbeta' &\n1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbR_{ZZ}' & \\mathbf{0} & \\mathbf{0} \\\\\n\\bbR_{ZX}' & \\bbR_{XX}' & \\mathbf{0} \\\\\n\\mathbf{r}_{Zy}' & \\mathbf{r}_{Xy}' & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbR_{ZZ} & \\bbR_{ZX} & \\mathbf{r}_{Zy} \\\\\n\\mathbf{0} & \\bbR_{XX} & \\mathbf{r}_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-\\bbbeta \\\\\n1\n\\end{bmatrix}\\\\\n&= \\left\\|\n\\begin{bmatrix}\n\\bbR_{ZZ} & \\bbR_{ZX} & \\mathbf{r}_{Zy}\\\\\n\\mathbf{0} & \\bbR_{XX}' & \\mathbf{r}_{Xy}\\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n-\\mathbf{u} \\\\\n-\\bbbeta \\\\\n1\n\\end{bmatrix}\n\\right\\|^2\\\\\n&= \\| \\mathbf{r}_{Zy}-\\bbR_{ZX}\\bbbeta-\\bbR_{ZZ}\\mathbf{u} \\|^2 +\n\\| \\mathbf{r}_{Xy}-\\bbR_{XX}\\bbbeta\\|^2 + r_{yy}^2 ,\n\\end{aligned}\n\\qquad(A.38)\\]\nusing the Cholesky factor of the blocked matrix,\n\\[\n\\begin{aligned}\n\\bbOmega_\\theta&=\n\\begin{bmatrix}\n\\bbLambda_\\theta'\\mathbf{Z'Z}\\bbLambda_\\theta+\\bbI &\n\\bbLambda_\\theta'\\mathbf{Z'X} & \\bbLambda_\\theta'\\mathbf{Z'y} \\\\\n\\mathbf{X'Z}\\bbLambda_\\theta & \\mathbf{X'X} & \\mathbf{X'y} \\\\\n\\mathbf{y'Z}\\bbLambda_\\theta & \\mathbf{y'X} & \\mathbf{y'y}\n\\end{bmatrix}\\\\\n& =\n\\begin{bmatrix}\n\\bbR_{ZZ}' & \\mathbf{0} & \\mathbf{0} \\\\\n\\bbR_{ZX}' & \\bbR'_{XX} & \\mathbf{0} \\\\\n\\mathbf{r}_{Zy}' & \\mathbf{r}'_{Xy} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bbR_{ZZ} & \\bbR_{ZX} & \\mathbf{r}_{Zy} \\\\\n\\mathbf{0} & \\bbR_{XX} & \\mathbf{r}_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix} .\n\\end{aligned}\n\\qquad(A.39)\\]\nNote that the block in the upper left, \\(\\bbLambda_\\theta'\\mathbf{Z'Z}\\bbLambda_\\theta+\\bbI\\), is positive definite even when \\(\\bbLambda_\\theta\\) is singular, because\n\\[\n\\mathbf{u}'\\left(\\bbLambda_\\theta'\\mathbf{Z'Z}\\bbLambda_\\theta+\\bbI\\right)\\mathbf{u} = \\left\\|\\mathbf{Z}\\bbLambda_\\theta\\mathbf{u}\\right\\|^2\n+\\left\\|\\mathbf{u}\\right\\|^2\n\\qquad(A.40)\\]\nand the first term is non-negative while the second is positive if \\(\\mathbf{u}\\ne\\mathbf{0}\\).\nThus \\(\\bbR_{ZZ}\\), with positive diagonal elements, can be evaluated and its determinant, \\(\\left|\\bbR_{ZZ}\\right|\\), is positive. This determinant appears in the marginal density of \\(\\mcY\\), from which the likelihood of the parameters is evaluated.\nTo evaluate the likelihood,\n\\[\nL(\\bbtheta,\\bbbeta,\\sigma|\\bby) = \\int_\\mathbf{u} f_{\\mcY,\\mcU}(\\bby,\\mathbf{u})\\, d\\mathbf{u}\n\\qquad(A.41)\\]\nwe isolate the part of the joint density that depends on \\(\\mathbf{u}\\) and perform a change of variable\n\\[\n\\mathbf{v}=\\bbR_{ZZ}\\mathbf{u}+\\bbR_{ZX}\\bbbeta-\\mathbf{r}_{Zy} .\n\\qquad(A.42)\\]\nFrom the properties of the multivariate Gaussian distribution\n\\[\n\\begin{aligned}\n\\int_{\\mathbf{u}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\n\\exp\\left(-\\frac{\\|\\bbR_{ZZ}\\mathbf{u}+\\bbR_{ZX}\\bbbeta-\\mathbf{r}_{Zy}\\|^2}{2\\sigma^2}\\right)\n\\,d\\mathbf{u}\n&= \\int_{\\mathbf{v}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\n\\exp\\left(-\\frac{\\|\\mathbf{v}\\|^2}{2\\sigma^2}\\right)|\\bbR_{ZZ}|^{-1}\\,d\\mathbf{v}\\\\\n&=|\\bbR_{ZZ}|^{-1}\n\\end{aligned}\n\\qquad(A.43)\\]\nfrom which we obtain the likelihood as\n\\[\nL(\\bbtheta,\\bbbeta,\\sigma;\\bby)=\n\\frac{|\\bbR_{ZZ}|^{-1}}{(2\\pi\\sigma^2)^{n/2}}\n\\exp\\left(-\\frac{r_{yy}^2 + \\|\\bbR_{XX}(\\bbbeta-\\widehat{\\bbbeta})\\|^2}{2\\sigma^2}\\right) ,\n\\qquad(A.44)\\]\nwhere the conditional estimate, \\(\\widehat{\\bbbeta}\\), given \\(\\bbtheta\\), satisfies \\[\n\\bbR_{XX}\\widehat{\\bbbeta} = \\mathbf{r}_{Xy} .\n\\]\nSetting \\(\\bbbeta=\\widehat{\\bbbeta}\\) and taking the logarithm provides the estimate of \\(\\sigma^2\\), given \\(\\bbtheta\\), as\n\\[\n\\widehat{\\sigma^2}=\\frac{r_\\mathbf{yy}^2}{n}\n\\qquad(A.45)\\]\nwhich gives the profiled log-likelihood, \\(\\ell(\\bbtheta|\\bby)=\\log L(\\bbtheta,\\widehat{\\bbbeta},\\widehat{\\sigma})\\), on the deviance scale, as\n\\[\n-2\\ell(\\bbtheta|\\bby)=2\\log(|\\bbR_{ZZ}|) +\nn\\left(1+\\log\\left(\\frac{2\\pi r_{yy}^2(\\bbtheta)}{n}\\right)\\right)\n\\qquad(A.46)\\]\nOne of the interesting aspects of this formulation is that it is not necessary to solve for the conditional estimate of \\(\\bbbeta\\) or the conditional modes of the random effects when evaluating the log-likelihood. The two values needed for the log-likelihood evaluation, \\(2\\log(|\\bbR_{ZZ}|)\\) and \\(r_\\mathbf{yy}^2\\), are obtained directly from the diagonal elements of the Cholesky factor.\nFurthermore, \\(\\bbOmega_{\\theta}\\) and, from that, the Cholesky factor, \\(\\bbR_{\\theta}\\), and the objective to be optimized can be evaluated for a given value of \\(\\bbtheta\\) from\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n\\mathbf{Z}^\\prime\\mathbf{Z} & \\mathbf{Z}^\\prime\\bbX & \\mathbf{Z}^\\prime\\bby \\\\\n\\bbX^\\prime\\mathbf{Z} & \\bbX^\\prime\\bbX & \\bbX^\\prime\\bby \\\\\n\\bby^\\prime\\mathbf{Z} & \\bby^\\prime\\bbX & \\bby^\\prime\\bby\n\\end{bmatrix}\n\\qquad(A.47)\\]\nand \\(\\bbLambda_{\\theta}\\).\nIn the MixedModels package the LinearMixedModel struct contains a symmetric blocked array in the A field and a similarly structured lower-triangular blocked array in the L field. Evaluation of the objective simply involves updating the template matrices, \\(\\bbLambda_i, i=1,\\dots,k\\) in the ReMat structures then updating L from A and the \\(\\lambda_i\\)."
  },
  {
    "objectID": "linalg.html#sec-REML",
    "href": "linalg.html#sec-REML",
    "title": "Appendix A — Linear Algebra for Linear Models",
    "section": "A.8 The REML criterion",
    "text": "A.8 The REML criterion\nThe so-called REML estimates of variance components are often preferred to the maximum likelihood estimates. (“REML” can be considered to be an acronym for “restricted” or “residual” maximum likelihood, although neither term is completely accurate because these estimates do not maximize a likelihood.) We can motivate the use of the REML criterion by considering a linear regression model,\n\\[\n\\mcY\\sim\\mcN(\\bbX\\bbbeta,\\sigma^2\\bbI_n),\n\\qquad(A.48)\\]\nin which we typically estimate \\(\\sigma^2\\) as\n\\[\n\\widehat{\\sigma^2_R}=\\frac{\\|\\bby-\\bbX\\widehat{\\bbbeta}\\|^2}{n-p}\n\\qquad(A.49)\\]\neven though the maximum likelihood estimate of \\(\\sigma^2\\) is\n\\[\n\\widehat{\\sigma^2_{L}}=\\frac{\\|\\bby-\\vec\nX\\widehat{\\bbbeta}\\|^2}{n} .\n\\qquad(A.50)\\]\nThe argument for preferring \\(\\widehat{\\sigma^2_R}\\) to \\(\\widehat{\\sigma^2_{L}}\\) as an estimate of \\(\\sigma^2\\) is that the numerator in both estimates is the sum of squared residuals at \\(\\widehat{\\bbbeta}\\) and, although the residual vector, \\(\\bby-\\bbX\\widehat{\\bbbeta}\\), is an \\(n\\)-dimensional vector, it satisfies \\(p\\) linearly independent constraints, \\(\\bbX'(\\bby-\\bbX\\widehat{\\bbbeta})=\\mathbf{0}\\). That is, the residual at \\(\\widehat{\\bbbeta}\\) is the projection of the observed response vector, \\(\\bby\\), into an \\((n-p)\\)-dimensional linear subspace of the \\(n\\)-dimensional response space. The estimate \\(\\widehat{\\sigma^2_R}\\) takes into account the fact that \\(\\sigma^2\\) is estimated from residuals that have only \\(n-p\\) degrees of freedom.\nAnother argument often put forward for REML estimation is that \\(\\widehat{\\sigma^2_R}\\) is an unbiased estimate of \\(\\sigma^2\\), in the sense that the expected value of the estimator is equal to the value of the parameter. However, determining the expected value of an estimator involves integrating with respect to the density of the estimator and we have seen that densities of estimators of variances will be skewed, often highly skewed. It is not clear why we should be interested in the expected value of a highly skewed estimator. If we were to transform to a more symmetric scale, such as the estimator of the standard deviation or the estimator of the logarithm of the standard deviation, the REML estimator would no longer be unbiased. Furthermore, this property of unbiasedness of variance estimators does not generalize from the linear regression model to linear mixed models. This is all to say that the distinction between REML and ML estimates of variances and variance components is probably less important than many people believe.\nNevertheless it is worthwhile seeing how the computational techniques described in this chapter apply to the REML criterion because the REML parameter estimates \\(\\widehat{\\bbtheta}_R\\) and \\(\\widehat{\\sigma_R^2}\\) for a linear mixed model have the property that they would specialize to \\(\\widehat{\\sigma^2_R}\\) from Equation A.49 for a linear regression model, as seen in Section 1.3.3.\nAlthough not usually derived in this way, the REML criterion (on the deviance scale) can be expressed as\n\\[\nd_R(\\bbtheta,\\sigma|\\bby)=-2\\log\n\\int_{\\mathbb{R}^p}L(\\bbtheta,\\bbbeta,\\sigma|\\bby)\\,d\\bbbeta .\n\\qquad(A.51)\\]\nThe REML estimates \\(\\widehat{\\bbtheta}_R\\) and \\(\\widehat{\\sigma_R^2}\\) minimize \\(d_R(\\bbtheta,\\sigma|\\bby)\\).\nTo evaluate this integral we form an expansion, similar to Equation A.44, of \\(r^2_{\\theta,\\beta}\\) about \\(\\widehat{\\bbbeta}_\\theta\\)\n\\[\nr^2_{\\theta,\\beta}=r^2_\\theta+\\|\\bbR_{XX}(\\bbbeta-\\widehat{\\bbbeta}_\\theta)\\|^2 .\n\\qquad(A.52)\\]\nfrom which we can derive\n\\[\n\\int_{\\mathbb{R}^p}\\frac{\\exp\\left(-\\frac{r^2_{\\theta,\\beta}}{2\\sigma^2}\\right)}\n{(2\\pi\\sigma^2)^{n/2}|\\bbR_{ZZ}|} \\,d\\bbbeta=\n\\frac{\\exp\\left(-\\frac{r^2_\\theta}{2\\sigma^2}\\right)}\n{(2\\pi\\sigma^2)^{(n-p)/2}|\\bbR_{ZZ}||\\bbR_X|}\n\\qquad(A.53)\\]\ncorresponding to a REML criterion on the deviance scale of\n\\[\nd_R(\\bbtheta,\\sigma|\\bby)=(n-p)\\log(2\\pi\\sigma^2)+\n2\\log\\left(|\\bbR_{ZZ}||\\bbR_X|\\right)+\\frac{r^2_\\theta}{\\sigma^2} .\n\\qquad(A.54)\\]\nPlugging in the conditional REML estimate, \\(\\widehat{\\sigma^2}_R=r^2_\\theta/(n-p)\\), provides the profiled REML criterion\n\\[\n\\tilde{d}_R(\\bbtheta|\\bby)=\n2\\log\\left(|\\bbR_{ZZ}||\\bbR_X|\\right)+(n-p)\n\\left[1+\\log\\left(\\frac{2\\pi r^2_\\theta}{n-p}\\right)\\right].\n\\qquad(A.55)\\]\nThe REML estimate of \\(\\bbtheta\\) is\n\\[\n\\widehat{\\bbtheta}_R=\\arg\\min_{\\bbtheta}\\tilde{d}_R(\\bbtheta|\\bby) ,\n\\qquad(A.56)\\]\nand the REML estimate of \\(\\sigma^2\\) is the conditional REML estimate of \\(\\sigma^2\\) at \\(\\widehat{\\bbtheta}_R\\),\n\\[\n\\widehat{\\sigma^2_R}=r^2_{\\widehat\\theta_R}/(n-p) .\n\\qquad(A.57)\\]\nIt is not entirely clear how one would define a “REML estimate” of \\(\\bbbeta\\) because the REML criterion, \\(d_R(\\bbtheta,\\sigma|\\bby)\\), defined in Equation A.54, does not depend on \\(\\bbbeta\\). However, it is customary (and not unreasonable) to use \\(\\widehat{\\bbbeta}_R=\\widehat{\\bbbeta}_{\\widehat{\\bbtheta}_R}\\) as the REML estimate of \\(\\bbbeta\\).\n\n\n\n\nBates, D., Maechler, M., Bolker, B. M., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01"
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Attach the packages to be used in this appendix."
  },
  {
    "objectID": "contrasts_kwdyz11.html#example-data",
    "href": "contrasts_kwdyz11.html#example-data",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.1 Example data",
    "text": "B.1 Example data\nWe take the KWDYZ dataset (Kliegl et al., 2010). This is an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Interestingly, a different theoretical perspective, derived from feature overlap, leads to a different set of contrasts. Can the results refute one of the theoretical perspectives?\nWe also have a dataset from a replication and extension of this study (Kliegl et al., 2015). Both data sets are also available in R-package RePsychLing"
  },
  {
    "objectID": "contrasts_kwdyz11.html#preprocessing",
    "href": "contrasts_kwdyz11.html#preprocessing",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.2 Preprocessing",
    "text": "B.2 Preprocessing\n\ndat1 = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select!(:subj => :Subj, :tar => :CTR, :rt)\nend\ncellmeans = combine(\n  groupby(dat1, [:CTR]),\n  :rt => mean,\n  :rt => std,\n  :rt => length,\n  :rt => (x -> std(x) / sqrt(length(x))) => :rt_semean,\n)\n\n4 rows × 5 columnsCTRrt_meanrt_stdrt_lengthrt_semeanStringFloat64Float64Int64Float641val358.03283.4581201410.5880692sod391.26792.66228631.731773dos405.14692.689328431.738374dod402.395.391428631.78278"
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.3 SeqDiffCoding",
    "text": "B.3 SeqDiffCoding\nThis contrast corresponds to MASS::contr.sdif() in R.\n\nform = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\nlevels = [\"val\", \"sod\", \"dos\", \"dod\"]\nm1 = let\n  contrasts = Dict(\n    :CTR => SeqDiffCoding(; levels), :Subj => Grouping()\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\u001b[32mMinimizing 185      Time: 0:00:00 ( 1.36 ms/it)\u001b[39m\n\u001b[34m  objective:  325809.54938125925\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0900\n54.97\n<1e-99\n55.1891\n\n\nCTR: sod\n33.7818\n3.2873\n10.28\n<1e-24\n23.2478\n\n\nCTR: dos\n13.9851\n2.3060\n6.06\n<1e-08\n10.7577\n\n\nCTR: dod\n-2.7469\n2.2138\n-1.24\n0.2147\n9.5041\n\n\nResidual\n69.8349"
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.4 HypothesisCoding",
    "text": "B.4 HypothesisCoding\nThis contrast corresponds to MASS::contr.sdif() in R. A general solution (not inverse of last contrast)\n\nm1b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        0 -1 1 0\n        0 0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0924\n54.95\n<1e-99\n55.2080\n\n\nCTR: spt\n33.7817\n3.2877\n10.28\n<1e-24\n23.2511\n\n\nCTR: obj\n13.9852\n2.3060\n6.06\n<1e-08\n10.7571\n\n\nCTR: grv\n2.7470\n2.2143\n1.24\n0.2148\n9.5113\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nControlling the ordering of levels for contrasts:\n\nkwarg levels to order the levels; the first is set as the baseline.\nkwarg base= to fix the baseline level.\n\nThe assignment of random factors such as Subj to Grouping() is only necessary when the sample size is very large and leads to an out-of-memory error; it is included only in the first example for reference."
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.5 DummyCoding",
    "text": "B.5 DummyCoding\nThi contrast corresponds to contr.treatment() in R\n\nm2 = let\n  contrasts = Dict(:CTR => DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n358.0914\n6.1533\n58.20\n<1e-99\n47.9047\n\n\nCTR: dod\n45.0200\n4.3641\n10.32\n<1e-24\n32.2952\n\n\nCTR: dos\n47.7669\n3.5570\n13.43\n<1e-40\n25.5401\n\n\nCTR: sod\n33.7817\n3.2876\n10.28\n<1e-24\n23.2499\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThis contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM."
  },
  {
    "objectID": "contrasts_kwdyz11.html#specialcoding",
    "href": "contrasts_kwdyz11.html#specialcoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.6 SpecialCoding",
    "text": "B.6 SpecialCoding\nThe contrasts returned by DummyCoding may be what you want. Can’t we have them, but also the GM rather than the mean of the base level? Yes, we can!\n\nm2b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0894\n54.97\n<1e-99\n55.1850\n\n\nCTR: sod\n33.7817\n3.2873\n10.28\n<1e-24\n23.2475\n\n\nCTR: dos\n47.7669\n3.5574\n13.43\n<1e-40\n25.5440\n\n\nCTR: dod\n45.0200\n4.3638\n10.32\n<1e-24\n32.2928\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nJust relevel the factor or move the column with -1s for a different base."
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding",
    "href": "contrasts_kwdyz11.html#effectscoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.7 EffectsCoding",
    "text": "B.7 EffectsCoding\nThis contrast coding corresponds to contr.sum() in R.\n\nm3 = let\n  contrasts = Dict(:CTR => EffectsCoding(; base=\"dod\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0910\n54.96\n<1e-99\n55.1970\n\n\nCTR: dos\n16.1248\n1.4404\n11.20\n<1e-28\n7.3310\n\n\nCTR: sod\n2.1396\n1.3337\n1.60\n0.1087\n6.0066\n\n\nCTR: val\n-31.6422\n2.6421\n-11.98\n<1e-32\n19.9492\n\n\nResidual\n69.8349"
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.8 HelmertCoding",
    "text": "B.8 HelmertCoding\n\nm4 = let\n  contrasts = Dict(:CTR => HelmertCoding())\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0927\n54.95\n<1e-99\n55.2103\n\n\nCTR: dos\n1.3735\n1.1077\n1.24\n0.2150\n4.7626\n\n\nCTR: sod\n-4.2039\n0.6847\n-6.14\n<1e-09\n3.3539\n\n\nCTR: val\n-10.5474\n0.8810\n-11.97\n<1e-32\n6.6523\n\n\nResidual\n69.8347"
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.9 Reverse HelmertCoding",
    "text": "B.9 Reverse HelmertCoding\n\nm4b = let\n  levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR => HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0927\n54.95\n<1e-99\n55.2103\n\n\nCTR: dos\n1.3735\n1.1077\n1.24\n0.2150\n4.7626\n\n\nCTR: sod\n-4.2039\n0.6847\n-6.14\n<1e-09\n3.3539\n\n\nCTR: val\n-10.5474\n0.8810\n-11.97\n<1e-32\n6.6523\n\n\nResidual\n69.8347\n\n\n\n\n\n\n\n\n\nHelmert contrasts are othogonal."
  },
  {
    "objectID": "contrasts_kwdyz11.html#anovacoding",
    "href": "contrasts_kwdyz11.html#anovacoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.10 AnovaCoding",
    "text": "B.10 AnovaCoding\nAnova contrasts are orthogonal.\n\nB.10.1 A(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n<1e-99\n55.2004\n\n\nCTR: A\n59.0052\n5.1826\n11.39\n<1e-29\n36.2076\n\n\nCTR: B\n31.0348\n4.6748\n6.64\n<1e-10\n31.7114\n\n\nCTR: AxB\n-36.5287\n3.0927\n-11.81\n<1e-31\n16.0050\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\nB.10.2 A(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1\n\n\nB.10.3 A(2) x B(2) x C(3)\nTO BE DONE"
  },
  {
    "objectID": "contrasts_kwdyz11.html#nestedcoding",
    "href": "contrasts_kwdyz11.html#nestedcoding",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.11 NestedCoding",
    "text": "B.11 NestedCoding\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\nm8 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1 0 0\n        0 0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0899\n54.97\n<1e-99\n55.1887\n\n\nCTR: do_so\n59.0053\n5.1799\n11.39\n<1e-29\n36.1846\n\n\nCTR: spt\n33.7817\n3.2873\n10.28\n<1e-24\n23.2475\n\n\nCTR: grv\n2.7470\n2.2146\n1.24\n0.2148\n9.5150\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "href": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.12 Other orthogonal contrasts",
    "text": "B.12 Other orthogonal contrasts\nFor factors with more than four levels there are many options for specifying orthogonal contrasts as long as one proceeds in a top-down strictly hiearchical fashion.\nSuppose you have a factor with seven levels and let’s ignore shifting colummns. In this case, you have six options for the first contrast, that is 6 vs. 1, 5 vs.2 , 4 vs. 3, 3 vs. 4, 2 vs. 5, and 1 vs. 6 levels. Then, you specify orthogonal contrasts for partitions with more than 2 elements and so on. That is, you don’t specify a contrast that crosses an earlier partition line.\nIn the following example, after an initial 4 vs 3 partitioning of levels, we specify AnovaCoding for the left and HelmertCoding for the right partition.\n\ncontrasts = Dict(\n  :CTR => HypothesisCoding(\n    [\n      -1/4 -1/4 -1/4 -1/4 +1/3 +1/3 +1/3\n      -1/2 -1/2 +1/2 +1/2 0 0 0\n      -1/2 +1/2 -1/2 +1/2 0 0 0\n      +1/2 -1/2 -1/2 +1/2 0 0 0\n      0 0 0 0 -1 +1 0\n      0 0 0 0 -1/2 -1/2 1\n    ];\n    levels=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\"],\n    labels=[\"c567.1234\", \"B\", \"C\", \"BxC\", \"c6.5\", \"c6.56\"],\n  ),\n);\n\nThere are two rules that hold for all orthogonal contrasts:\n\nThe weights within rows sum to zero.\nFor all pairs of rows, the sum of the products of weights in the same columns sums to zero."
  },
  {
    "objectID": "contrasts_kwdyz11.html#summary-dave-kleinschmidt",
    "href": "contrasts_kwdyz11.html#summary-dave-kleinschmidt",
    "title": "Appendix B — Contrast Coding of Visual Attention Effects",
    "section": "B.13 Summary (Dave Kleinschmidt)",
    "text": "B.13 Summary (Dave Kleinschmidt)\nStatsModels\nStatsModels.jl provides a few commonly used contrast coding schemes, some less-commonly used schemes, and structs that allow you to manually specify your own, custom schemes.\n\nB.13.1 Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively).\n\n\nB.13.2 “Exotic” contrasts\nWe also provide HelmertCoding and SeqDiffCoding (corresponding to base R’s contr.helmert() and MASS::contr.sdif()).\n\n\nB.13.3 Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it’s good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they’ll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information).\n\n\nKliegl, R., Kushela, J., & Laubrock, J. (2015). Object orientation and target size modulate the speed of visual attention. Department of Psychology, University of Potsdam.\n\n\nKliegl, R., Wei, P., Dambacher, M., Yan, M., & Zhou, X. (2010). Experimental effects and individual differences in linear mixed models: Estimating the relationship between spatial, object, and attraction effects in visual attention. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2010.00238\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. Journal of Memory and Language, 110, 104038. https://doi.org/10.1016/j.jml.2019.104038"
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "Appendix C — Contrast Coding of Physical Fitness Effects",
    "section": "",
    "text": "Ths script uses a subset of data reported in Fühner, Golle, Granacher, & Kliegl (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. (Fühner et al., 2021)\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollement, that is in their ninth year of life in the third grade.\nThe script is structured in three main sections:"
  },
  {
    "objectID": "contrasts_fggk21.html#setup",
    "href": "contrasts_fggk21.html#setup",
    "title": "Appendix C — Contrast Coding of Physical Fitness Effects",
    "section": "C.1 Setup",
    "text": "C.1 Setup\n\nC.1.1 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing AlgebraOfGraphics: linear\nusing Arrow\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing MixedModels\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nProgressMeter.ijulia_behavior(:clear);\n\n\n┌ Warning: SIMD capacity not detected by ScanByte, using scalar fallback\n└ @ ScanByte /Users/reinholdkliegl/.julia/packages/ScanByte/cr4PT/src/ScanByte.jl:11\n\n\n\n\nC.1.2 Readme for ‘./data/fggk21.rds’\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: “Girls” (n=55,086), “Boys” (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units\n\n\n\nC.1.3 Preprocessing\n\nC.1.3.1 Read data\n\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1Cohort201120190String2SchoolS100043S8002000String3ChildC002352C1179660String4Sexfemalemale0String5age8.560737.994528.558529.106090Float646TestBPTStar_r0String7score226.1411.141524.651161530.00Float64\n\n\n\n\nC.1.3.2 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\nbegin\n  dat = @chain df begin\n    @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n    @groupby(:Test, :Sex)\n    combine(x -> x[sample(1:nrow(x), 500), :])\n  end\nend\n\n5,000 rows × 7 columnsTestSexCohortSchoolChildagescoreStringStringStringStringStringFloat64Float641S20_rBoys2014S103524C1030328.955514.76192S20_rBoys2011S104620C0801078.750174.166673S20_rBoys2016S103299C0747018.70915.128214S20_rBoys2015S101047C0732388.698153.846155S20_rBoys2011S103299C0149318.169754.545456S20_rBoys2016S101527C0166498.197134.545457S20_rBoys2016S101023C0169518.208084.545458S20_rBoys2011S104280C0040898.04.255329S20_rBoys2017S101357C0968128.892544.8780510S20_rBoys2018S113207C0348958.358663.9215711S20_rBoys2019S106630C0576638.555784.6511612S20_rBoys2019S111594C0291818.30394.2553213S20_rBoys2018S106112C0912308.854215.1282114S20_rBoys2017S102921C1063948.971944.2553215S20_rBoys2013S110097C0217658.246414.1666716S20_rBoys2015S105831C0189248.219034.1666717S20_rBoys2011S104644C0234428.251884.6511618S20_rBoys2013S101825C0977378.91173.9215719S20_rBoys2018S101527C0903498.854214.2553220S20_rBoys2019S111790C1062288.96924.5454521S20_rBoys2017S105685C1067038.974674.761922S20_rBoys2018S100304C0717138.692684.023S20_rBoys2018S110383C1105069.021224.5454524S20_rBoys2017S112227C0828398.780294.761925S20_rBoys2018S104905C0820198.769344.761926S20_rBoys2016S104255C0649848.629714.761927S20_rBoys2012S104735C0410988.416154.0816328S20_rBoys2018S100596C0901548.854213.8461529S20_rBoys2011S104954C0084178.084875.2631630S20_rBoys2016S102532C0674298.646135.12821⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\n\nC.1.3.3 Transformations\n\nbegin\n  transform!(dat, :age, :age => (x -> x .- 8.5) => :a1) # centered age (linear)\n  select!(groupby(dat, :Test), :, :score => zscore => :zScore) # z-score\nend\n\n5,000 rows × 9 columnsTestSexCohortSchoolChildagescorea1zScoreStringStringStringStringStringFloat64Float64Float64Float641S20_rBoys2014S103524C1030328.955514.76190.455510.5678592S20_rBoys2011S104620C0801078.750174.166670.250171-0.8743633S20_rBoys2016S103299C0747018.70915.128210.2091031.455384S20_rBoys2015S101047C0732388.698153.846150.198152-1.650945S20_rBoys2011S103299C0149318.169754.54545-0.3302530.04341436S20_rBoys2016S101527C0166498.197134.54545-0.3028750.04341437S20_rBoys2016S101023C0169518.208084.54545-0.2919230.04341438S20_rBoys2011S104280C0040898.04.25532-0.5-0.6595649S20_rBoys2017S101357C0968128.892544.878050.3925390.84926810S20_rBoys2018S113207C0348958.358663.92157-0.141342-1.4682211S20_rBoys2019S106630C0576638.555784.651160.05578370.29953812S20_rBoys2019S111594C0291818.30394.25532-0.196099-0.65956413S20_rBoys2018S106112C0912308.854215.128210.3542091.4553814S20_rBoys2017S102921C1063948.971944.255320.471937-0.65956415S20_rBoys2013S110097C0217658.246414.16667-0.253593-0.87436316S20_rBoys2015S105831C0189248.219034.16667-0.280972-0.87436317S20_rBoys2011S104644C0234428.251884.65116-0.2481180.29953818S20_rBoys2013S101825C0977378.91173.921570.411704-1.4682219S20_rBoys2018S101527C0903498.854214.255320.354209-0.65956420S20_rBoys2019S111790C1062288.96924.545450.4691990.043414321S20_rBoys2017S105685C1067038.974674.76190.4746750.56785922S20_rBoys2018S100304C0717138.692684.00.192676-1.2781923S20_rBoys2018S110383C1105069.021224.545450.5212180.043414324S20_rBoys2017S112227C0828398.780294.76190.2802870.56785925S20_rBoys2018S104905C0820198.769344.76190.2693360.56785926S20_rBoys2016S104255C0649848.629714.76190.1297060.56785927S20_rBoys2012S104735C0410988.416154.08163-0.0838467-1.0803928S20_rBoys2018S100596C0901548.854213.846150.354209-1.6509429S20_rBoys2011S104954C0084178.084875.26316-0.4151271.7823630S20_rBoys2016S102532C0674298.646135.128210.1461331.45538⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\nbegin\n  dat2 = combine(\n    groupby(dat, [:Test, :Sex]),\n    :score => mean,\n    :score => std,\n    :zScore => mean,\n    :zScore => std,\n  )\nend\n\n10 rows × 6 columnsTestSexscore_meanscore_stdzScore_meanzScore_stdStringStringFloat64Float64Float64Float641S20_rBoys4.583270.4114520.1350440.996922BPTBoys4.03120.6937070.3591520.9589923SLJBoys129.41219.21730.2049531.009134Star_rBoys2.059620.2981310.08074981.061355RunBoys1048.79156.2070.2476811.061446S20_rGirls4.47180.406822-0.1350440.9857027BPTGirls3.51160.656573-0.3591520.9076578SLJGirls121.60618.0609-0.2049530.9484079Star_rGirls2.014260.260868-0.08074980.92869310RunGirls975.89127.654-0.2476810.867421\n\n\n\n\nC.1.3.4 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566."
  },
  {
    "objectID": "contrasts_fggk21.html#contrast-coding",
    "href": "contrasts_fggk21.html#contrast-coding",
    "title": "Appendix C — Contrast Coding of Physical Fitness Effects",
    "section": "C.2 Contrast coding",
    "text": "C.2 Contrast coding\nContrast coding is part of StatsModels.jl. Here is the primary author’s (i.e., Dave Kleinschmidt’s documentation of Modeling Categorical Data.\nThe random factors Child, School, and Cohort are assigned a Grouping contrast. This contrast is needed when the number of groups (i.e., units, levels) is very large. This is the case for Child (i.e., the 108,925 children in the full and probably also the 11,566 children in the reduced data set). The assignment is not necessary for the typical sample size of experiments. However, we use this coding of random factors irrespective of the number of units associated with them to be transparent about the distinction between random and fixed factors.\nA couple of general remarks about the following examples. First, all contrasts defined in this tutorial return an estimate of the Grand Mean (GM) in the intercept, that is they are so-called sum-to-zero contrasts. In both Julia and R the default contrast is Dummy coding which is not a sum-to-zero contrast, but returns the mean of the reference (control) group - unfortunately for (quasi-)experimentally minded scientists.\nSecond, The factor Sex has only two levels. We use EffectCoding (also known as Sum coding in R) to estimate the difference of the levels from the Grand Mean. Unlike in R, the default sign of the effect is for the second level (base is the first, not the last level), but this can be changed with the base kwarg in the command. Effect coding is a sum-to-zero contrast, but when applied to factors with more than two levels does not yield orthogonal contrasts.\nFinally, contrasts for the five levels of the fixed factor Test represent the hypotheses about differences between them. In this tutorial, we use this factor to illustrate various options.\nWe (initially) include only Test as fixed factor and Child as random factor. More complex LMMs can be specified by simply adding other fixed or random factors to the formula.\n\nC.2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small “2-1” effect “correlates” negatively with a larger “3-2” effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  ),\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :Child  => Grouping()\n  :School => Grouping()\n  :Test   => SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Cohort => Grouping()\n  :Sex    => EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nf_ovi_1 = @formula zScore ~ 1 + Test + (1 | Child);\n\n\nm_ovi_SeqDiff_1 = fit(MixedModel, f_ovi_1, dat; contrasts=contr1)\n\n\u001b[32mMinimizing 14   Time: 0:00:00 (13.48 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0004\n0.0142\n-0.03\n0.9767\n0.7220\n\n\nTest: Star_r\n0.0008\n0.0444\n0.02\n0.9861\n\n\n\nTest: S20_r\n0.0050\n0.0443\n0.11\n0.9106\n\n\n\nTest: SLJ\n-0.0036\n0.0443\n-0.08\n0.9361\n\n\n\nTest: BPT\n0.0004\n0.0443\n0.01\n0.9930\n\n\n\nResidual\n0.6902\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = let\n  form = @formula zScore ~ 1 + Test * a1 + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0133\n0.0145\n-0.92\n0.3587\n0.7216\n\n\nTest: Star_r\n-0.0067\n0.0449\n-0.15\n0.8810\n\n\n\nTest: S20_r\n0.0116\n0.0451\n0.26\n0.7964\n\n\n\nTest: SLJ\n-0.0149\n0.0452\n-0.33\n0.7408\n\n\n\nTest: BPT\n-0.0064\n0.0451\n-0.14\n0.8875\n\n\n\na1\n0.2287\n0.0496\n4.61\n<1e-05\n\n\n\nTest: Star_r & a1\n0.0959\n0.1528\n0.63\n0.5304\n\n\n\nTest: S20_r & a1\n-0.0881\n0.1553\n-0.57\n0.5703\n\n\n\nTest: SLJ & a1\n0.1357\n0.1554\n0.87\n0.3826\n\n\n\nTest: BPT & a1\n0.1839\n0.1538\n1.20\n0.2316\n\n\n\nResidual\n0.6868\n\n\n\n\n\n\n\n\n\nThe difference between older and younger childrend is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = let\n  form = @formula zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.87\n0.3823\n0.7002\n\n\nTest: Star_r\n-0.0085\n0.0438\n-0.19\n0.8458\n\n\n\nTest: S20_r\n0.0100\n0.0439\n0.23\n0.8191\n\n\n\nTest: SLJ\n-0.0114\n0.0440\n-0.26\n0.7957\n\n\n\nTest: BPT\n-0.0111\n0.0439\n-0.25\n0.8010\n\n\n\na1\n0.2188\n0.0483\n4.53\n<1e-05\n\n\n\nSex: Boys\n0.2052\n0.0138\n14.84\n<1e-49\n\n\n\nTest: Star_r & a1\n0.1463\n0.1489\n0.98\n0.3259\n\n\n\nTest: S20_r & a1\n-0.1053\n0.1513\n-0.70\n0.4864\n\n\n\nTest: SLJ & a1\n0.1117\n0.1514\n0.74\n0.4609\n\n\n\nTest: BPT & a1\n0.2501\n0.1498\n1.67\n0.0952\n\n\n\nTest: Star_r & Sex: Boys\n-0.1739\n0.0431\n-4.03\n<1e-04\n\n\n\nTest: S20_r & Sex: Boys\n0.0585\n0.0430\n1.36\n0.1743\n\n\n\nTest: SLJ & Sex: Boys\n0.0753\n0.0431\n1.75\n0.0805\n\n\n\nTest: BPT & Sex: Boys\n0.1526\n0.0430\n3.55\n0.0004\n\n\n\nResidual\n0.6713\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role – as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls’ ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\nf_ovi = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Child)\nm_ovi_SeqDiff = fit(MixedModel, f_ovi, dat; contrasts=contr1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.87\n0.3838\n0.7016\n\n\nTest: Star_r\n-0.0094\n0.0438\n-0.22\n0.8294\n\n\n\nTest: S20_r\n0.0094\n0.0439\n0.21\n0.8305\n\n\n\nTest: SLJ\n-0.0116\n0.0440\n-0.26\n0.7919\n\n\n\nTest: BPT\n-0.0094\n0.0439\n-0.21\n0.8313\n\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.32\n<1e-45\n\n\n\nTest: Star_r & a1\n0.1496\n0.1489\n1.00\n0.3152\n\n\n\nTest: S20_r & a1\n-0.1095\n0.1513\n-0.72\n0.4690\n\n\n\nTest: SLJ & a1\n0.1100\n0.1515\n0.73\n0.4679\n\n\n\nTest: BPT & a1\n0.2523\n0.1499\n1.68\n0.0923\n\n\n\nTest: Star_r & Sex: Boys\n-0.1793\n0.0438\n-4.09\n<1e-04\n\n\n\nTest: S20_r & Sex: Boys\n0.0567\n0.0439\n1.29\n0.1961\n\n\n\nTest: SLJ & Sex: Boys\n0.0746\n0.0440\n1.69\n0.0901\n\n\n\nTest: BPT & Sex: Boys\n0.1522\n0.0439\n3.46\n0.0005\n\n\n\na1 & Sex: Boys\n0.0590\n0.0483\n1.22\n0.2224\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.1105\n0.1489\n0.74\n0.4580\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0417\n0.1513\n0.28\n0.7831\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.0068\n0.1515\n-0.05\n0.9641\n\n\n\nTest: BPT & a1 & Sex: Boys\n0.0254\n0.1499\n0.17\n0.8655\n\n\n\nResidual\n0.6694\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics.\n\n\nC.2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiosrespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performats also depends on endurance. HeC1 yields a measure of Coordination “corrected” for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate “orthogonal” than “non-orthogonal” random-effect structures.\n\ncontr2 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = fit(MixedModel, f_ovi, dat; contrasts=contr2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.87\n0.3838\n0.7016\n\n\nTest: Star_r\n-0.0047\n0.0219\n-0.22\n0.8294\n\n\n\nTest: S20_r\n0.0016\n0.0126\n0.12\n0.9018\n\n\n\nTest: SLJ\n-0.0021\n0.0090\n-0.24\n0.8134\n\n\n\nTest: BPT\n-0.0031\n0.0069\n-0.45\n0.6494\n\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.32\n<1e-45\n\n\n\nTest: Star_r & a1\n0.0748\n0.0745\n1.00\n0.3152\n\n\n\nTest: S20_r & a1\n-0.0116\n0.0438\n-0.26\n0.7915\n\n\n\nTest: SLJ & a1\n0.0217\n0.0306\n0.71\n0.4781\n\n\n\nTest: BPT & a1\n0.0635\n0.0237\n2.68\n0.0074\n\n\n\nTest: Star_r & Sex: Boys\n-0.0896\n0.0219\n-4.09\n<1e-04\n\n\n\nTest: S20_r & Sex: Boys\n-0.0110\n0.0126\n-0.87\n0.3858\n\n\n\nTest: SLJ & Sex: Boys\n0.0132\n0.0090\n1.46\n0.1430\n\n\n\nTest: BPT & Sex: Boys\n0.0383\n0.0069\n5.54\n<1e-07\n\n\n\na1 & Sex: Boys\n0.0590\n0.0483\n1.22\n0.2224\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.0553\n0.0745\n0.74\n0.4580\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0323\n0.0438\n0.74\n0.4609\n\n\n\nTest: SLJ & a1 & Sex: Boys\n0.0144\n0.0306\n0.47\n0.6366\n\n\n\nTest: BPT & a1 & Sex: Boys\n0.0137\n0.0237\n0.58\n0.5620\n\n\n\nResidual\n0.6694\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent.\n\n\nC.2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1 0 0 0\n       0 -1 +1 0 0\n       0 0 -1 +1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = fit(MixedModel, f_ovi, dat; contrasts=contr3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.87\n0.3838\n0.7016\n\n\nTest: BPT-other\n-0.0629\n0.1384\n-0.45\n0.6494\n\n\n\nTest: Star-End\n-0.0094\n0.0438\n-0.22\n0.8294\n\n\n\nTest: S20-Star\n0.0094\n0.0439\n0.21\n0.8305\n\n\n\nTest: SLJ-S20\n-0.0116\n0.0440\n-0.26\n0.7919\n\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.32\n<1e-45\n\n\n\nTest: BPT-other & a1\n1.2696\n0.4742\n2.68\n0.0074\n\n\n\nTest: Star-End & a1\n0.1496\n0.1489\n1.00\n0.3152\n\n\n\nTest: S20-Star & a1\n-0.1095\n0.1513\n-0.72\n0.4690\n\n\n\nTest: SLJ-S20 & a1\n0.1100\n0.1515\n0.73\n0.4679\n\n\n\nTest: BPT-other & Sex: Boys\n0.7668\n0.1384\n5.54\n<1e-07\n\n\n\nTest: Star-End & Sex: Boys\n-0.1793\n0.0438\n-4.09\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0567\n0.0439\n1.29\n0.1961\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0746\n0.0440\n1.69\n0.0901\n\n\n\na1 & Sex: Boys\n0.0590\n0.0483\n1.22\n0.2224\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n0.2750\n0.4742\n0.58\n0.5620\n\n\n\nTest: Star-End & a1 & Sex: Boys\n0.1105\n0.1489\n0.74\n0.4580\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0417\n0.1513\n0.28\n0.7831\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0068\n0.1515\n-0.05\n0.9641\n\n\n\nResidual\n0.6694\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 +1 0 0 0\n      0 -1 +1 0 0\n      0 0 -1 +1 0\n      0 0 0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = fit(MixedModel, f_ovi, dat; contrasts=contr1b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.87\n0.3838\n0.7016\n\n\nTest: Star-Run\n-0.0094\n0.0438\n-0.22\n0.8294\n\n\n\nTest: S20-Star\n0.0094\n0.0439\n0.21\n0.8305\n\n\n\nTest: SLJ-S20\n-0.0116\n0.0440\n-0.26\n0.7919\n\n\n\nTest: BPT-SLJ\n-0.0094\n0.0439\n-0.21\n0.8313\n\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.32\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.1496\n0.1489\n1.00\n0.3152\n\n\n\nTest: S20-Star & a1\n-0.1095\n0.1513\n-0.72\n0.4690\n\n\n\nTest: SLJ-S20 & a1\n0.1100\n0.1515\n0.73\n0.4679\n\n\n\nTest: BPT-SLJ & a1\n0.2523\n0.1499\n1.68\n0.0923\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1793\n0.0438\n-4.09\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0567\n0.0439\n1.29\n0.1961\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0746\n0.0440\n1.69\n0.0901\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1522\n0.0439\n3.46\n0.0005\n\n\n\na1 & Sex: Boys\n0.0590\n0.0483\n1.22\n0.2224\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.1105\n0.1489\n0.74\n0.4580\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0417\n0.1513\n0.28\n0.7831\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0068\n0.1515\n-0.05\n0.9641\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0254\n0.1499\n0.17\n0.8655\n\n\n\nResidual\n0.6694\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 99   Time: 0:00:00 ( 3.96 ms/it)\u001b[39m\n\u001b[34m  objective:  13847.409481173658\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0123\n0.0141\n-0.88\n0.3814\n0.6991\n\n\nTest: Star-Run\n-0.0089\n0.0441\n-0.20\n0.8401\n0.0000\n\n\nTest: S20-Star\n0.0095\n0.0440\n0.22\n0.8292\n0.3527\n\n\nTest: SLJ-S20\n-0.0120\n0.0438\n-0.27\n0.7843\n0.0000\n\n\nTest: BPT-SLJ\n-0.0102\n0.0437\n-0.23\n0.8153\n0.0000\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.32\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.1469\n0.1501\n0.98\n0.3278\n\n\n\nTest: S20-Star & a1\n-0.1066\n0.1516\n-0.70\n0.4818\n\n\n\nTest: SLJ-S20 & a1\n0.1058\n0.1506\n0.70\n0.4827\n\n\n\nTest: BPT-SLJ & a1\n0.2528\n0.1491\n1.70\n0.0900\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1782\n0.0441\n-4.04\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0558\n0.0440\n1.27\n0.2049\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0755\n0.0438\n1.73\n0.0844\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1510\n0.0437\n3.46\n0.0005\n\n\n\na1 & Sex: Boys\n0.0589\n0.0483\n1.22\n0.2230\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.1090\n0.1501\n0.73\n0.4677\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0383\n0.1516\n0.25\n0.8007\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0037\n0.1506\n-0.02\n0.9807\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0247\n0.1491\n0.17\n0.8682\n\n\n\nResidual\n0.6494\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 2611     Time: 0:00:10 ( 3.96 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0140\n-0.90\n0.3667\n\n\n\nTest: Star-Run\n-0.0108\n0.0443\n-0.24\n0.8073\n\n\n\nTest: S20-Star\n0.0135\n0.0448\n0.30\n0.7625\n\n\n\nTest: SLJ-S20\n-0.0204\n0.0443\n-0.46\n0.6456\n\n\n\nTest: BPT-SLJ\n-0.0106\n0.0429\n-0.25\n0.8042\n\n\n\na1\n0.2217\n0.0481\n4.61\n<1e-05\n\n\n\nSex: Boys\n0.2011\n0.0140\n14.33\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.1815\n0.1511\n1.20\n0.2296\n\n\n\nTest: S20-Star & a1\n-0.1363\n0.1543\n-0.88\n0.3773\n\n\n\nTest: SLJ-S20 & a1\n0.1035\n0.1522\n0.68\n0.4966\n\n\n\nTest: BPT-SLJ & a1\n0.2418\n0.1464\n1.65\n0.0985\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1803\n0.0443\n-4.07\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0538\n0.0448\n1.20\n0.2298\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0803\n0.0443\n1.81\n0.0697\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1469\n0.0429\n3.42\n0.0006\n\n\n\na1 & Sex: Boys\n0.0588\n0.0481\n1.22\n0.2216\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.1533\n0.1511\n1.01\n0.3103\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0083\n0.1543\n0.05\n0.9570\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.0089\n0.1522\n0.06\n0.9531\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0088\n0.1464\n0.06\n0.9519\n\n\n\nTest: BPT\n\n\n\n\n0.9192\n\n\nTest: SLJ\n\n\n\n\n0.9722\n\n\nTest: Star_r\n\n\n\n\n0.9891\n\n\nTest: Run\n\n\n\n\n0.9656\n\n\nTest: S20_r\n\n\n\n\n0.9910\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = let\n  f_cpx_0 = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, f_cpx_0, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 2611     Time: 0:00:10 ( 4.00 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0140\n-0.90\n0.3667\n\n\n\nTest: Star-Run\n-0.0108\n0.0443\n-0.24\n0.8073\n\n\n\nTest: S20-Star\n0.0135\n0.0448\n0.30\n0.7625\n\n\n\nTest: SLJ-S20\n-0.0204\n0.0443\n-0.46\n0.6456\n\n\n\nTest: BPT-SLJ\n-0.0106\n0.0429\n-0.25\n0.8042\n\n\n\na1\n0.2217\n0.0481\n4.61\n<1e-05\n\n\n\nSex: Boys\n0.2011\n0.0140\n14.33\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.1815\n0.1511\n1.20\n0.2296\n\n\n\nTest: S20-Star & a1\n-0.1363\n0.1543\n-0.88\n0.3773\n\n\n\nTest: SLJ-S20 & a1\n0.1035\n0.1522\n0.68\n0.4966\n\n\n\nTest: BPT-SLJ & a1\n0.2418\n0.1464\n1.65\n0.0985\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1803\n0.0443\n-4.07\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0538\n0.0448\n1.20\n0.2298\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0803\n0.0443\n1.81\n0.0697\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1469\n0.0429\n3.42\n0.0006\n\n\n\na1 & Sex: Boys\n0.0588\n0.0481\n1.22\n0.2216\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.1533\n0.1511\n1.01\n0.3103\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.0083\n0.1543\n0.05\n0.9570\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.0089\n0.1522\n0.06\n0.9531\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0088\n0.1464\n0.06\n0.9519\n\n\n\nTest: BPT\n\n\n\n\n0.9192\n\n\nTest: SLJ\n\n\n\n\n0.9722\n\n\nTest: Star_r\n\n\n\n\n0.9891\n\n\nTest: Run\n\n\n\n\n0.9656\n\n\nTest: S20_r\n\n\n\n\n0.9910\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.93230755\n0.96556075\n\n\n\n\n\n\n\nTest: Star_r\n0.97837337\n0.98912758\n-0.41\n\n\n\n\n\n\nTest: S20_r\n0.98207639\n0.99099767\n+0.65\n+0.43\n\n\n\n\n\nTest: SLJ\n0.94523906\n0.97223406\n+0.19\n+0.70\n+0.74\n\n\n\n\nTest: BPT\n0.84499309\n0.91923506\n+0.70\n-0.10\n+0.59\n+0.29\n\n\nResidual\n\n0.00000000\n0.00001110\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .\n Test: Star_r  -0.41   1.0     .      .      .\n Test: S20_r    0.65   0.43   1.0     .      .\n Test: SLJ      0.19   0.7    0.74   1.0     .\n Test: BPT      0.7   -0.1    0.59   0.29   1.0\n\nNormalized cumulative variances:\n[0.5347, 0.8954, 0.9662, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.41   0.51  -0.43   0.03  -0.62\n Test: Star_r  -0.23  -0.68   0.22   0.39  -0.54\n Test: S20_r   -0.59  -0.05  -0.27   0.51   0.57\n Test: SLJ     -0.48  -0.39  -0.17  -0.76   0.07\n Test: BPT     -0.45   0.35   0.82  -0.08   0.02,)\n\n\n\nf_cpx_1 = @formula(\n  zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child)\n)\nm_cpx_1_SeqDiff =\nfit(MixedModel, f_cpx_1, dat; contrasts=contr1b)\n\n\u001b[32mMinimizing 3325     Time: 0:00:13 ( 3.99 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0140\n-0.90\n0.3671\n0.6835\n\n\nTest: Star-Run\n-0.0063\n0.0438\n-0.14\n0.8865\n1.3524\n\n\nTest: S20-Star\n0.0145\n0.0443\n0.33\n0.7433\n1.2353\n\n\nTest: SLJ-S20\n-0.0231\n0.0447\n-0.52\n0.6046\n0.8322\n\n\nTest: BPT-SLJ\n-0.0112\n0.0431\n-0.26\n0.7942\n1.2233\n\n\na1\n0.2195\n0.0482\n4.56\n<1e-05\n\n\n\nSex: Boys\n0.2013\n0.0140\n14.33\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.1619\n0.1496\n1.08\n0.2789\n\n\n\nTest: S20-Star & a1\n-0.1153\n0.1531\n-0.75\n0.4516\n\n\n\nTest: SLJ-S20 & a1\n0.1135\n0.1539\n0.74\n0.4607\n\n\n\nTest: BPT-SLJ & a1\n0.2274\n0.1469\n1.55\n0.1215\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1806\n0.0438\n-4.12\n<1e-04\n\n\n\nTest: S20-Star & Sex: Boys\n0.0572\n0.0443\n1.29\n0.1970\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0789\n0.0447\n1.76\n0.0776\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1486\n0.0431\n3.45\n0.0006\n\n\n\na1 & Sex: Boys\n0.0594\n0.0482\n1.23\n0.2173\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.1598\n0.1496\n1.07\n0.2855\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0005\n0.1531\n-0.00\n0.9973\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.0046\n0.1539\n-0.03\n0.9760\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n0.0160\n0.1469\n0.11\n0.9132\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .      .      .\n Test: Star-Run  -0.18   1.0     .      .      .\n Test: S20-Star   0.37  -0.87   1.0     .      .\n Test: SLJ-S20   -0.11   0.93  -0.92   1.0     .\n Test: BPT-SLJ   -0.29  -0.76   0.71  -0.74   1.0\n\nNormalized cumulative variances:\n[0.6979, 0.9428, 0.978, 1.0, 1.0]\n\nComponent loadings\n                   PC1    PC2   PC3    PC4    PC5\n (Intercept)     -0.08  -0.88  0.3    0.17   0.31\n Test: Star-Run   0.52   0.03  0.29  -0.73   0.35\n Test: S20-Star  -0.51  -0.2   0.19  -0.57  -0.58\n Test: SLJ-S20    0.52  -0.0   0.55   0.31  -0.58\n Test: BPT-SLJ   -0.45   0.42  0.7    0.14   0.34,)\n\n\n\n\nC.2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principle components of the published LMM based on test scores, not test effects (contrasts) - coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 0 0 0 +1\n      -3 +2 +2 +2 -3\n      0 +2 -1 -1 0\n      0 0 +1 -1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = fit(MixedModel, f_cpx_1, dat; contrasts=contr4)\n\n\u001b[32mMinimizing 1678     Time: 0:00:06 ( 4.02 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0122\n0.0141\n-0.87\n0.3866\n0.7275\n\n\nTest: c5.1\n-0.0199\n0.0429\n-0.46\n0.6431\n1.4459\n\n\nTest: c234.15\n0.0287\n0.1675\n0.17\n0.8638\n1.5905\n\n\nTest: c2.34\n0.0028\n0.0764\n0.04\n0.9709\n1.3101\n\n\nTest: c3.4\n0.0140\n0.0443\n0.32\n0.7514\n1.1241\n\n\na1\n0.2171\n0.0483\n4.50\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.34\n<1e-45\n\n\n\nTest: c5.1 & a1\n0.4019\n0.1465\n2.74\n0.0061\n\n\n\nTest: c234.15 & a1\n-0.4495\n0.5723\n-0.79\n0.4322\n\n\n\nTest: c2.34 & a1\n0.0672\n0.2613\n0.26\n0.7971\n\n\n\nTest: c3.4 & a1\n-0.1282\n0.1527\n-0.84\n0.4012\n\n\n\nTest: c5.1 & Sex: Boys\n0.1067\n0.0429\n2.49\n0.0128\n\n\n\nTest: c234.15 & Sex: Boys\n-1.0335\n0.1675\n-6.17\n<1e-09\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1930\n0.0764\n-2.53\n0.0115\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0720\n0.0443\n-1.62\n0.1044\n\n\n\na1 & Sex: Boys\n0.0577\n0.0483\n1.19\n0.2323\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1566\n0.1465\n1.07\n0.2853\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3988\n0.5723\n0.70\n0.4859\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0321\n0.2613\n-0.12\n0.9023\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0162\n0.1527\n0.11\n0.9153\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.52928988\n0.72752311\n\n\n\n\n\n\n\nTest: c5.1\n2.09053903\n1.44586965\n-0.17\n\n\n\n\n\n\nTest: c234.15\n2.52953820\n1.59045220\n+0.75\n-0.56\n\n\n\n\n\nTest: c2.34\n1.71637531\n1.31010508\n+0.12\n+0.16\n+0.49\n\n\n\n\nTest: c3.4\n1.26356184\n1.12408267\n-0.08\n+0.24\n-0.47\n-0.55\n\n\nResidual\n\n0.00000000\n0.00006409\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1     -0.17   1.0     .      .      .\n Test: c234.15   0.75  -0.56   1.0     .      .\n Test: c2.34     0.12   0.16   0.49   1.0     .\n Test: c3.4     -0.08   0.24  -0.47  -0.55   1.0\n\nNormalized cumulative variances:\n[0.4857, 0.7441, 0.9306, 1.0, 1.0]\n\nComponent loadings\n                  PC1    PC2   PC3    PC4    PC5\n (Intercept)    -0.42  -0.35  0.63   0.33   0.44\n Test: c5.1      0.32   0.53  0.62   0.29  -0.38\n Test: c234.15  -0.62  -0.16  0.11  -0.2   -0.73\n Test: c2.34    -0.38   0.64  0.14  -0.55   0.36\n Test: c3.4      0.43  -0.41  0.43  -0.68  -0.02,)\n\n\nThere is a numerical interaction with a z-value > 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger “unfitness” gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related “fitness” gains of girls than boys (i.e., in Run_r).\n\ncontr4b = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => HypothesisCoding(\n      [\n        0.49 -0.04 0.20 0.03 -0.85\n        0.70 -0.56 -0.21 -0.13 0.37\n        0.31 0.68 -0.56 -0.35 0.00\n        0.04 0.08 0.61 -0.78 0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    ),\n  ),\n);\n\n\nm_cpx_1_PC_2 = fit(MixedModel, f_cpx_1, dat; contrasts=contr4b)\n\n\u001b[32mMinimizing 2739     Time: 0:00:11 ( 4.03 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0121\n0.0142\n-0.85\n0.3935\n0.7153\n\n\nTest: c5.1\n0.0207\n0.0298\n0.70\n0.4864\n0.5540\n\n\nTest: c234.15\n0.0002\n0.0312\n0.01\n0.9944\n1.1116\n\n\nTest: c12.34\n-0.0004\n0.0313\n-0.01\n0.9889\n0.3939\n\n\nTest: c3.4\n0.0060\n0.0314\n0.19\n0.8497\n0.7096\n\n\na1\n0.2089\n0.0488\n4.29\n<1e-04\n\n\n\nSex: Boys\n0.1921\n0.0142\n13.52\n<1e-40\n\n\n\nTest: c5.1 & a1\n-0.3131\n0.1024\n-3.06\n0.0022\n\n\n\nTest: c234.15 & a1\n-0.0081\n0.1067\n-0.08\n0.9391\n\n\n\nTest: c12.34 & a1\n0.0329\n0.1078\n0.30\n0.7604\n\n\n\nTest: c3.4 & a1\n-0.0430\n0.1078\n-0.40\n0.6899\n\n\n\nTest: c5.1 & Sex: Boys\n-0.1093\n0.0298\n-3.67\n0.0002\n\n\n\nTest: c234.15 & Sex: Boys\n0.1818\n0.0312\n5.82\n<1e-08\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0296\n0.0313\n-0.95\n0.3443\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0323\n0.0314\n-1.03\n0.3043\n\n\n\na1 & Sex: Boys\n0.0613\n0.0488\n1.26\n0.2090\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.0996\n0.1024\n-0.97\n0.3308\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.1015\n0.1067\n-0.95\n0.3414\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0677\n0.1078\n-0.63\n0.5296\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0068\n0.1078\n0.06\n0.9495\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.51159180\n0.71525646\n\n\n\n\n\n\n\nTest: c5.1\n0.30692875\n0.55401151\n+0.41\n\n\n\n\n\n\nTest: c234.15\n1.23568757\n1.11161485\n+0.02\n-0.56\n\n\n\n\n\nTest: c12.34\n0.15512183\n0.39385508\n-0.37\n-0.26\n-0.66\n\n\n\n\nTest: c3.4\n0.50354022\n0.70960568\n-0.07\n+0.17\n+0.17\n-0.36\n\n\nResidual\n\n0.00000000\n0.00003226\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1      0.41   1.0     .      .      .\n Test: c234.15   0.02  -0.56   1.0     .      .\n Test: c12.34   -0.37  -0.26  -0.66   1.0     .\n Test: c3.4     -0.07   0.17   0.17  -0.36   1.0\n\nNormalized cumulative variances:\n[0.3794, 0.7163, 0.9194, 1.0, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.3    0.46  -0.55  -0.62   0.01\n Test: c5.1     -0.07   0.73   0.17   0.41  -0.52\n Test: c234.15  -0.54  -0.49  -0.19   0.05  -0.65\n Test: c12.34    0.69  -0.08   0.06  -0.46  -0.55\n Test: c3.4     -0.37   0.06   0.79  -0.48   0.0,)"
  },
  {
    "objectID": "contrasts_fggk21.html#other-topics",
    "href": "contrasts_fggk21.html#other-topics",
    "title": "Appendix C — Contrast Coding of Physical Fitness Effects",
    "section": "C.3 Other topics",
    "text": "C.3 Other topics\n\nC.3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13848.129911349735\n 13848.129911349735\n 13848.12991134973\n\n\n\n\nC.3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\nbegin\n  f_Child = @formula zScore ~\n    1 + Test * a1 * Sex + (1 + Test | Child)\n  m_Child_SDC = fit(MixedModel, f_Child, dat; contrasts=contr1)\n  m_Child_HeC = fit(MixedModel, f_Child, dat; contrasts=contr2)\n  m_Child_HyC = fit(MixedModel, f_Child, dat; contrasts=contr3)\n  m_Child_PCA = fit(MixedModel, f_Child, dat; contrasts=contr4)\nend\n\n\u001b[32mMinimizing 1678     Time: 0:00:06 ( 4.07 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0122\n0.0141\n-0.87\n0.3866\n0.7275\n\n\nTest: c5.1\n-0.0199\n0.0429\n-0.46\n0.6431\n1.4459\n\n\nTest: c234.15\n0.0287\n0.1675\n0.17\n0.8638\n1.5905\n\n\nTest: c2.34\n0.0028\n0.0764\n0.04\n0.9709\n1.3101\n\n\nTest: c3.4\n0.0140\n0.0443\n0.32\n0.7514\n1.1241\n\n\na1\n0.2171\n0.0483\n4.50\n<1e-05\n\n\n\nSex: Boys\n0.2016\n0.0141\n14.34\n<1e-45\n\n\n\nTest: c5.1 & a1\n0.4019\n0.1465\n2.74\n0.0061\n\n\n\nTest: c234.15 & a1\n-0.4495\n0.5723\n-0.79\n0.4322\n\n\n\nTest: c2.34 & a1\n0.0672\n0.2613\n0.26\n0.7971\n\n\n\nTest: c3.4 & a1\n-0.1282\n0.1527\n-0.84\n0.4012\n\n\n\nTest: c5.1 & Sex: Boys\n0.1067\n0.0429\n2.49\n0.0128\n\n\n\nTest: c234.15 & Sex: Boys\n-1.0335\n0.1675\n-6.17\n<1e-09\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1930\n0.0764\n-2.53\n0.0115\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0720\n0.0443\n-1.62\n0.1044\n\n\n\na1 & Sex: Boys\n0.0577\n0.0483\n1.19\n0.2323\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1566\n0.1465\n1.07\n0.2853\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3988\n0.5723\n0.70\n0.4859\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0321\n0.2613\n-0.12\n0.9023\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0162\n0.1527\n0.11\n0.9153\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.57986264\n0.76148712\n\n\n\n\n\n\n\nTest: Star_r\n1.24277268\n1.11479715\n-0.06\n\n\n\n\n\n\nTest: S20_r\n0.84691786\n0.92028140\n+0.06\n-0.51\n\n\n\n\n\nTest: SLJ\n0.56274674\n0.75016448\n+0.14\n+0.12\n-0.88\n\n\n\n\nTest: BPT\n1.23070584\n1.10937182\n-0.28\n-0.69\n+0.45\n-0.39\n\n\nResidual\n\n0.00000000\n0.00004094\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.51080618\n0.71470706\n\n\n\n\n\n\n\nTest: Star_r\n0.33318379\n0.57722074\n+0.02\n\n\n\n\n\n\nTest: S20_r\n0.05026010\n0.22418764\n+0.31\n-0.20\n\n\n\n\n\nTest: SLJ\n0.02695086\n0.16416716\n-0.01\n+0.94\n+0.11\n\n\n\n\nTest: BPT\n0.04185836\n0.20459317\n-0.28\n-0.53\n+0.22\n-0.37\n\n\nResidual\n\n0.00000000\n0.00005521\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.53964823\n0.73460753\n\n\n\n\n\n\n\nTest: BPT-other\n8.87806697\n2.97960853\n-0.06\n\n\n\n\n\n\nTest: Star-End\n1.98986652\n1.41062629\n+0.01\n-0.59\n\n\n\n\n\nTest: S20-Star\n0.99496804\n0.99748085\n+0.18\n+0.19\n-0.80\n\n\n\n\nTest: SLJ-S20\n0.70020036\n0.83677976\n-0.11\n-0.41\n+0.63\n-0.86\n\n\nResidual\n\n0.00000001\n0.00008309\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.52928988\n0.72752311\n\n\n\n\n\n\n\nTest: c5.1\n2.09053903\n1.44586965\n-0.17\n\n\n\n\n\n\nTest: c234.15\n2.52953820\n1.59045220\n+0.75\n-0.56\n\n\n\n\n\nTest: c2.34\n1.71637531\n1.31010508\n+0.12\n+0.16\n+0.49\n\n\n\n\nTest: c3.4\n1.26356184\n1.12408267\n-0.08\n+0.24\n-0.47\n-0.55\n\n\nResidual\n\n0.00000000\n0.00006409\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nbegin\n  f_Child0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  m_Child_SDC0 = fit(MixedModel, f_Child0, dat; contrasts=contr1)\n  m_Child_HeC0 = fit(MixedModel, f_Child0, dat; contrasts=contr2)\n  m_Child_HyC0 = fit(MixedModel, f_Child0, dat; contrasts=contr3)\n  m_Child_PCA0 = fit(MixedModel, f_Child0, dat; contrasts=contr4)\nend\n\n\u001b[32mMinimizing 544      Time: 0:00:01 ( 3.13 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0122\n0.0141\n-0.87\n0.3843\n0.7302\n\n\nTest: c5.1\n-0.0200\n0.0428\n-0.47\n0.6405\n1.2122\n\n\nTest: c234.15\n0.0292\n0.1678\n0.17\n0.8620\n0.3138\n\n\nTest: c2.34\n-0.0151\n0.0772\n-0.20\n0.8453\n1.9450\n\n\nTest: c3.4\n0.0113\n0.0449\n0.25\n0.8017\n1.1538\n\n\na1\n0.2185\n0.0483\n4.52\n<1e-05\n\n\n\nSex: Boys\n0.2017\n0.0141\n14.33\n<1e-45\n\n\n\nTest: c5.1 & a1\n0.4026\n0.1464\n2.75\n0.0060\n\n\n\nTest: c234.15 & a1\n-0.5252\n0.5734\n-0.92\n0.3597\n\n\n\nTest: c2.34 & a1\n0.1095\n0.2637\n0.42\n0.6780\n\n\n\nTest: c3.4 & a1\n-0.0992\n0.1545\n-0.64\n0.5211\n\n\n\nTest: c5.1 & Sex: Boys\n0.1059\n0.0428\n2.47\n0.0134\n\n\n\nTest: c234.15 & Sex: Boys\n-1.0090\n0.1678\n-6.01\n<1e-08\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1875\n0.0772\n-2.43\n0.0151\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0734\n0.0449\n-1.63\n0.1021\n\n\n\na1 & Sex: Boys\n0.0588\n0.0483\n1.22\n0.2235\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1710\n0.1464\n1.17\n0.2427\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3013\n0.5734\n0.53\n0.5993\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0644\n0.2637\n-0.24\n0.8071\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0079\n0.1545\n0.05\n0.9594\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13847\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13453\n394\n10\n<1e-77\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13399\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13465\n-66\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13439\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13481\n-42\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13388\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13483\n-95\n10\nNaN\n\n\n\n\n\nRK: The above results are not quite in line with what I obtained with earlier runs of this code, I think. This still needs some follow up.\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nbegin\n  zcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\n  gof_summary = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM,\n      dof=dof.(mods),\n      deviance=deviance.(mods),\n      AIC=aic.(mods),\n      BIC=bic.(mods),\n    ),\n    :deviance,\n  )\nend\n\n4 rows × 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641PCA02613388.313440.313609.72HeC02613398.613450.613620.03HyC02613439.313491.313660.84SDC02613847.413899.414068.9\n\n\nRK: The following description is from an old version of the script; it does not match the current results.\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM.\n\n\nC.3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula zScore ~\n  1 + Test * a1 * Sex + (1 + Test + a1 | School);\nm_School_SeqDiff = fit(MixedModel, f_School, dat; contrasts=contr1);\nm_School_Helmert = fit(MixedModel, f_School, dat; contrasts=contr2);\nm_School_Hypo = fit(MixedModel, f_School, dat; contrasts=contr3);\nm_School_PCA = fit(MixedModel, f_School, dat; contrasts=contr4);\n\n\u001b[32mMinimizing 1006     Time: 0:00:00 ( 0.52 ms/it)\u001b[39m\n\u001b[34m  objective:  13792.96415705431\u001b[39m\n\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.048677\n0.220629\n\n\n\n\n\n\n\n\nTest: Star_r\n0.169858\n0.412139\n+0.21\n\n\n\n\n\n\n\nTest: S20_r\n0.113403\n0.336753\n-0.02\n-0.77\n\n\n\n\n\n\nTest: SLJ\n0.156081\n0.395071\n-0.27\n+0.06\n-0.57\n\n\n\n\n\nTest: BPT\n0.126690\n0.355936\n-0.20\n-0.29\n+0.49\n-0.69\n\n\n\n\na1\n0.053874\n0.232108\n+0.16\n-0.67\n+0.20\n+0.22\n+0.11\n\n\nResidual\n\n0.842352\n0.917797\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0486789\n0.2206330\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0424737\n0.2060915\n+0.21\n\n\n\n\n\n\n\nTest: S20_r\n0.0053807\n0.0733533\n+0.18\n-0.25\n\n\n\n\n\n\nTest: SLJ\n0.0051911\n0.0720496\n-0.28\n-0.04\n-0.61\n\n\n\n\n\nTest: BPT\n0.0026647\n0.0516206\n-0.51\n-0.44\n+0.16\n-0.12\n\n\n\n\na1\n0.0538624\n0.2320827\n+0.16\n-0.67\n-0.32\n+0.13\n+0.27\n\n\nResidual\n\n0.8423630\n0.9178034\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.048678\n0.220631\n\n\n\n\n\n\n\n\nTest: BPT-other\n1.066346\n1.032640\n-0.51\n\n\n\n\n\n\n\nTest: Star-End\n0.169846\n0.412124\n+0.21\n-0.44\n\n\n\n\n\n\nTest: S20-Star\n0.113383\n0.336724\n-0.02\n+0.37\n-0.77\n\n\n\n\n\nTest: SLJ-S20\n0.156139\n0.395144\n-0.27\n-0.15\n+0.06\n-0.57\n\n\n\n\na1\n0.053856\n0.232068\n+0.16\n+0.27\n-0.67\n+0.20\n+0.22\n\n\nResidual\n\n0.842347\n0.917794\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.048679\n0.220633\n\n\n\n\n\n\n\n\nTest: c5.1\n0.059767\n0.244473\n-0.39\n\n\n\n\n\n\n\nTest: c234.15\n2.217842\n1.489242\n+0.39\n-0.15\n\n\n\n\n\n\nTest: c2.34\n0.305316\n0.552554\n+0.21\n+0.21\n+0.65\n\n\n\n\n\nTest: c3.4\n0.156124\n0.395125\n+0.27\n+0.07\n-0.15\n+0.02\n\n\n\n\na1\n0.053870\n0.232099\n+0.16\n-0.34\n-0.65\n-0.40\n-0.22\n\n\nResidual\n\n0.842375\n0.917810\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nbegin\n  f_School0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School)\n  m_School_SDC0 = fit(MixedModel, f_School0, dat; contrasts=contr1)\n  m_School_HeC0 = fit(MixedModel, f_School0, dat; contrasts=contr2)\n  m_School_HyC0 = fit(MixedModel, f_School0, dat; contrasts=contr3)\n  m_School_PCA0 = fit(MixedModel, f_School0, dat; contrasts=contr4)\n  #     \n  zcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods2 = [\n    m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n  ]\n  gof_summary2 = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM2,\n      dof=dof.(mods2),\n      deviance=deviance.(mods2),\n      AIC=aic.(mods2),\n      BIC=bic.(mods2),\n    ),\n    :deviance,\n  )\nend\n\n\u001b[32mMinimizing 258      Time: 0:00:00 ( 0.39 ms/it)\u001b[39m\n\u001b[34m  objective:  13818.13229580311\u001b[39m\n\n\n4 rows × 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641HeC02713809.413863.414039.42PCA02713809.913863.914039.93HyC02713818.113872.114048.14SDC02713821.313875.314051.2\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting."
  },
  {
    "objectID": "contrasts_fggk21.html#thats-it",
    "href": "contrasts_fggk21.html#thats-it",
    "title": "Appendix C — Contrast Coding of Physical Fitness Effects",
    "section": "C.4 That’s it",
    "text": "C.4 That’s it\nThat’s it for this tutorial. It is time to try your own contrast coding. You can use these data; there are many alternatives to set up hypotheses for the five tests. Of course and even better, code up some contrasts for data of your own.\nHave fun!\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4"
  }
]