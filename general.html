<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Phillip Alday" />
  <meta name="author" content="Dave Kleinschmidt" />
  <meta name="author" content="Reinhold Kliegl" />
  <meta name="author" content="Douglas Bates" />
  <title>Generalizing to Other Forms of Mixed Models - Embrace Uncertainty</title>
  <link rel="shortcut icon" type="image/png" href="/favicon.png"/>
  <link rel="stylesheet" href="/style.css"/>
    <script src="/mousetrap.min.js"></script>
    <!-- TODO: Add url_prefix. -->
  <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("/JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="/github.min.css">
<script src="/highlight.min.js"></script>
<script src="/julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        hljs.highlightElement(el);
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="/">Embrace Uncertainty</a>
</div><br />
<span class="books-subtitle">
Fitting Mixed-Effects Models with Julia
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="/ExamLMM"><b>1</b> A Simple, Linear, Mixed-..</a></li>
<li><a class="menu-level-2" href="/memod"><b>1.1</b> Mixed-effects models</a></li>
<li><a class="menu-level-2" href="/DyestuffData"><b>1.2</b> The dyestuff and dyest..</a></li>
<li><a class="menu-level-2" href="/FittingLMMs"><b>1.3</b> Fitting linear mixed m..</a></li>
<li><a class="menu-level-2" href="/Probability"><b>1.4</b> The linear mixed-effec..</a></li>
<li><a class="menu-level-2" href="/variability"><b>1.5</b> Assessing the variabil..</a></li>
<li><a class="menu-level-2" href="/assessRE"><b>1.6</b> Assessing the random e..</a></li>
<li><a class="menu-level-2" href="/ChIntroSummary"><b>1.7</b> Chapter summary</a></li>
<li><a class="menu-level-1" href="/Multiple"><b>2</b> Models With Multiple Ran..</a></li>
<li><a class="menu-level-2" href="/crossedRE"><b>2.1</b> A Model With Crossed R..</a></li>
<li><a class="menu-level-2" href="/NestedRE"><b>2.2</b> A Model With Nested Ra..</a></li>
<li><a class="menu-level-2" href="/partially"><b>2.3</b> A Model With Partially..</a></li>
<li><a class="menu-level-2" href="/MultSummary"><b>2.4</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/longitudinal"><b>3</b> # Models for Longitudina..</a></li>
<li><a class="menu-level-2" href="/sleep"><b>3.1</b> Data</a></li>
<li><a class="menu-level-2" href="/SleepMixed"><b>3.2</b> Data</a></li>
<li><a class="menu-level-2" href="/assess-prec-param"><b>3.3</b> Assessing the Precisio..</a></li>
<li><a class="menu-level-2" href="/fm07re"><b>3.4</b> Examining the Random E..</a></li>
<li><a class="menu-level-2" href="/chapter-summary"><b>3.5</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/computational"><b>4</b> Computational Methods fo..</a></li>
<li><a class="menu-level-2" href="/defnLMM"><b>4.1</b> Definitions and Basic ..</a></li>
<li><a class="menu-level-2" href="/conddistUgivenY"><b>4.2</b> </a></li>
<li><a class="menu-level-2" href="/IntegratingH"><b>4.3</b> in the Linear Mixed Mo..</a></li>
<li><a class="menu-level-2" href="/PLSsol"><b>4.4</b> </a></li>
<li><a class="menu-level-2" href="/REML"><b>4.5</b> The REML Criterion</a></li>
<li><a class="menu-level-2" href="/stepByStep"><b>4.6</b> Step-by-step Evaluatio..</a></li>
<li><a class="menu-level-2" href="/general"><b>4.7</b> Generalizing to Other ..</a></li>
<li><a class="menu-level-2" href="/lmmsummary"><b>4.8</b> Chapter Summary</a></li>
<li><a class="menu-level-1" href="/GLMMbinomial"><b>5</b> Generalized Linear Mixed..</a></li>
<li><a class="menu-level-2" href="/contraception"><b>5.1</b> Artificial contracepti..</a></li>
<li><a class="menu-level-2" href="/GLMMlink"><b>5.2</b> Link functions and int..</a></li>
<li><a class="menu-level-1" href="/references"><b></b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="4.7" id="sec:general"><span class="header-section-number">4.7</span> Generalizing to Other Forms of Mixed Models</h2>
<p>In later chapters we cover the theory and practice of generalized linear mixed models (GLMMs), nonlinear mixed models (NLMMs) and generalized nonlinear mixed models (GNLMMs). Because quite a bit of the theoretical and computational methodology covered in this chapter extends to those models we will cover the common aspects here.</p>
<h3 data-number="4.7.1" id="sec:modelForms"><span class="header-section-number">4.7.1</span> Descriptions of the Model Forms</h3>
<p>We apply the name “generalized” to models in which the conditional distribution, <span class="math inline">\((\mathcal{Y}|\mathcal{U}=\mathbf{u})\)</span>, is not required to be Gaussian but does preserve some of the properties of the spherical Gaussian conditional distribution <span class="math display">\[(\mathcal{Y}|\mathcal{U}=\mathbf{u})\sim\mathcal{N}
  (\mathbf{Z}\Lambda_\theta\mathbf{u}+\mathbf{X}\mathbf{\beta},\sigma^2\mathbf{I}_n)\]</span> from the linear mixed model. In particular, the components of <span class="math inline">\(\mathcal{Y}\)</span> are <em>conditionally independent</em>, given <span class="math inline">\(\mathcal{U}=\mathbf{u}\)</span>. Furthermore, <span class="math inline">\(\mathbf{u}\)</span> affects the distribution only through the conditional mean, which we will continue to write as <span class="math inline">\(\mathbf{\mu}\)</span>, and it affects the conditional mean only through the linear predictor, <span class="math inline">\(\mathbf{\gamma}=\mathbf{Z}\Lambda_\theta\mathbf{u}+\mathbf{X}\mathbf{\beta}\)</span>.</p>
<p>Typically we do not have <span class="math inline">\(\mathbf{\mu}=\mathbf{\gamma}\)</span>, however. The elements of the linear predictor, <span class="math inline">\(\mathbf{\gamma}\)</span>, can be positive or negative or zero. Theoretically they can take on any value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. But many distributional forms used in GLMMs put constraints on the value of the mean. For example, the mean of a Bernoulli random variable, modeling a binary response, must be in the range <span class="math inline">\(0&lt;\mu&lt;1\)</span> and the mean of a Poisson random variable, modeling a count, must be positive. To achieve these constraints we write the conditional mean, <span class="math inline">\(\mathbf{\mu}\)</span>, as a transformation of the unbounded predictor, written <span class="math inline">\(\vec\eta\)</span>. For historical, and some theoretical, reasons the inverse of this transformation is called the <em>link function</em>, written <span class="math display">\[\label{eq:linkfunction}
  \vec\eta=\mathbf{g}(\mathbf{\mu}) ,\]</span> and the transformation we want is called the <em>inverse link</em>, written <span class="math inline">\(\mathbf{g}^{-1}\)</span>.</p>
<p>Both <span class="math inline">\(\mathbf{g}\)</span> and <span class="math inline">\(\mathbf{g}^{-1}\)</span> are determined by scalar functions, <span class="math inline">\(g\)</span> and <span class="math inline">\(g^{-1}\)</span>, respectively, applied to the individual components of the vector argument. That is, <span class="math inline">\(\vec\eta\)</span> must be <span class="math inline">\(n\)</span>-dimensional and the vector-valued function <span class="math inline">\(\mathbf{\mu}=\mathbf{g}^{-1}(\vec\eta)\)</span> is defined by the component functions <span class="math inline">\(\mu_i=g^{-1}(\eta_i),\,i=1,\dots,n\)</span>. Among other things, this means that the Jacobian matrix of the inverse link, <span class="math inline">\(\frac{d\mathbf{\mu}}{d\vec\eta}\)</span>, will be diagonal.</p>
<p>Because the link function, <span class="math inline">\(\mathbf{g}\)</span>, and the inverse link, <span class="math inline">\(\vec g^{-1}\)</span>, are nonlinear functions (there would be no purpose in using a linear link function) many people use the terms “generalized linear mixed model” and “nonlinear mixed model” interchangeably. We reserve the term “nonlinear mixed model” for the type of models used, for example, in pharmacokinetics and pharmacodynamics, where the conditional distribution is a spherical multivariate Gaussian <span class="math display">\[\label{eq:NLMMconddist}
  (\mathcal{Y}|\mathcal{U}=\mathbf{u})\sim\mathcal{N}(\mathbf{\mu}, \sigma^2\mathbf{I}_n)\]</span> but <span class="math inline">\(\mathbf{\mu}\)</span> depends nonlinearly on <span class="math inline">\(\mathbf{\gamma}\)</span>. For NLMMs the length of the linear predictor, <span class="math inline">\(\mathbf{\gamma}\)</span>, is a multiple, <span class="math inline">\(ns\)</span>, of <span class="math inline">\(n\)</span>, the length of <span class="math inline">\(\mathbf{\mu}\)</span>.</p>
<p>Like the map from <span class="math inline">\(\vec\eta\)</span> to <span class="math inline">\(\mathbf{\mu}\)</span>, the map from <span class="math inline">\(\mathbf{\gamma}\)</span> to <span class="math inline">\(\mathbf{\mu}\)</span> has a “diagonal” property, which we now describe. If we use <span class="math inline">\(\mathbf{\gamma}\)</span> to fill the columns of an <span class="math inline">\(n\times s\)</span> matrix, <span class="math inline">\(\Gamma\)</span>, then <span class="math inline">\(\mu_i\)</span> depends only on the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\Gamma\)</span>. In fact, <span class="math inline">\(\mu_i\)</span> is determined by a nonlinear model function, <span class="math inline">\(f\)</span>, applied to the <span class="math inline">\(i\)</span> row of <span class="math inline">\(\Gamma\)</span>. Writing <span class="math inline">\(\mathbf{\mu}=\mathbf{f}(\mathbf{\gamma})\)</span> based on the component function <span class="math inline">\(f\)</span>, we see that the Jacobian of <span class="math inline">\(\mathbf{f}\)</span>, <span class="math inline">\(\frac{d\mathbf{\mu}}{d\mathbf{\gamma}}\)</span>, will be the vertical concatenation of <span class="math inline">\(s\)</span> diagonal <span class="math inline">\(n\times n\)</span> matrices.</p>
<p>Because we will allow for generalized nonlinear mixed models (GNLMMs), in which the mapping from <span class="math inline">\(\mathbf{\gamma}\)</span> to <span class="math inline">\(\mathbf{\mu}\)</span> has the form <span class="math display">\[\label{eq:GammaEtaMu}
  \mathbf{\gamma}\;\rightarrow\;\vec\eta\;\rightarrow\;\mathbf{\mu} ,\]</span> we will use (<a href="#eq:GammaEtaMu" data-reference-type="ref" data-reference="eq:GammaEtaMu">[eq:GammaEtaMu]</a>) in our definitions.</p>
<h3 data-number="4.7.2" id="sec:conditionalmode"><span class="header-section-number">4.7.2</span> Determining the Conditional Mode, <span class="math inline">\(\tilde{\mathbf{u}}\)</span></h3>
<p>For all these types of mixed models, the conditional distribution, <span class="math inline">\((\mathcal{U}|\mathcal{Y}=\mathbf{y}_{\text{obs}})\)</span>, is a continuous distribution for which we can determine the unscaled conditional density, <span class="math inline">\(h(\mathbf{u})\)</span>. As for linear mixed models, we define the conditional mode, <span class="math inline">\(\tilde{\mathbf{u}}\)</span>, as the value that maximizes the unscaled conditional density.</p>
<p>Determining the conditional mode, <span class="math inline">\(\tilde{\mathbf{u}}\)</span>, in a nonlinear mixed model is a penalized nonlinear least squares (PNLS) problem <span class="math display">\[\label{eq:PNLSprob}
  \tilde{\mathbf{u}}=\arg\min_{\mathbf{u}}\|\mathbf{y}_{\text{obs}}-\mathbf{\mu}\|^2+\|\mathbf{u}\|^2\]</span> which we solve by adapting the iterative techniques, such as the Gauss-Newton method <span class="citation" data-cites="bateswatts88:_nonlin">(<a href="/references#ref-bateswatts88:_nonlin" role="doc-biblioref">Bates &amp; Watts, 1988</a> Sect. 2.2.1)</span>, used for nonlinear least squares. Starting at an initial value, <span class="math inline">\(\vec u^{(0)}\)</span>, (the bracketed superscript denotes the iteration number) with conditional mean, <span class="math inline">\(\mathbf{\mu}^{(0)}\)</span>, we determine an increment <span class="math inline">\(\vec\delta^{(1)}\)</span> by solving the penalized linear least squares problem, <span class="math display">\[\label{eq:PNLSiter}
  \vec\delta^{(1)}=\arg\min_{\vec\delta}\left\|
    \begin{bmatrix}
      \mathbf{y}_{\text{obs}}-\mathbf{\mu}^{(0)}\\
      \mathbf{0}-\mathbf{u}^{(0)}
    \end{bmatrix} -
    \begin{bmatrix}
      \mathbf{u}^{(0)}\\
      \mathbf{I}_q
    \end{bmatrix}\vec\delta\right\|^2\]</span> where <span class="math display">\[\label{eq:Udef}
  \mathbf{u}^{(0)}=\left.\frac{d\mathbf{\mu}}{d\mathbf{u}}\right|_{\mathbf{u}^{(0)}} .\]</span> Naturally, we use the sparse Cholesky decomposition, <span class="math inline">\(\vec L_\theta^{(0)}\)</span>, satisfying <span class="math display">\[\label{eq:sparseCholPNLS}
  \mathbf{L}_\theta^{(0)}\left(\mathbf{L}_\theta^{(0)}\right)=
  \mathbf{P}\left[\left(\mathbf{u}^{(0)}\right)&#39;\mathbf{u}^{(0)}+\mathbf{I}_q\right]
  \mathbf{P}&#39;\]</span> to determine this increment. The next iteration begins at <span class="math display">\[\label{eq:u1}
  \mathbf{u}^{(1)} = \mathbf{u}^{(0)} + k \vec\delta^{(1)}\]</span> where <span class="math inline">\(k\)</span> is the step factor chosen, perhaps by step-halving <span class="citation" data-cites="bateswatts88:_nonlin">(<a href="/references#ref-bateswatts88:_nonlin" role="doc-biblioref">Bates &amp; Watts, 1988</a> Sect. 2.2.1)</span>, to ensure that the penalized residual sum of squares decreases at each iteration. Convergence is declared when the orthogonality convergence criterion <span class="citation" data-cites="bateswatts88:_nonlin">(<a href="/references#ref-bateswatts88:_nonlin" role="doc-biblioref">Bates &amp; Watts, 1988</a> Sect. 2.2.3)</span> is below some pre-specified tolerance.</p>
<p>The <em>Laplace approximation</em> to the deviance is <span class="math display">\[\label{eq:Laplace}
  d(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y}_{\text{obs}})\approx
  n\log(2\pi\sigma^2)+2\log|\mathbf{L}_{\theta,\beta}|+
  \frac{r^2_{\theta,\beta}}{\sigma^2},\]</span> where the Cholesky factor, <span class="math inline">\(\mathbf{L}_{\theta,\beta}\)</span>, and the penalized residual sum of squares, <span class="math inline">\(r^2_{\theta,\beta}\)</span>, are both evaluated at the conditional mode, <span class="math inline">\(\tilde{\mathbf{u}}\)</span>. The Cholesky factor depends on <span class="math inline">\(\mathbf{\theta}\)</span>, <span class="math inline">\(\mathbf{\beta}\)</span> and <span class="math inline">\(\mathbf{u}\)</span> for these models but typically the dependence on <span class="math inline">\(\mathbf{\beta}\)</span> and <span class="math inline">\(\mathbf{u}\)</span> is weak.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="/stepByStep"><b>4.6</b> Step-by-step Evaluatio..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="/lmmsummary"><b>4.8</b> Chapter Summary</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Phillip Alday, Dave Kleinschmidt, Reinhold Kliegl, Douglas Bates
</div>
</div>
</div>
</body>
</html>